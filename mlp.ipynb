{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCH = 50000\n",
    "# num_of_categories = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import preprossesing as pre\n",
    "import math\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(df):\n",
    "    \n",
    "    df = pre.standardize(df)\n",
    "    df = pre.encoder(df)\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    train, val = pre.test_validation_split(df)\n",
    "    \n",
    "    y_train = torch.tensor(train['price'].values, dtype=torch.float32)\n",
    "    X_train = train.drop(['price'], axis=1)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    \n",
    "    y_val = torch.tensor(val['price'].values, dtype=torch.float32)\n",
    "    X_val = val.drop(['price'], axis=1)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (model_layer): Sequential(\n",
       "    (0): Linear(in_features=137, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (gear_box_layer): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fuel_type_layer): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (registration_fee_layer): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (engine_capacity_layer): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(8, 256),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(32, 1) \n",
    "        )\n",
    "        \n",
    "        self.model_layer = nn.Sequential(\n",
    "        nn.Linear(137, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.gear_box_layer = nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fuel_type_layer = nn.Sequential( \n",
    "        nn.Linear(4, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.registration_fee_layer = nn.Sequential(\n",
    "        nn.Linear(6, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.engine_capacity_layer = nn.Sequential(\n",
    "        nn.Linear(8, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        model = self.model_layer(x[:, :137])\n",
    "        gear_box = self.gear_box_layer(x[:, 137:140])\n",
    "        fuel_type = self.fuel_type_layer(x[:, 140:144])\n",
    "        registration_fee = self.registration_fee_layer(x[:, 144:150])\n",
    "        engine_capacity = self.engine_capacity_layer(x[:, 150:158])\n",
    "        operating_hours = x[:, 158].view(-1, 1)\n",
    "        year = x[:, 159].view(-1, 1)\n",
    "        efficiency = x[:, 160].view(-1, 1)\n",
    "\n",
    "        \n",
    "        x = torch.cat((model, year, gear_box, operating_hours, fuel_type, registration_fee, efficiency, engine_capacity), 1)\n",
    "        \n",
    "        return self.layers(x)\n",
    "\n",
    "model = mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8100, 164]) torch.Size([8100]) torch.Size([900, 164]) torch.Size([900])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_train, y_train, X_val, y_val = prepare_model(df)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 17579.6640625 Validation loss: 17613.798828125 best Validation loss: 17613.798828125\n",
      "Epoch: 100 Loss: 16542.26953125 Validation loss: 16517.12890625 best Validation loss: 16517.12890625\n",
      "Epoch: 200 Loss: 5731.60888671875 Validation loss: 5834.8291015625 best Validation loss: 5834.8291015625\n",
      "Epoch: 300 Loss: 3533.74169921875 Validation loss: 3631.673583984375 best Validation loss: 3631.673583984375\n",
      "Epoch: 400 Loss: 2609.904296875 Validation loss: 2692.227294921875 best Validation loss: 2692.227294921875\n",
      "Epoch: 500 Loss: 2360.77001953125 Validation loss: 2442.94775390625 best Validation loss: 2442.94775390625\n",
      "Epoch: 600 Loss: 2226.771240234375 Validation loss: 2321.1845703125 best Validation loss: 2321.1845703125\n",
      "Epoch: 700 Loss: 2153.609619140625 Validation loss: 2263.776123046875 best Validation loss: 2263.776123046875\n",
      "Epoch: 800 Loss: 2113.60009765625 Validation loss: 2231.437255859375 best Validation loss: 2231.437255859375\n",
      "Epoch: 900 Loss: 2089.00244140625 Validation loss: 2213.183349609375 best Validation loss: 2213.183349609375\n",
      "Epoch: 1000 Loss: 2072.41162109375 Validation loss: 2203.061279296875 best Validation loss: 2203.061279296875\n",
      "Epoch: 1100 Loss: 2059.635009765625 Validation loss: 2198.138427734375 best Validation loss: 2198.138427734375\n",
      "Epoch: 1200 Loss: 2049.90087890625 Validation loss: 2194.775634765625 best Validation loss: 2194.775634765625\n",
      "Epoch: 1300 Loss: 2042.9495849609375 Validation loss: 2192.738525390625 best Validation loss: 2192.738525390625\n",
      "Epoch: 1400 Loss: 2037.5379638671875 Validation loss: 2190.68603515625 best Validation loss: 2190.68603515625\n",
      "Epoch: 1500 Loss: 2033.3341064453125 Validation loss: 2189.44287109375 best Validation loss: 2189.44287109375\n",
      "Epoch: 1600 Loss: 2029.6923828125 Validation loss: 2188.240478515625 best Validation loss: 2188.231201171875\n",
      "Epoch: 1700 Loss: 2026.631103515625 Validation loss: 2187.58349609375 best Validation loss: 2187.581298828125\n",
      "Epoch: 1800 Loss: 2023.9283447265625 Validation loss: 2187.558349609375 best Validation loss: 2187.311767578125\n",
      "Epoch: 1900 Loss: 2021.3895263671875 Validation loss: 2187.4501953125 best Validation loss: 2187.311767578125\n",
      "Epoch: 2000 Loss: 2018.90771484375 Validation loss: 2187.661865234375 best Validation loss: 2187.11083984375\n",
      "Epoch: 2100 Loss: 2016.611083984375 Validation loss: 2187.358642578125 best Validation loss: 2187.11083984375\n",
      "Epoch: 2200 Loss: 2014.2222900390625 Validation loss: 2186.66064453125 best Validation loss: 2186.66064453125\n",
      "Epoch: 2300 Loss: 2011.7720947265625 Validation loss: 2186.243408203125 best Validation loss: 2186.2255859375\n",
      "Epoch: 2400 Loss: 2009.5052490234375 Validation loss: 2185.272705078125 best Validation loss: 2185.272705078125\n",
      "Epoch: 2500 Loss: 2007.5987548828125 Validation loss: 2184.427001953125 best Validation loss: 2184.427001953125\n",
      "Epoch: 2600 Loss: 2005.6937255859375 Validation loss: 2181.984619140625 best Validation loss: 2181.984619140625\n",
      "Epoch: 2700 Loss: 2004.1375732421875 Validation loss: 2180.34326171875 best Validation loss: 2180.333251953125\n",
      "Epoch: 2800 Loss: 2002.89697265625 Validation loss: 2178.978515625 best Validation loss: 2178.978515625\n",
      "Epoch: 2900 Loss: 2001.5106201171875 Validation loss: 2177.21630859375 best Validation loss: 2177.21630859375\n",
      "Epoch: 3000 Loss: 2000.2076416015625 Validation loss: 2176.666259765625 best Validation loss: 2176.6083984375\n",
      "Epoch: 3100 Loss: 1998.9156494140625 Validation loss: 2175.63037109375 best Validation loss: 2175.62060546875\n",
      "Epoch: 3200 Loss: 1997.85888671875 Validation loss: 2174.73828125 best Validation loss: 2174.73828125\n",
      "Epoch: 3300 Loss: 1996.9716796875 Validation loss: 2173.7978515625 best Validation loss: 2173.79443359375\n",
      "Epoch: 3400 Loss: 1996.1668701171875 Validation loss: 2173.1611328125 best Validation loss: 2173.1611328125\n",
      "Epoch: 3500 Loss: 1995.1268310546875 Validation loss: 2170.1630859375 best Validation loss: 2170.0869140625\n",
      "Epoch: 3600 Loss: 1993.9967041015625 Validation loss: 2170.232421875 best Validation loss: 2170.065185546875\n",
      "Epoch: 3700 Loss: 1993.2357177734375 Validation loss: 2169.6435546875 best Validation loss: 2169.5244140625\n",
      "Epoch: 3800 Loss: 1992.4881591796875 Validation loss: 2168.478759765625 best Validation loss: 2168.478759765625\n",
      "Epoch: 3900 Loss: 1991.97314453125 Validation loss: 2167.904296875 best Validation loss: 2167.891357421875\n",
      "Epoch: 4000 Loss: 1991.486083984375 Validation loss: 2167.21484375 best Validation loss: 2167.20361328125\n",
      "Epoch: 4100 Loss: 1990.6409912109375 Validation loss: 2165.885009765625 best Validation loss: 2165.712890625\n",
      "Epoch: 4200 Loss: 1990.099853515625 Validation loss: 2164.93115234375 best Validation loss: 2164.93115234375\n",
      "Epoch: 4300 Loss: 1989.462158203125 Validation loss: 2163.399169921875 best Validation loss: 2163.369140625\n",
      "Epoch: 4400 Loss: 1988.9476318359375 Validation loss: 2162.473388671875 best Validation loss: 2162.4453125\n",
      "Epoch: 4500 Loss: 1988.4151611328125 Validation loss: 2161.305419921875 best Validation loss: 2161.305419921875\n",
      "Epoch: 4600 Loss: 1987.927490234375 Validation loss: 2160.022705078125 best Validation loss: 2160.022705078125\n",
      "Epoch: 4700 Loss: 1987.4693603515625 Validation loss: 2158.189453125 best Validation loss: 2158.189453125\n",
      "Epoch: 4800 Loss: 1987.1168212890625 Validation loss: 2156.952880859375 best Validation loss: 2156.947998046875\n",
      "Epoch: 4900 Loss: 1986.739013671875 Validation loss: 2155.9384765625 best Validation loss: 2155.9384765625\n",
      "Epoch: 5000 Loss: 1986.389892578125 Validation loss: 2155.1357421875 best Validation loss: 2155.052978515625\n",
      "Epoch: 5100 Loss: 1986.0267333984375 Validation loss: 2154.16259765625 best Validation loss: 2154.1591796875\n",
      "Epoch: 5200 Loss: 1985.723876953125 Validation loss: 2153.32763671875 best Validation loss: 2153.32763671875\n",
      "Epoch: 5300 Loss: 1985.42138671875 Validation loss: 2152.26123046875 best Validation loss: 2152.254150390625\n",
      "Epoch: 5400 Loss: 1985.162109375 Validation loss: 2151.23095703125 best Validation loss: 2151.23095703125\n",
      "Epoch: 5500 Loss: 1984.90087890625 Validation loss: 2150.40966796875 best Validation loss: 2150.3369140625\n",
      "Epoch: 5600 Loss: 1984.5638427734375 Validation loss: 2149.37548828125 best Validation loss: 2149.320556640625\n",
      "Epoch: 5700 Loss: 1984.219482421875 Validation loss: 2148.330078125 best Validation loss: 2148.330078125\n",
      "Epoch: 5800 Loss: 1983.933349609375 Validation loss: 2147.001953125 best Validation loss: 2147.001953125\n",
      "Epoch: 5900 Loss: 1983.631591796875 Validation loss: 2145.59326171875 best Validation loss: 2145.59326171875\n",
      "Epoch: 6000 Loss: 1983.328125 Validation loss: 2144.293212890625 best Validation loss: 2144.270263671875\n",
      "Epoch: 6100 Loss: 1983.064208984375 Validation loss: 2142.89306640625 best Validation loss: 2142.89306640625\n",
      "Epoch: 6200 Loss: 1982.7572021484375 Validation loss: 2141.606689453125 best Validation loss: 2141.580322265625\n",
      "Epoch: 6300 Loss: 1982.5504150390625 Validation loss: 2140.053955078125 best Validation loss: 2140.053955078125\n",
      "Epoch: 6400 Loss: 1982.2293701171875 Validation loss: 2138.338134765625 best Validation loss: 2138.313232421875\n",
      "Epoch: 6500 Loss: 1981.9503173828125 Validation loss: 2136.8935546875 best Validation loss: 2136.8935546875\n",
      "Epoch: 6600 Loss: 1981.7247314453125 Validation loss: 2135.636474609375 best Validation loss: 2135.54345703125\n",
      "Epoch: 6700 Loss: 1981.4361572265625 Validation loss: 2134.28271484375 best Validation loss: 2134.28271484375\n",
      "Epoch: 6800 Loss: 1981.2191162109375 Validation loss: 2133.0673828125 best Validation loss: 2133.0673828125\n",
      "Epoch: 6900 Loss: 1980.924072265625 Validation loss: 2131.754150390625 best Validation loss: 2131.754150390625\n",
      "Epoch: 7000 Loss: 1980.6453857421875 Validation loss: 2130.3388671875 best Validation loss: 2130.2822265625\n",
      "Epoch: 7100 Loss: 1980.4287109375 Validation loss: 2128.934814453125 best Validation loss: 2128.934814453125\n",
      "Epoch: 7200 Loss: 1980.142333984375 Validation loss: 2127.467041015625 best Validation loss: 2127.467041015625\n",
      "Epoch: 7300 Loss: 1979.8660888671875 Validation loss: 2126.207763671875 best Validation loss: 2126.184814453125\n",
      "Epoch: 7400 Loss: 1979.6595458984375 Validation loss: 2124.98828125 best Validation loss: 2124.9072265625\n",
      "Epoch: 7500 Loss: 1979.4493408203125 Validation loss: 2123.94873046875 best Validation loss: 2123.881591796875\n",
      "Epoch: 7600 Loss: 1979.22900390625 Validation loss: 2122.87109375 best Validation loss: 2122.747314453125\n",
      "Epoch: 7700 Loss: 1978.7603759765625 Validation loss: 2121.553955078125 best Validation loss: 2121.511962890625\n",
      "Epoch: 7800 Loss: 1978.181884765625 Validation loss: 2120.5068359375 best Validation loss: 2120.490478515625\n",
      "Epoch: 7900 Loss: 1977.470458984375 Validation loss: 2119.573486328125 best Validation loss: 2119.52587890625\n",
      "Epoch: 8000 Loss: 1976.7008056640625 Validation loss: 2118.34375 best Validation loss: 2118.34375\n",
      "Epoch: 8100 Loss: 1975.9100341796875 Validation loss: 2117.539306640625 best Validation loss: 2117.502685546875\n",
      "Epoch: 8200 Loss: 1974.9344482421875 Validation loss: 2116.774658203125 best Validation loss: 2116.774658203125\n",
      "Epoch: 8300 Loss: 1974.146240234375 Validation loss: 2115.92333984375 best Validation loss: 2115.914794921875\n",
      "Epoch: 8400 Loss: 1973.21484375 Validation loss: 2114.986083984375 best Validation loss: 2114.986083984375\n",
      "Epoch: 8500 Loss: 1972.4732666015625 Validation loss: 2114.181640625 best Validation loss: 2114.181640625\n",
      "Epoch: 8600 Loss: 1971.60693359375 Validation loss: 2113.551513671875 best Validation loss: 2113.450927734375\n",
      "Epoch: 8700 Loss: 1970.6351318359375 Validation loss: 2113.607177734375 best Validation loss: 2113.11962890625\n",
      "Epoch: 8800 Loss: 1969.697509765625 Validation loss: 2113.400146484375 best Validation loss: 2113.06787109375\n",
      "Epoch: 8900 Loss: 1969.0167236328125 Validation loss: 2112.228271484375 best Validation loss: 2111.977294921875\n",
      "Epoch: 9000 Loss: 1967.7188720703125 Validation loss: 2110.439208984375 best Validation loss: 2110.439208984375\n",
      "Epoch: 9100 Loss: 1966.5394287109375 Validation loss: 2110.41796875 best Validation loss: 2110.1611328125\n",
      "Epoch: 9200 Loss: 1965.4498291015625 Validation loss: 2109.016357421875 best Validation loss: 2109.016357421875\n",
      "Epoch: 9300 Loss: 1964.1904296875 Validation loss: 2107.7314453125 best Validation loss: 2107.7314453125\n",
      "Epoch: 9400 Loss: 1962.8997802734375 Validation loss: 2107.16357421875 best Validation loss: 2106.945068359375\n",
      "Epoch: 9500 Loss: 1961.7801513671875 Validation loss: 2106.107177734375 best Validation loss: 2105.883056640625\n",
      "Epoch: 9600 Loss: 1960.3052978515625 Validation loss: 2103.82958984375 best Validation loss: 2103.213134765625\n",
      "Epoch: 9700 Loss: 1958.8157958984375 Validation loss: 2103.034912109375 best Validation loss: 2102.266845703125\n",
      "Epoch: 9800 Loss: 1957.735107421875 Validation loss: 2102.386474609375 best Validation loss: 2102.266845703125\n",
      "Epoch: 9900 Loss: 1956.5975341796875 Validation loss: 2101.721435546875 best Validation loss: 2101.211181640625\n",
      "Epoch: 10000 Loss: 1955.123046875 Validation loss: 2100.422119140625 best Validation loss: 2099.65771484375\n",
      "Epoch: 10100 Loss: 1954.1063232421875 Validation loss: 2100.15771484375 best Validation loss: 2099.65771484375\n",
      "Epoch: 10200 Loss: 1953.032470703125 Validation loss: 2100.188232421875 best Validation loss: 2099.63037109375\n",
      "Epoch: 10300 Loss: 1952.25390625 Validation loss: 2099.72265625 best Validation loss: 2099.604248046875\n",
      "Epoch: 10400 Loss: 1951.664306640625 Validation loss: 2099.4296875 best Validation loss: 2099.39794921875\n",
      "Epoch: 10500 Loss: 1950.9478759765625 Validation loss: 2099.167724609375 best Validation loss: 2099.167724609375\n",
      "Epoch: 10600 Loss: 1950.2479248046875 Validation loss: 2098.94580078125 best Validation loss: 2098.4560546875\n",
      "Epoch: 10700 Loss: 1949.1839599609375 Validation loss: 2098.255126953125 best Validation loss: 2097.547607421875\n",
      "Epoch: 10800 Loss: 1948.3258056640625 Validation loss: 2098.141845703125 best Validation loss: 2097.547607421875\n",
      "Epoch: 10900 Loss: 1947.4609375 Validation loss: 2097.593505859375 best Validation loss: 2097.30615234375\n",
      "Epoch: 11000 Loss: 1946.291748046875 Validation loss: 2097.10888671875 best Validation loss: 2096.728759765625\n",
      "Epoch: 11100 Loss: 1945.338134765625 Validation loss: 2095.543212890625 best Validation loss: 2095.388671875\n",
      "Epoch: 11200 Loss: 1944.3677978515625 Validation loss: 2095.444580078125 best Validation loss: 2095.182861328125\n",
      "Epoch: 11300 Loss: 1943.5721435546875 Validation loss: 2095.24169921875 best Validation loss: 2095.136962890625\n",
      "Epoch: 11400 Loss: 1942.6907958984375 Validation loss: 2094.93505859375 best Validation loss: 2094.93505859375\n",
      "Epoch: 11500 Loss: 1941.8165283203125 Validation loss: 2095.02490234375 best Validation loss: 2094.4599609375\n",
      "Epoch: 11600 Loss: 1941.0047607421875 Validation loss: 2095.031982421875 best Validation loss: 2094.381103515625\n",
      "Epoch: 11700 Loss: 1940.26513671875 Validation loss: 2095.509033203125 best Validation loss: 2094.381103515625\n",
      "Epoch: 11800 Loss: 1939.6162109375 Validation loss: 2094.943115234375 best Validation loss: 2094.381103515625\n",
      "Epoch: 11900 Loss: 1939.054931640625 Validation loss: 2095.0048828125 best Validation loss: 2094.381103515625\n",
      "Epoch: 12000 Loss: 1938.39453125 Validation loss: 2095.29736328125 best Validation loss: 2094.381103515625\n",
      "Epoch: 12100 Loss: 1937.6624755859375 Validation loss: 2095.677978515625 best Validation loss: 2094.381103515625\n",
      "Epoch: 12200 Loss: 1937.1756591796875 Validation loss: 2095.8662109375 best Validation loss: 2094.381103515625\n",
      "Epoch: 12300 Loss: 1936.5048828125 Validation loss: 2095.64990234375 best Validation loss: 2094.381103515625\n",
      "Epoch: 12400 Loss: 1936.318603515625 Validation loss: 2095.663330078125 best Validation loss: 2094.381103515625\n",
      "Epoch: 12500 Loss: 1935.4239501953125 Validation loss: 2095.811767578125 best Validation loss: 2094.381103515625\n",
      "Epoch: 12600 Loss: 1935.174560546875 Validation loss: 2095.850830078125 best Validation loss: 2094.381103515625\n",
      "Epoch: 12700 Loss: 1934.658935546875 Validation loss: 2096.0400390625 best Validation loss: 2094.381103515625\n",
      "Epoch: 12800 Loss: 1934.1383056640625 Validation loss: 2096.409423828125 best Validation loss: 2094.381103515625\n",
      "Epoch: 12900 Loss: 1933.56640625 Validation loss: 2097.156494140625 best Validation loss: 2094.381103515625\n",
      "Epoch: 13000 Loss: 1933.1806640625 Validation loss: 2097.101318359375 best Validation loss: 2094.381103515625\n",
      "Epoch: 13100 Loss: 1933.0125732421875 Validation loss: 2097.103515625 best Validation loss: 2094.381103515625\n",
      "Epoch: 13200 Loss: 1932.287109375 Validation loss: 2098.006103515625 best Validation loss: 2094.381103515625\n",
      "Epoch: 13300 Loss: 1931.861328125 Validation loss: 2097.240478515625 best Validation loss: 2094.381103515625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_val)\n\u001b[1;32m     20\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(validation_loss, val_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[45], line 49\u001b[0m, in \u001b[0;36mmlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 49\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m137\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     gear_box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgear_box_layer(x[:, \u001b[38;5;241m137\u001b[39m:\u001b[38;5;241m140\u001b[39m])\n\u001b[1;32m     51\u001b[0m     fuel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuel_type_layer(x[:, \u001b[38;5;241m140\u001b[39m:\u001b[38;5;241m144\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "last_val_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for n in range(NUMBER_OF_EPOCH):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)[:, 0]\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    training_loss = np.append(training_loss, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = model(X_val)[:, 0]\n",
    "    val_loss = loss_fn(y_pred, y_val)\n",
    "    validation_loss = np.append(validation_loss, val_loss.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model, 'mlp_model.pth')\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(f'Epoch: {n} Loss: {loss.item()}'f' Validation loss: {val_loss.item()}'f' best Validation loss: {best_val_loss}')\n",
    "        \n",
    "    if last_val_loss < val_loss:\n",
    "        count += 1\n",
    "        if count == 25:\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "    last_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x310c39ad0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/cy9kvd894_bfkfb71dsbxt2w0000gn/T/ipykernel_69898/228361571.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mlp_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "model = torch.load('mlp_model.pth')\n",
    "model.eval()\n",
    "\n",
    "index = df['id']\n",
    "df = df.drop(['id'], axis=1)\n",
    "X = pre.encoder(df)\n",
    "X = pre.standardize(X, scaler=pickle.load(open('scaler.pkl', 'rb')))\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_pred = model(X)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(y_pred.detach().numpy(), columns=['price'])\n",
    "df = pd.concat([index, df], axis=1)\n",
    "df.rename(columns={'price': 'answer'}, inplace=True)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
