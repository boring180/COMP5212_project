{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCH = 50000\n",
    "# num_of_categories = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import preprossesing as pre\n",
    "import math\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(df):\n",
    "    \n",
    "    df = pre.standardize(df)\n",
    "    df = pre.encoder(df)\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    train, val = pre.test_validation_split(df)\n",
    "    \n",
    "    y_train = torch.tensor(train['price'].values, dtype=torch.float32)\n",
    "    X_train = train.drop(['price'], axis=1)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    \n",
    "    y_val = torch.tensor(val['price'].values, dtype=torch.float32)\n",
    "    X_val = val.drop(['price'], axis=1)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (model_layer): Sequential(\n",
       "    (0): Linear(in_features=151, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (gear_box_layer): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fuel_type_layer): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (registration_fee_layer): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (engine_capacity_layer): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(8, 512),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        self.model_layer = nn.Sequential(\n",
    "        nn.Linear(151, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.gear_box_layer = nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fuel_type_layer = nn.Sequential( \n",
    "        nn.Linear(4, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.registration_fee_layer = nn.Sequential(\n",
    "        nn.Linear(8, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.engine_capacity_layer = nn.Sequential(\n",
    "        nn.Linear(10, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        model = self.model_layer(x[:, :151])\n",
    "        gear_box = self.gear_box_layer(x[:, 151:154])\n",
    "        fuel_type = self.fuel_type_layer(x[:, 154:158])\n",
    "        registration_fee = self.registration_fee_layer(x[:, 158:166])\n",
    "        engine_capacity = self.engine_capacity_layer(x[:, 166:176])\n",
    "        operating_hours = x[:, 176].view(-1, 1)\n",
    "        year = x[:, 177].view(-1, 1)\n",
    "        efficiency = x[:, 178].view(-1, 1)\n",
    "\n",
    "        \n",
    "        x = torch.cat((model, year, gear_box, operating_hours, fuel_type, registration_fee, efficiency, engine_capacity), 1)\n",
    "        \n",
    "        return self.layers(x)\n",
    "\n",
    "model = mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8550, 179]) torch.Size([8550]) torch.Size([950, 179]) torch.Size([950])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_train, y_train, X_val, y_val = prepare_model(df)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 18396.865234375 Validation loss: 18099.69140625 best Validation loss: 18099.69140625\n",
      "Epoch: 100 Loss: 8759.181640625 Validation loss: 8707.869140625 best Validation loss: 8707.869140625\n",
      "Epoch: 200 Loss: 5615.587890625 Validation loss: 5550.7783203125 best Validation loss: 5550.7783203125\n",
      "Epoch: 300 Loss: 2889.8056640625 Validation loss: 2939.080322265625 best Validation loss: 2939.080322265625\n",
      "Epoch: 400 Loss: 2188.22216796875 Validation loss: 2152.32958984375 best Validation loss: 2148.801025390625\n",
      "Epoch: 500 Loss: 2167.21630859375 Validation loss: 2134.814697265625 best Validation loss: 2131.171142578125\n",
      "Epoch: 600 Loss: 2160.0419921875 Validation loss: 2133.347412109375 best Validation loss: 2131.171142578125\n",
      "Epoch: 700 Loss: 2145.114013671875 Validation loss: 2120.03466796875 best Validation loss: 2119.291015625\n",
      "Epoch: 800 Loss: 2130.98486328125 Validation loss: 2104.037109375 best Validation loss: 2103.902099609375\n",
      "Epoch: 900 Loss: 2123.380615234375 Validation loss: 2096.273681640625 best Validation loss: 2091.9375\n",
      "Epoch: 1000 Loss: 2119.333740234375 Validation loss: 2093.57275390625 best Validation loss: 2088.501953125\n",
      "Epoch: 1100 Loss: 2116.171875 Validation loss: 2089.958740234375 best Validation loss: 2087.3388671875\n",
      "Epoch: 1200 Loss: 2114.41064453125 Validation loss: 2086.08642578125 best Validation loss: 2085.86962890625\n",
      "Epoch: 1300 Loss: 2112.6494140625 Validation loss: 2090.884033203125 best Validation loss: 2084.742919921875\n",
      "Epoch: 1400 Loss: 2110.8095703125 Validation loss: 2087.096435546875 best Validation loss: 2083.315185546875\n",
      "Epoch: 1500 Loss: 2109.271728515625 Validation loss: 2083.4111328125 best Validation loss: 2081.494873046875\n",
      "Epoch: 1600 Loss: 2108.114501953125 Validation loss: 2080.70458984375 best Validation loss: 2080.656494140625\n",
      "Epoch: 1700 Loss: 2106.565673828125 Validation loss: 2079.140380859375 best Validation loss: 2078.704833984375\n",
      "Epoch: 1800 Loss: 2104.77001953125 Validation loss: 2078.739990234375 best Validation loss: 2078.072998046875\n",
      "Epoch: 1900 Loss: 2103.146728515625 Validation loss: 2080.358154296875 best Validation loss: 2076.658935546875\n",
      "Epoch: 2000 Loss: 2102.157470703125 Validation loss: 2082.00927734375 best Validation loss: 2074.980712890625\n",
      "Epoch: 2100 Loss: 2101.319580078125 Validation loss: 2072.40380859375 best Validation loss: 2072.3212890625\n",
      "Epoch: 2200 Loss: 2099.538330078125 Validation loss: 2076.611083984375 best Validation loss: 2071.14208984375\n",
      "Epoch: 2300 Loss: 2098.81494140625 Validation loss: 2067.805908203125 best Validation loss: 2067.805908203125\n",
      "Epoch: 2400 Loss: 2097.08251953125 Validation loss: 2073.345947265625 best Validation loss: 2065.879638671875\n",
      "Epoch: 2500 Loss: 2095.3916015625 Validation loss: 2063.679931640625 best Validation loss: 2062.418212890625\n",
      "Epoch: 2600 Loss: 2094.35498046875 Validation loss: 2060.03271484375 best Validation loss: 2059.2529296875\n",
      "Epoch: 2700 Loss: 2092.507080078125 Validation loss: 2060.603515625 best Validation loss: 2055.76953125\n",
      "Epoch: 2800 Loss: 2091.56298828125 Validation loss: 2058.941162109375 best Validation loss: 2052.389892578125\n",
      "Epoch: 2900 Loss: 2090.05517578125 Validation loss: 2051.444580078125 best Validation loss: 2050.909912109375\n",
      "Epoch: 3000 Loss: 2088.614013671875 Validation loss: 2051.89697265625 best Validation loss: 2047.8232421875\n",
      "Epoch: 3100 Loss: 2087.48193359375 Validation loss: 2051.708740234375 best Validation loss: 2046.24658203125\n",
      "Epoch: 3200 Loss: 2086.9091796875 Validation loss: 2052.662353515625 best Validation loss: 2042.9423828125\n",
      "Epoch: 3300 Loss: 2084.938232421875 Validation loss: 2047.845458984375 best Validation loss: 2041.926025390625\n",
      "Epoch: 3400 Loss: 2083.770751953125 Validation loss: 2046.1463623046875 best Validation loss: 2040.968505859375\n",
      "Epoch: 3500 Loss: 2084.5126953125 Validation loss: 2038.580322265625 best Validation loss: 2038.580322265625\n",
      "Epoch: 3600 Loss: 2082.3662109375 Validation loss: 2049.27880859375 best Validation loss: 2038.1019287109375\n",
      "Epoch: 3700 Loss: 2080.80078125 Validation loss: 2046.1226806640625 best Validation loss: 2037.1884765625\n",
      "Epoch: 3800 Loss: 2079.017578125 Validation loss: 2038.1575927734375 best Validation loss: 2035.806396484375\n",
      "Epoch: 3900 Loss: 2078.11962890625 Validation loss: 2044.477783203125 best Validation loss: 2034.986328125\n",
      "Epoch: 4000 Loss: 2076.41162109375 Validation loss: 2037.90185546875 best Validation loss: 2033.5057373046875\n",
      "Epoch: 4100 Loss: 2075.171142578125 Validation loss: 2033.2626953125 best Validation loss: 2032.10546875\n",
      "Epoch: 4200 Loss: 2074.157470703125 Validation loss: 2029.1785888671875 best Validation loss: 2029.1785888671875\n",
      "Epoch: 4300 Loss: 2072.42919921875 Validation loss: 2028.55859375 best Validation loss: 2028.0296630859375\n",
      "Epoch: 4400 Loss: 2071.13427734375 Validation loss: 2025.4344482421875 best Validation loss: 2025.4344482421875\n",
      "Epoch: 4500 Loss: 2069.438720703125 Validation loss: 2024.075927734375 best Validation loss: 2023.2144775390625\n",
      "Epoch: 4600 Loss: 2069.24951171875 Validation loss: 2033.1710205078125 best Validation loss: 2020.7562255859375\n",
      "Epoch: 4700 Loss: 2068.110595703125 Validation loss: 2019.2464599609375 best Validation loss: 2019.0218505859375\n",
      "Epoch: 4800 Loss: 2064.1142578125 Validation loss: 2022.3096923828125 best Validation loss: 2018.11865234375\n",
      "Epoch: 4900 Loss: 2063.143798828125 Validation loss: 2025.7960205078125 best Validation loss: 2015.91015625\n",
      "Epoch: 5000 Loss: 2062.277099609375 Validation loss: 2027.2415771484375 best Validation loss: 2015.8653564453125\n",
      "Epoch: 5100 Loss: 2060.7314453125 Validation loss: 2015.1478271484375 best Validation loss: 2014.5921630859375\n",
      "Epoch: 5200 Loss: 2058.519775390625 Validation loss: 2019.5726318359375 best Validation loss: 2013.1104736328125\n",
      "Epoch: 5300 Loss: 2059.003173828125 Validation loss: 2012.9586181640625 best Validation loss: 2012.846923828125\n",
      "Epoch: 5400 Loss: 2057.50830078125 Validation loss: 2012.403564453125 best Validation loss: 2012.021484375\n",
      "Epoch: 5500 Loss: 2055.658447265625 Validation loss: 2013.0882568359375 best Validation loss: 2010.728515625\n",
      "Epoch: 5600 Loss: 2054.3720703125 Validation loss: 2020.227783203125 best Validation loss: 2010.728515625\n",
      "Epoch: 5700 Loss: 2052.59375 Validation loss: 2013.4569091796875 best Validation loss: 2010.1531982421875\n",
      "Epoch: 5800 Loss: 2051.713623046875 Validation loss: 2013.0316162109375 best Validation loss: 2009.3277587890625\n",
      "Epoch: 5900 Loss: 2052.335693359375 Validation loss: 2010.15478515625 best Validation loss: 2009.3277587890625\n",
      "Epoch: 6000 Loss: 2049.392333984375 Validation loss: 2013.3115234375 best Validation loss: 2009.251220703125\n",
      "Epoch: 6100 Loss: 2049.248291015625 Validation loss: 2010.7926025390625 best Validation loss: 2008.5369873046875\n",
      "Epoch: 6200 Loss: 2051.490234375 Validation loss: 2007.9541015625 best Validation loss: 2007.9541015625\n",
      "Epoch: 6300 Loss: 2046.2772216796875 Validation loss: 2013.3341064453125 best Validation loss: 2007.6231689453125\n",
      "Epoch: 6400 Loss: 2045.855712890625 Validation loss: 2018.2142333984375 best Validation loss: 2007.174072265625\n",
      "Epoch: 6500 Loss: 2045.56494140625 Validation loss: 2019.4578857421875 best Validation loss: 2006.77392578125\n",
      "Epoch: 6600 Loss: 2047.0562744140625 Validation loss: 2006.1943359375 best Validation loss: 2005.4781494140625\n",
      "Epoch: 6700 Loss: 2042.4552001953125 Validation loss: 2008.8712158203125 best Validation loss: 2004.499755859375\n",
      "Epoch: 6800 Loss: 2042.7919921875 Validation loss: 2019.3779296875 best Validation loss: 2004.188232421875\n",
      "Epoch: 6900 Loss: 2040.55029296875 Validation loss: 2008.712890625 best Validation loss: 2004.0098876953125\n",
      "Epoch: 7000 Loss: 2039.3336181640625 Validation loss: 2008.8389892578125 best Validation loss: 2003.443603515625\n",
      "Epoch: 7100 Loss: 2040.3414306640625 Validation loss: 2003.99609375 best Validation loss: 2002.1146240234375\n",
      "Epoch: 7200 Loss: 2038.2288818359375 Validation loss: 2004.4678955078125 best Validation loss: 2001.99267578125\n",
      "Epoch: 7300 Loss: 2036.5479736328125 Validation loss: 2007.5362548828125 best Validation loss: 2001.0242919921875\n",
      "Epoch: 7400 Loss: 2035.377197265625 Validation loss: 2008.54931640625 best Validation loss: 2000.722900390625\n",
      "Epoch: 7500 Loss: 2036.596435546875 Validation loss: 2016.751708984375 best Validation loss: 2000.060791015625\n",
      "Epoch: 7600 Loss: 2033.748291015625 Validation loss: 2004.7984619140625 best Validation loss: 1998.9849853515625\n",
      "Epoch: 7700 Loss: 2032.5694580078125 Validation loss: 2002.35546875 best Validation loss: 1998.9849853515625\n",
      "Epoch: 7800 Loss: 2031.7906494140625 Validation loss: 2002.8963623046875 best Validation loss: 1998.2745361328125\n",
      "Epoch: 7900 Loss: 2030.465087890625 Validation loss: 2003.3614501953125 best Validation loss: 1997.2823486328125\n",
      "Epoch: 8000 Loss: 2031.601806640625 Validation loss: 2014.9853515625 best Validation loss: 1996.2320556640625\n",
      "Epoch: 8100 Loss: 2028.4346923828125 Validation loss: 2003.11474609375 best Validation loss: 1996.2320556640625\n",
      "Epoch: 8200 Loss: 2029.2415771484375 Validation loss: 1997.3565673828125 best Validation loss: 1995.6910400390625\n",
      "Epoch: 8300 Loss: 2026.980224609375 Validation loss: 1998.7919921875 best Validation loss: 1994.9525146484375\n",
      "Epoch: 8400 Loss: 2029.0537109375 Validation loss: 1994.735595703125 best Validation loss: 1994.660400390625\n",
      "Epoch: 8500 Loss: 2025.26123046875 Validation loss: 1997.2833251953125 best Validation loss: 1993.8780517578125\n",
      "Epoch: 8600 Loss: 2025.51123046875 Validation loss: 1995.1988525390625 best Validation loss: 1993.4957275390625\n",
      "Epoch: 8700 Loss: 2025.0833740234375 Validation loss: 2007.5428466796875 best Validation loss: 1993.3814697265625\n",
      "Epoch: 8800 Loss: 2022.24951171875 Validation loss: 2000.1923828125 best Validation loss: 1992.6357421875\n",
      "Epoch: 8900 Loss: 2023.979736328125 Validation loss: 2009.3587646484375 best Validation loss: 1992.6357421875\n",
      "Epoch: 9000 Loss: 2025.025634765625 Validation loss: 2016.0224609375 best Validation loss: 1991.78125\n",
      "Epoch: 9100 Loss: 2020.168212890625 Validation loss: 2004.6016845703125 best Validation loss: 1991.5107421875\n",
      "Epoch: 9200 Loss: 2019.345703125 Validation loss: 1994.4473876953125 best Validation loss: 1991.5107421875\n",
      "Epoch: 9300 Loss: 2018.3570556640625 Validation loss: 1994.4937744140625 best Validation loss: 1991.3465576171875\n",
      "Epoch: 9400 Loss: 2018.5682373046875 Validation loss: 1993.2755126953125 best Validation loss: 1991.137451171875\n",
      "Epoch: 9500 Loss: 2017.3817138671875 Validation loss: 1993.372314453125 best Validation loss: 1991.137451171875\n",
      "Epoch: 9600 Loss: 2015.788330078125 Validation loss: 2005.0499267578125 best Validation loss: 1990.3792724609375\n",
      "Epoch: 9700 Loss: 2014.322998046875 Validation loss: 1999.5196533203125 best Validation loss: 1989.9342041015625\n",
      "Epoch: 9800 Loss: 2015.44287109375 Validation loss: 1991.52392578125 best Validation loss: 1989.9342041015625\n",
      "Epoch: 9900 Loss: 2014.4796142578125 Validation loss: 1990.7252197265625 best Validation loss: 1989.9342041015625\n",
      "Epoch: 10000 Loss: 2011.678955078125 Validation loss: 1994.3094482421875 best Validation loss: 1989.362548828125\n",
      "Epoch: 10100 Loss: 2014.630859375 Validation loss: 2007.81982421875 best Validation loss: 1989.163818359375\n",
      "Epoch: 10200 Loss: 2011.446533203125 Validation loss: 1990.0419921875 best Validation loss: 1989.163818359375\n",
      "Epoch: 10300 Loss: 2012.1912841796875 Validation loss: 1989.1529541015625 best Validation loss: 1988.8653564453125\n",
      "Epoch: 10400 Loss: 2009.970703125 Validation loss: 1990.476318359375 best Validation loss: 1988.257080078125\n",
      "Epoch: 10500 Loss: 2008.6678466796875 Validation loss: 1990.8299560546875 best Validation loss: 1988.1328125\n",
      "Epoch: 10600 Loss: 2016.1668701171875 Validation loss: 2023.3809814453125 best Validation loss: 1987.465087890625\n",
      "Epoch: 10700 Loss: 2007.9942626953125 Validation loss: 1988.1187744140625 best Validation loss: 1987.0513916015625\n",
      "Epoch: 10800 Loss: 2004.87158203125 Validation loss: 1997.6689453125 best Validation loss: 1986.6314697265625\n",
      "Epoch: 10900 Loss: 2003.9351806640625 Validation loss: 1993.5225830078125 best Validation loss: 1985.7119140625\n",
      "Epoch: 11000 Loss: 2002.8995361328125 Validation loss: 1992.6976318359375 best Validation loss: 1985.448486328125\n",
      "Epoch: 11100 Loss: 2002.0423583984375 Validation loss: 1992.2646484375 best Validation loss: 1985.1572265625\n",
      "Epoch: 11200 Loss: 2001.236572265625 Validation loss: 1991.864501953125 best Validation loss: 1984.852783203125\n",
      "Epoch: 11300 Loss: 2000.450927734375 Validation loss: 1992.5416259765625 best Validation loss: 1984.7052001953125\n",
      "Epoch: 11400 Loss: 1999.78759765625 Validation loss: 1990.3228759765625 best Validation loss: 1984.38916015625\n",
      "Epoch: 11500 Loss: 1999.5048828125 Validation loss: 1987.8077392578125 best Validation loss: 1984.11083984375\n",
      "Epoch: 11600 Loss: 1999.4852294921875 Validation loss: 1985.7506103515625 best Validation loss: 1983.5633544921875\n",
      "Epoch: 11700 Loss: 1999.2781982421875 Validation loss: 1984.9442138671875 best Validation loss: 1983.165771484375\n",
      "Epoch: 11800 Loss: 1999.600341796875 Validation loss: 1984.272705078125 best Validation loss: 1983.165771484375\n",
      "Epoch: 11900 Loss: 1999.5703125 Validation loss: 1984.5008544921875 best Validation loss: 1983.165771484375\n",
      "Epoch: 12000 Loss: 1996.936767578125 Validation loss: 1987.392333984375 best Validation loss: 1983.165771484375\n",
      "Epoch: 12100 Loss: 1997.0057373046875 Validation loss: 1987.2210693359375 best Validation loss: 1983.165771484375\n",
      "Epoch: 12200 Loss: 1993.9881591796875 Validation loss: 1993.2235107421875 best Validation loss: 1983.165771484375\n",
      "Epoch: 12300 Loss: 1994.759033203125 Validation loss: 1989.824951171875 best Validation loss: 1983.165771484375\n",
      "Epoch: 12400 Loss: 1994.945556640625 Validation loss: 1988.7646484375 best Validation loss: 1983.165771484375\n",
      "Epoch: 12500 Loss: 1992.370849609375 Validation loss: 1991.9310302734375 best Validation loss: 1983.165771484375\n",
      "Epoch: 12600 Loss: 1992.8236083984375 Validation loss: 1989.7144775390625 best Validation loss: 1983.165771484375\n",
      "Epoch: 12700 Loss: 2000.103271484375 Validation loss: 1986.8638916015625 best Validation loss: 1983.165771484375\n",
      "Epoch: 12800 Loss: 1991.6937255859375 Validation loss: 1989.6177978515625 best Validation loss: 1983.165771484375\n",
      "Epoch: 12900 Loss: 1996.1121826171875 Validation loss: 1987.2705078125 best Validation loss: 1983.165771484375\n",
      "Epoch: 13000 Loss: 1994.2335205078125 Validation loss: 1987.7730712890625 best Validation loss: 1983.165771484375\n",
      "Epoch: 13100 Loss: 1992.42333984375 Validation loss: 1988.689453125 best Validation loss: 1983.165771484375\n",
      "Epoch: 13200 Loss: 1990.8668212890625 Validation loss: 1989.0933837890625 best Validation loss: 1983.165771484375\n",
      "Epoch: 13300 Loss: 1990.463623046875 Validation loss: 1988.1815185546875 best Validation loss: 1983.165771484375\n",
      "Epoch: 13400 Loss: 1990.173828125 Validation loss: 1987.943115234375 best Validation loss: 1983.165771484375\n",
      "Epoch: 13500 Loss: 1989.5283203125 Validation loss: 1988.8770751953125 best Validation loss: 1983.165771484375\n",
      "Epoch: 13600 Loss: 1985.5057373046875 Validation loss: 1997.646728515625 best Validation loss: 1983.165771484375\n",
      "Epoch: 13700 Loss: 1986.7742919921875 Validation loss: 1990.84375 best Validation loss: 1983.165771484375\n",
      "Epoch: 13800 Loss: 1990.39697265625 Validation loss: 1988.8359375 best Validation loss: 1983.165771484375\n",
      "Epoch: 13900 Loss: 1984.8587646484375 Validation loss: 1992.131103515625 best Validation loss: 1983.165771484375\n",
      "Epoch: 14000 Loss: 1986.473388671875 Validation loss: 2010.1573486328125 best Validation loss: 1983.165771484375\n",
      "Epoch: 14100 Loss: 1986.82373046875 Validation loss: 2012.8121337890625 best Validation loss: 1983.165771484375\n",
      "Epoch: 14200 Loss: 1986.03857421875 Validation loss: 2011.79736328125 best Validation loss: 1983.165771484375\n",
      "Epoch: 14300 Loss: 1991.47216796875 Validation loss: 2027.6346435546875 best Validation loss: 1983.165771484375\n",
      "Epoch: 14400 Loss: 1981.4744873046875 Validation loss: 1995.91015625 best Validation loss: 1983.165771484375\n",
      "Epoch: 14500 Loss: 1981.395263671875 Validation loss: 1993.198486328125 best Validation loss: 1983.165771484375\n",
      "Epoch: 14600 Loss: 1981.3636474609375 Validation loss: 1992.40869140625 best Validation loss: 1983.165771484375\n",
      "Epoch: 14700 Loss: 1984.1484375 Validation loss: 1989.636474609375 best Validation loss: 1983.165771484375\n",
      "Epoch: 14800 Loss: 1980.089111328125 Validation loss: 1999.3223876953125 best Validation loss: 1983.165771484375\n",
      "Epoch: 14900 Loss: 1982.41455078125 Validation loss: 1993.4544677734375 best Validation loss: 1983.165771484375\n",
      "Epoch: 15000 Loss: 1981.923828125 Validation loss: 1991.4296875 best Validation loss: 1983.165771484375\n",
      "Epoch: 15100 Loss: 1978.96142578125 Validation loss: 1995.4620361328125 best Validation loss: 1983.165771484375\n",
      "Epoch: 15200 Loss: 1979.7774658203125 Validation loss: 1993.573974609375 best Validation loss: 1983.165771484375\n",
      "Epoch: 15300 Loss: 1990.294189453125 Validation loss: 2028.4864501953125 best Validation loss: 1983.165771484375\n",
      "Epoch: 15400 Loss: 1980.194580078125 Validation loss: 1991.4664306640625 best Validation loss: 1983.165771484375\n",
      "Epoch: 15500 Loss: 1978.3641357421875 Validation loss: 1991.946533203125 best Validation loss: 1983.165771484375\n",
      "Epoch: 15600 Loss: 1980.4609375 Validation loss: 2010.8736572265625 best Validation loss: 1983.165771484375\n",
      "Epoch: 15700 Loss: 1981.5556640625 Validation loss: 2015.5855712890625 best Validation loss: 1983.165771484375\n",
      "Epoch: 15800 Loss: 1980.7705078125 Validation loss: 2014.95556640625 best Validation loss: 1983.165771484375\n",
      "Epoch: 15900 Loss: 1978.89892578125 Validation loss: 2014.7947998046875 best Validation loss: 1983.165771484375\n",
      "Epoch: 16000 Loss: 1980.093994140625 Validation loss: 2015.678955078125 best Validation loss: 1983.165771484375\n",
      "Epoch: 16100 Loss: 1976.0732421875 Validation loss: 1997.0401611328125 best Validation loss: 1983.165771484375\n",
      "Epoch: 16200 Loss: 1982.722412109375 Validation loss: 1993.6993408203125 best Validation loss: 1983.165771484375\n",
      "Epoch: 16300 Loss: 1975.9525146484375 Validation loss: 1995.5224609375 best Validation loss: 1983.165771484375\n",
      "Epoch: 16400 Loss: 1974.0101318359375 Validation loss: 2002.0025634765625 best Validation loss: 1983.165771484375\n",
      "Epoch: 16500 Loss: 1979.4573974609375 Validation loss: 1993.7576904296875 best Validation loss: 1983.165771484375\n",
      "Epoch: 16600 Loss: 1977.8701171875 Validation loss: 1994.2099609375 best Validation loss: 1983.165771484375\n",
      "Epoch: 16700 Loss: 1973.5887451171875 Validation loss: 2008.92822265625 best Validation loss: 1983.165771484375\n",
      "Epoch: 16800 Loss: 1974.5596923828125 Validation loss: 1997.211181640625 best Validation loss: 1983.165771484375\n",
      "Epoch: 16900 Loss: 1972.83544921875 Validation loss: 2006.3404541015625 best Validation loss: 1983.165771484375\n",
      "Epoch: 17000 Loss: 1972.095703125 Validation loss: 2001.2850341796875 best Validation loss: 1983.165771484375\n",
      "Epoch: 17100 Loss: 1971.9129638671875 Validation loss: 2004.7974853515625 best Validation loss: 1983.165771484375\n",
      "Epoch: 17200 Loss: 1985.10693359375 Validation loss: 2036.584228515625 best Validation loss: 1983.165771484375\n",
      "Epoch: 17300 Loss: 1971.366943359375 Validation loss: 2008.56103515625 best Validation loss: 1983.165771484375\n",
      "Epoch: 17400 Loss: 1970.7227783203125 Validation loss: 2007.0206298828125 best Validation loss: 1983.165771484375\n",
      "Epoch: 17500 Loss: 1970.1209716796875 Validation loss: 2005.220947265625 best Validation loss: 1983.165771484375\n",
      "Epoch: 17600 Loss: 1969.900146484375 Validation loss: 2001.56982421875 best Validation loss: 1983.165771484375\n",
      "Epoch: 17700 Loss: 1973.1336669921875 Validation loss: 1996.7347412109375 best Validation loss: 1983.165771484375\n",
      "Epoch: 17800 Loss: 1971.699462890625 Validation loss: 1998.4755859375 best Validation loss: 1983.165771484375\n",
      "Epoch: 17900 Loss: 1975.816650390625 Validation loss: 1996.50146484375 best Validation loss: 1983.165771484375\n",
      "Epoch: 18000 Loss: 1975.8858642578125 Validation loss: 1996.9278564453125 best Validation loss: 1983.165771484375\n",
      "Epoch: 18100 Loss: 1972.7427978515625 Validation loss: 1997.3525390625 best Validation loss: 1983.165771484375\n",
      "Epoch: 18200 Loss: 1968.045166015625 Validation loss: 2000.9168701171875 best Validation loss: 1983.165771484375\n",
      "Epoch: 18300 Loss: 1967.7288818359375 Validation loss: 2001.460205078125 best Validation loss: 1983.165771484375\n",
      "Epoch: 18400 Loss: 1967.0469970703125 Validation loss: 2003.9525146484375 best Validation loss: 1983.165771484375\n",
      "Epoch: 18500 Loss: 1966.787109375 Validation loss: 2005.607666015625 best Validation loss: 1983.165771484375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(training_loss, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "last_val_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for n in range(NUMBER_OF_EPOCH):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)[:, 0]\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    training_loss = np.append(training_loss, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = model(X_val)[:, 0]\n",
    "    val_loss = loss_fn(y_pred, y_val)\n",
    "    validation_loss = np.append(validation_loss, val_loss.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model, 'mlp_model.pth')\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(f'Epoch: {n} Loss: {loss.item()}'f' Validation loss: {val_loss.item()}'f' best Validation loss: {best_val_loss}')\n",
    "        \n",
    "    if last_val_loss < val_loss:\n",
    "        count += 1\n",
    "        if count == 25:\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "    last_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x315ec4f50>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/cy9kvd894_bfkfb71dsbxt2w0000gn/T/ipykernel_90506/228361571.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mlp_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "model = torch.load('mlp_model.pth')\n",
    "model.eval()\n",
    "\n",
    "index = df['id']\n",
    "df = df.drop(['id'], axis=1)\n",
    "X = pre.encoder(df)\n",
    "X = pre.standardize(X, scaler=pickle.load(open('scaler.pkl', 'rb')))\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_pred = model(X)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(y_pred.detach().numpy(), columns=['price'])\n",
    "df = pd.concat([index, df], axis=1)\n",
    "df.rename(columns={'price': 'answer'}, inplace=True)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
