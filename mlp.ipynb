{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCH = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import preprossesing as pre\n",
    "import math\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(df):\n",
    "    \n",
    "    df = pre.standardize(df)\n",
    "    df = pre.encoder(df)\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    train, val = pre.test_validation_split(df)\n",
    "    \n",
    "    y_train = torch.tensor(train['price'].values, dtype=torch.float32)\n",
    "    X_train = train.drop(['price'], axis=1)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    \n",
    "    y_val = torch.tensor(val['price'].values, dtype=torch.float32)\n",
    "    X_val = val.drop(['price'], axis=1)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (manufacturers): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=143, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (gearbox_type): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fuel_type): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (registration_fees): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (engine_capacity): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(9, 64),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(16, 1) \n",
    "        )\n",
    "        \n",
    "        self.manufacturers = nn.Sequential(\n",
    "        nn.Linear(9, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(143, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.gearbox_type = nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fuel_type = nn.Sequential(\n",
    "        nn.Linear(4, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.registration_fees = nn.Sequential(\n",
    "        nn.Linear(12, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.engine_capacity = nn.Sequential(\n",
    "        nn.Linear(15, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        manufacutre_output = self.manufacturers(x[:, 3:12])\n",
    "        model_output = self.model(x[:, 12:155])\n",
    "        gearbox_output = self.gearbox_type(x[:, 155:158])\n",
    "        fuel_output = self.fuel_type(x[:, 158:162])\n",
    "        registration_fees_output = self.registration_fees(x[:, 162:174])\n",
    "        engine_capacity_output = self.engine_capacity(x[:, 174:189])\n",
    "        x = torch.cat((x[:, :3], manufacutre_output, model_output, gearbox_output, fuel_output, registration_fees_output, engine_capacity_output), 1)\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "model = mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_train, y_train, X_val, y_val = prepare_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 19160.572265625 Validation loss: 19022.7265625 best Validation loss: 19022.7265625\n",
      "Epoch: 100 Loss: 19126.24609375 Validation loss: 18986.7734375 best Validation loss: 18986.7734375\n",
      "Epoch: 200 Loss: 17711.236328125 Validation loss: 17527.416015625 best Validation loss: 17527.416015625\n",
      "Epoch: 300 Loss: 7585.12841796875 Validation loss: 7479.57666015625 best Validation loss: 7479.57666015625\n",
      "Epoch: 400 Loss: 5974.44482421875 Validation loss: 5888.576171875 best Validation loss: 5888.576171875\n",
      "Epoch: 500 Loss: 5062.52294921875 Validation loss: 4964.53271484375 best Validation loss: 4964.53271484375\n",
      "Epoch: 600 Loss: 4541.85986328125 Validation loss: 4436.80712890625 best Validation loss: 4436.80712890625\n",
      "Epoch: 700 Loss: 4224.8447265625 Validation loss: 4114.0576171875 best Validation loss: 4114.0576171875\n",
      "Epoch: 800 Loss: 3973.368408203125 Validation loss: 3856.96923828125 best Validation loss: 3856.96923828125\n",
      "Epoch: 900 Loss: 3747.6904296875 Validation loss: 3628.10693359375 best Validation loss: 3628.10693359375\n",
      "Epoch: 1000 Loss: 3554.671630859375 Validation loss: 3436.84423828125 best Validation loss: 3436.84423828125\n",
      "Epoch: 1100 Loss: 3397.260009765625 Validation loss: 3286.811279296875 best Validation loss: 3286.811279296875\n",
      "Epoch: 1200 Loss: 3287.112548828125 Validation loss: 3184.218505859375 best Validation loss: 3184.218505859375\n",
      "Epoch: 1300 Loss: 3211.282958984375 Validation loss: 3111.624755859375 best Validation loss: 3111.624755859375\n",
      "Epoch: 1400 Loss: 3156.37646484375 Validation loss: 3053.06494140625 best Validation loss: 3053.06494140625\n",
      "Epoch: 1500 Loss: 3114.938720703125 Validation loss: 3006.106689453125 best Validation loss: 3006.106689453125\n",
      "Epoch: 1600 Loss: 3078.28955078125 Validation loss: 2961.16943359375 best Validation loss: 2961.16943359375\n",
      "Epoch: 1700 Loss: 3033.88427734375 Validation loss: 2905.20556640625 best Validation loss: 2905.20556640625\n",
      "Epoch: 1800 Loss: 2989.432373046875 Validation loss: 2849.51416015625 best Validation loss: 2849.51416015625\n",
      "Epoch: 1900 Loss: 2943.806640625 Validation loss: 2792.787353515625 best Validation loss: 2792.787353515625\n",
      "Epoch: 2000 Loss: 2899.8759765625 Validation loss: 2736.169189453125 best Validation loss: 2736.169189453125\n",
      "Epoch: 2100 Loss: 2855.968017578125 Validation loss: 2679.733642578125 best Validation loss: 2679.733642578125\n",
      "Epoch: 2200 Loss: 2816.0888671875 Validation loss: 2626.754150390625 best Validation loss: 2626.754150390625\n",
      "Epoch: 2300 Loss: 2781.32568359375 Validation loss: 2583.917236328125 best Validation loss: 2583.917236328125\n",
      "Epoch: 2400 Loss: 2753.211181640625 Validation loss: 2549.466552734375 best Validation loss: 2549.466552734375\n",
      "Epoch: 2500 Loss: 2729.16748046875 Validation loss: 2526.04443359375 best Validation loss: 2526.04443359375\n",
      "Epoch: 2600 Loss: 2708.728515625 Validation loss: 2504.0498046875 best Validation loss: 2504.0498046875\n",
      "Epoch: 2700 Loss: 2690.648681640625 Validation loss: 2487.1875 best Validation loss: 2487.1875\n",
      "Epoch: 2800 Loss: 2674.062255859375 Validation loss: 2473.31494140625 best Validation loss: 2473.31494140625\n",
      "Epoch: 2900 Loss: 2660.0087890625 Validation loss: 2460.26123046875 best Validation loss: 2460.26123046875\n",
      "Epoch: 3000 Loss: 2647.196044921875 Validation loss: 2449.3115234375 best Validation loss: 2449.3115234375\n",
      "Epoch: 3100 Loss: 2634.50244140625 Validation loss: 2436.4248046875 best Validation loss: 2436.4248046875\n",
      "Epoch: 3200 Loss: 2621.480224609375 Validation loss: 2425.15087890625 best Validation loss: 2424.989990234375\n",
      "Epoch: 3300 Loss: 2609.4345703125 Validation loss: 2417.68408203125 best Validation loss: 2417.68408203125\n",
      "Epoch: 3400 Loss: 2597.98486328125 Validation loss: 2409.092529296875 best Validation loss: 2409.092529296875\n",
      "Epoch: 3500 Loss: 2585.416748046875 Validation loss: 2398.94091796875 best Validation loss: 2398.5634765625\n",
      "Epoch: 3600 Loss: 2570.624267578125 Validation loss: 2389.276611328125 best Validation loss: 2389.276611328125\n",
      "Epoch: 3700 Loss: 2557.43017578125 Validation loss: 2378.14208984375 best Validation loss: 2378.14208984375\n",
      "Epoch: 3800 Loss: 2545.616455078125 Validation loss: 2368.353759765625 best Validation loss: 2368.34716796875\n",
      "Epoch: 3900 Loss: 2532.653564453125 Validation loss: 2365.400634765625 best Validation loss: 2365.02978515625\n",
      "Epoch: 4000 Loss: 2519.809326171875 Validation loss: 2357.27001953125 best Validation loss: 2357.27001953125\n",
      "Epoch: 4100 Loss: 2507.44189453125 Validation loss: 2348.54150390625 best Validation loss: 2348.4033203125\n",
      "Epoch: 4200 Loss: 2495.3408203125 Validation loss: 2339.393310546875 best Validation loss: 2338.849609375\n",
      "Epoch: 4300 Loss: 2483.88427734375 Validation loss: 2332.42919921875 best Validation loss: 2331.9189453125\n",
      "Epoch: 4400 Loss: 2472.689453125 Validation loss: 2325.97314453125 best Validation loss: 2325.97314453125\n",
      "Epoch: 4500 Loss: 2461.419677734375 Validation loss: 2321.068603515625 best Validation loss: 2320.3369140625\n",
      "Epoch: 4600 Loss: 2450.65576171875 Validation loss: 2318.998046875 best Validation loss: 2318.748046875\n",
      "Epoch: 4700 Loss: 2439.916259765625 Validation loss: 2319.95068359375 best Validation loss: 2317.7685546875\n",
      "Epoch: 4800 Loss: 2430.587158203125 Validation loss: 2320.702880859375 best Validation loss: 2317.7685546875\n",
      "Epoch: 4900 Loss: 2423.116455078125 Validation loss: 2317.54443359375 best Validation loss: 2317.40966796875\n",
      "Epoch: 5000 Loss: 2416.344482421875 Validation loss: 2317.195068359375 best Validation loss: 2316.55322265625\n",
      "Epoch: 5100 Loss: 2409.896240234375 Validation loss: 2314.88818359375 best Validation loss: 2314.177734375\n",
      "Epoch: 5200 Loss: 2403.533447265625 Validation loss: 2310.46142578125 best Validation loss: 2310.46142578125\n",
      "Epoch: 5300 Loss: 2397.78466796875 Validation loss: 2310.393310546875 best Validation loss: 2308.81787109375\n",
      "Epoch: 5400 Loss: 2391.28271484375 Validation loss: 2308.437744140625 best Validation loss: 2307.785400390625\n",
      "Epoch: 5500 Loss: 2384.3115234375 Validation loss: 2306.858154296875 best Validation loss: 2303.804443359375\n",
      "Epoch: 5600 Loss: 2377.510498046875 Validation loss: 2304.49658203125 best Validation loss: 2303.150634765625\n",
      "Epoch: 5700 Loss: 2371.630126953125 Validation loss: 2304.68505859375 best Validation loss: 2303.150634765625\n",
      "Epoch: 5800 Loss: 2366.232177734375 Validation loss: 2301.849365234375 best Validation loss: 2301.408447265625\n",
      "Epoch: 5900 Loss: 2361.389892578125 Validation loss: 2297.5185546875 best Validation loss: 2297.5185546875\n",
      "Epoch: 6000 Loss: 2357.111083984375 Validation loss: 2297.10791015625 best Validation loss: 2296.134521484375\n",
      "Epoch: 6100 Loss: 2353.656005859375 Validation loss: 2295.48193359375 best Validation loss: 2295.24658203125\n",
      "Epoch: 6200 Loss: 2350.57421875 Validation loss: 2294.922607421875 best Validation loss: 2294.007080078125\n",
      "Epoch: 6300 Loss: 2347.89111328125 Validation loss: 2294.225341796875 best Validation loss: 2293.783935546875\n",
      "Epoch: 6400 Loss: 2345.615478515625 Validation loss: 2294.36669921875 best Validation loss: 2293.44384765625\n",
      "Epoch: 6500 Loss: 2343.063232421875 Validation loss: 2296.095947265625 best Validation loss: 2293.44384765625\n",
      "Epoch: 6600 Loss: 2340.902099609375 Validation loss: 2296.558837890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 6700 Loss: 2338.596923828125 Validation loss: 2296.76904296875 best Validation loss: 2293.44384765625\n",
      "Epoch: 6800 Loss: 2335.95361328125 Validation loss: 2294.799560546875 best Validation loss: 2293.44384765625\n",
      "Epoch: 6900 Loss: 2333.23486328125 Validation loss: 2295.402587890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 7000 Loss: 2330.802978515625 Validation loss: 2297.068603515625 best Validation loss: 2293.44384765625\n",
      "Epoch: 7100 Loss: 2328.6982421875 Validation loss: 2298.326416015625 best Validation loss: 2293.44384765625\n",
      "Epoch: 7200 Loss: 2326.7119140625 Validation loss: 2300.864501953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 7300 Loss: 2324.7001953125 Validation loss: 2302.73876953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 7400 Loss: 2322.3681640625 Validation loss: 2300.281005859375 best Validation loss: 2293.44384765625\n",
      "Epoch: 7500 Loss: 2320.212646484375 Validation loss: 2301.982421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 7600 Loss: 2318.252197265625 Validation loss: 2302.628173828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 7700 Loss: 2315.92578125 Validation loss: 2300.814208984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 7800 Loss: 2313.631103515625 Validation loss: 2297.281494140625 best Validation loss: 2293.44384765625\n",
      "Epoch: 7900 Loss: 2311.610595703125 Validation loss: 2296.42529296875 best Validation loss: 2293.44384765625\n",
      "Epoch: 8000 Loss: 2309.626953125 Validation loss: 2298.48486328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 8100 Loss: 2307.976806640625 Validation loss: 2298.384765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 8200 Loss: 2306.194091796875 Validation loss: 2299.1552734375 best Validation loss: 2293.44384765625\n",
      "Epoch: 8300 Loss: 2303.967041015625 Validation loss: 2298.75048828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 8400 Loss: 2302.13525390625 Validation loss: 2299.33837890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 8500 Loss: 2300.3798828125 Validation loss: 2297.790771484375 best Validation loss: 2293.44384765625\n",
      "Epoch: 8600 Loss: 2298.7548828125 Validation loss: 2298.458251953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 8700 Loss: 2297.196044921875 Validation loss: 2297.855224609375 best Validation loss: 2293.44384765625\n",
      "Epoch: 8800 Loss: 2295.7607421875 Validation loss: 2298.049072265625 best Validation loss: 2293.44384765625\n",
      "Epoch: 8900 Loss: 2294.398193359375 Validation loss: 2298.227294921875 best Validation loss: 2293.44384765625\n",
      "Epoch: 9000 Loss: 2293.00439453125 Validation loss: 2298.7216796875 best Validation loss: 2293.44384765625\n",
      "Epoch: 9100 Loss: 2291.5478515625 Validation loss: 2298.017333984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 9200 Loss: 2290.22802734375 Validation loss: 2299.044677734375 best Validation loss: 2293.44384765625\n",
      "Epoch: 9300 Loss: 2288.577880859375 Validation loss: 2298.563232421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 9400 Loss: 2287.122802734375 Validation loss: 2298.868896484375 best Validation loss: 2293.44384765625\n",
      "Epoch: 9500 Loss: 2285.6806640625 Validation loss: 2301.269287109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 9600 Loss: 2284.311767578125 Validation loss: 2300.951171875 best Validation loss: 2293.44384765625\n",
      "Epoch: 9700 Loss: 2282.822265625 Validation loss: 2301.639892578125 best Validation loss: 2293.44384765625\n",
      "Epoch: 9800 Loss: 2281.620361328125 Validation loss: 2300.9150390625 best Validation loss: 2293.44384765625\n",
      "Epoch: 9900 Loss: 2280.454345703125 Validation loss: 2301.311767578125 best Validation loss: 2293.44384765625\n",
      "Epoch: 10000 Loss: 2279.143798828125 Validation loss: 2301.49365234375 best Validation loss: 2293.44384765625\n",
      "Epoch: 10100 Loss: 2278.046142578125 Validation loss: 2301.33837890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 10200 Loss: 2276.785400390625 Validation loss: 2303.42431640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 10300 Loss: 2275.68310546875 Validation loss: 2303.495361328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 10400 Loss: 2274.578857421875 Validation loss: 2303.589599609375 best Validation loss: 2293.44384765625\n",
      "Epoch: 10500 Loss: 2273.524169921875 Validation loss: 2303.700439453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 10600 Loss: 2272.5771484375 Validation loss: 2303.832763671875 best Validation loss: 2293.44384765625\n",
      "Epoch: 10700 Loss: 2271.246337890625 Validation loss: 2305.880126953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 10800 Loss: 2269.64208984375 Validation loss: 2301.708984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 10900 Loss: 2268.4013671875 Validation loss: 2301.853759765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 11000 Loss: 2267.1552734375 Validation loss: 2300.326171875 best Validation loss: 2293.44384765625\n",
      "Epoch: 11100 Loss: 2266.15234375 Validation loss: 2300.0556640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 11200 Loss: 2264.876708984375 Validation loss: 2300.7421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 11300 Loss: 2263.608642578125 Validation loss: 2301.33740234375 best Validation loss: 2293.44384765625\n",
      "Epoch: 11400 Loss: 2262.60693359375 Validation loss: 2303.989990234375 best Validation loss: 2293.44384765625\n",
      "Epoch: 11500 Loss: 2261.176513671875 Validation loss: 2303.47998046875 best Validation loss: 2293.44384765625\n",
      "Epoch: 11600 Loss: 2259.96484375 Validation loss: 2305.716064453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 11700 Loss: 2258.8203125 Validation loss: 2302.47314453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 11800 Loss: 2256.851806640625 Validation loss: 2301.613037109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 11900 Loss: 2254.92138671875 Validation loss: 2302.939453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 12000 Loss: 2252.6044921875 Validation loss: 2301.6923828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 12100 Loss: 2250.757080078125 Validation loss: 2303.96484375 best Validation loss: 2293.44384765625\n",
      "Epoch: 12200 Loss: 2248.946533203125 Validation loss: 2305.615234375 best Validation loss: 2293.44384765625\n",
      "Epoch: 12300 Loss: 2247.30419921875 Validation loss: 2305.30126953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 12400 Loss: 2245.881591796875 Validation loss: 2307.58984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 12500 Loss: 2243.971923828125 Validation loss: 2308.017578125 best Validation loss: 2293.44384765625\n",
      "Epoch: 12600 Loss: 2242.26904296875 Validation loss: 2306.998046875 best Validation loss: 2293.44384765625\n",
      "Epoch: 12700 Loss: 2240.845703125 Validation loss: 2307.140869140625 best Validation loss: 2293.44384765625\n",
      "Epoch: 12800 Loss: 2239.671630859375 Validation loss: 2306.64697265625 best Validation loss: 2293.44384765625\n",
      "Epoch: 12900 Loss: 2238.034912109375 Validation loss: 2308.891845703125 best Validation loss: 2293.44384765625\n",
      "Epoch: 13000 Loss: 2236.328857421875 Validation loss: 2308.235595703125 best Validation loss: 2293.44384765625\n",
      "Epoch: 13100 Loss: 2234.74267578125 Validation loss: 2308.6396484375 best Validation loss: 2293.44384765625\n",
      "Epoch: 13200 Loss: 2233.524658203125 Validation loss: 2308.423583984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 13300 Loss: 2232.328369140625 Validation loss: 2311.228759765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 13400 Loss: 2230.93115234375 Validation loss: 2309.36083984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 13500 Loss: 2229.7568359375 Validation loss: 2311.2412109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 13600 Loss: 2228.465087890625 Validation loss: 2311.5732421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 13700 Loss: 2227.25244140625 Validation loss: 2310.840576171875 best Validation loss: 2293.44384765625\n",
      "Epoch: 13800 Loss: 2225.992919921875 Validation loss: 2311.14453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 13900 Loss: 2224.962158203125 Validation loss: 2312.058349609375 best Validation loss: 2293.44384765625\n",
      "Epoch: 14000 Loss: 2223.997802734375 Validation loss: 2309.24755859375 best Validation loss: 2293.44384765625\n",
      "Epoch: 14100 Loss: 2222.72265625 Validation loss: 2312.983154296875 best Validation loss: 2293.44384765625\n",
      "Epoch: 14200 Loss: 2221.838623046875 Validation loss: 2309.67236328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 14300 Loss: 2220.78515625 Validation loss: 2313.728515625 best Validation loss: 2293.44384765625\n",
      "Epoch: 14400 Loss: 2219.5390625 Validation loss: 2312.669921875 best Validation loss: 2293.44384765625\n",
      "Epoch: 14500 Loss: 2218.629638671875 Validation loss: 2313.13916015625 best Validation loss: 2293.44384765625\n",
      "Epoch: 14600 Loss: 2217.59619140625 Validation loss: 2313.0205078125 best Validation loss: 2293.44384765625\n",
      "Epoch: 14700 Loss: 2216.487548828125 Validation loss: 2314.035888671875 best Validation loss: 2293.44384765625\n",
      "Epoch: 14800 Loss: 2215.43603515625 Validation loss: 2313.003662109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 14900 Loss: 2214.5888671875 Validation loss: 2311.3681640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 15000 Loss: 2213.49365234375 Validation loss: 2315.8759765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 15100 Loss: 2212.284912109375 Validation loss: 2315.441650390625 best Validation loss: 2293.44384765625\n",
      "Epoch: 15200 Loss: 2211.114990234375 Validation loss: 2316.19873046875 best Validation loss: 2293.44384765625\n",
      "Epoch: 15300 Loss: 2210.3251953125 Validation loss: 2314.48486328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 15400 Loss: 2209.112060546875 Validation loss: 2313.96044921875 best Validation loss: 2293.44384765625\n",
      "Epoch: 15500 Loss: 2206.4072265625 Validation loss: 2309.135986328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 15600 Loss: 2204.433349609375 Validation loss: 2306.030029296875 best Validation loss: 2293.44384765625\n",
      "Epoch: 15700 Loss: 2202.873291015625 Validation loss: 2308.352783203125 best Validation loss: 2293.44384765625\n",
      "Epoch: 15800 Loss: 2201.235595703125 Validation loss: 2308.563232421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 15900 Loss: 2199.70068359375 Validation loss: 2310.4033203125 best Validation loss: 2293.44384765625\n",
      "Epoch: 16000 Loss: 2198.631103515625 Validation loss: 2312.995849609375 best Validation loss: 2293.44384765625\n",
      "Epoch: 16100 Loss: 2196.983154296875 Validation loss: 2310.95361328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 16200 Loss: 2195.602294921875 Validation loss: 2312.496337890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 16300 Loss: 2194.396728515625 Validation loss: 2312.426025390625 best Validation loss: 2293.44384765625\n",
      "Epoch: 16400 Loss: 2192.8974609375 Validation loss: 2312.873046875 best Validation loss: 2293.44384765625\n",
      "Epoch: 16500 Loss: 2191.77587890625 Validation loss: 2314.799560546875 best Validation loss: 2293.44384765625\n",
      "Epoch: 16600 Loss: 2190.534423828125 Validation loss: 2312.285400390625 best Validation loss: 2293.44384765625\n",
      "Epoch: 16700 Loss: 2189.1728515625 Validation loss: 2311.9541015625 best Validation loss: 2293.44384765625\n",
      "Epoch: 16800 Loss: 2188.275634765625 Validation loss: 2310.708984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 16900 Loss: 2187.087646484375 Validation loss: 2311.708251953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 17000 Loss: 2185.870361328125 Validation loss: 2312.603759765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 17100 Loss: 2184.70849609375 Validation loss: 2312.242431640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 17200 Loss: 2183.6826171875 Validation loss: 2313.419189453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 17300 Loss: 2182.3974609375 Validation loss: 2314.15771484375 best Validation loss: 2293.44384765625\n",
      "Epoch: 17400 Loss: 2181.10498046875 Validation loss: 2312.670654296875 best Validation loss: 2293.44384765625\n",
      "Epoch: 17500 Loss: 2179.810546875 Validation loss: 2311.095458984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 17600 Loss: 2178.616455078125 Validation loss: 2315.072509765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 17700 Loss: 2177.107421875 Validation loss: 2313.984130859375 best Validation loss: 2293.44384765625\n",
      "Epoch: 17800 Loss: 2176.229736328125 Validation loss: 2312.541259765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 17900 Loss: 2175.06787109375 Validation loss: 2315.1181640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 18000 Loss: 2173.91796875 Validation loss: 2314.989990234375 best Validation loss: 2293.44384765625\n",
      "Epoch: 18100 Loss: 2173.0087890625 Validation loss: 2314.3701171875 best Validation loss: 2293.44384765625\n",
      "Epoch: 18200 Loss: 2171.384521484375 Validation loss: 2313.475830078125 best Validation loss: 2293.44384765625\n",
      "Epoch: 18300 Loss: 2170.728271484375 Validation loss: 2316.97607421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 18400 Loss: 2168.98291015625 Validation loss: 2313.18408203125 best Validation loss: 2293.44384765625\n",
      "Epoch: 18500 Loss: 2167.783935546875 Validation loss: 2315.945068359375 best Validation loss: 2293.44384765625\n",
      "Epoch: 18600 Loss: 2166.42138671875 Validation loss: 2315.20458984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 18700 Loss: 2165.47509765625 Validation loss: 2316.2333984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 18800 Loss: 2164.206298828125 Validation loss: 2315.735595703125 best Validation loss: 2293.44384765625\n",
      "Epoch: 18900 Loss: 2163.234375 Validation loss: 2316.62890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 19000 Loss: 2162.19287109375 Validation loss: 2314.870849609375 best Validation loss: 2293.44384765625\n",
      "Epoch: 19100 Loss: 2161.088623046875 Validation loss: 2315.347412109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 19200 Loss: 2160.18115234375 Validation loss: 2316.681884765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 19300 Loss: 2159.2392578125 Validation loss: 2313.147705078125 best Validation loss: 2293.44384765625\n",
      "Epoch: 19400 Loss: 2158.251708984375 Validation loss: 2313.6376953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 19500 Loss: 2157.00048828125 Validation loss: 2312.9150390625 best Validation loss: 2293.44384765625\n",
      "Epoch: 19600 Loss: 2156.029296875 Validation loss: 2313.52880859375 best Validation loss: 2293.44384765625\n",
      "Epoch: 19700 Loss: 2154.965087890625 Validation loss: 2312.800537109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 19800 Loss: 2154.072509765625 Validation loss: 2313.90966796875 best Validation loss: 2293.44384765625\n",
      "Epoch: 19900 Loss: 2153.317138671875 Validation loss: 2311.27392578125 best Validation loss: 2293.44384765625\n",
      "Epoch: 20000 Loss: 2151.99658203125 Validation loss: 2313.838134765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 20100 Loss: 2150.8408203125 Validation loss: 2311.85791015625 best Validation loss: 2293.44384765625\n",
      "Epoch: 20200 Loss: 2149.572265625 Validation loss: 2311.247314453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 20300 Loss: 2148.120361328125 Validation loss: 2310.62548828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 20400 Loss: 2147.51953125 Validation loss: 2310.992431640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 20500 Loss: 2145.722900390625 Validation loss: 2311.2958984375 best Validation loss: 2293.44384765625\n",
      "Epoch: 20600 Loss: 2144.49462890625 Validation loss: 2311.67822265625 best Validation loss: 2293.44384765625\n",
      "Epoch: 20700 Loss: 2143.89208984375 Validation loss: 2311.45751953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 20800 Loss: 2142.478515625 Validation loss: 2309.123046875 best Validation loss: 2293.44384765625\n",
      "Epoch: 20900 Loss: 2141.604736328125 Validation loss: 2310.7890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 21000 Loss: 2140.27587890625 Validation loss: 2310.080078125 best Validation loss: 2293.44384765625\n",
      "Epoch: 21100 Loss: 2139.13330078125 Validation loss: 2308.80078125 best Validation loss: 2293.44384765625\n",
      "Epoch: 21200 Loss: 2138.0361328125 Validation loss: 2309.5673828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 21300 Loss: 2137.074462890625 Validation loss: 2308.4189453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 21400 Loss: 2136.310302734375 Validation loss: 2310.215087890625 best Validation loss: 2293.44384765625\n",
      "Epoch: 21500 Loss: 2135.946044921875 Validation loss: 2309.29296875 best Validation loss: 2293.44384765625\n",
      "Epoch: 21600 Loss: 2134.22021484375 Validation loss: 2306.72998046875 best Validation loss: 2293.44384765625\n",
      "Epoch: 21700 Loss: 2133.245849609375 Validation loss: 2307.037109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 21800 Loss: 2132.08203125 Validation loss: 2307.19091796875 best Validation loss: 2293.44384765625\n",
      "Epoch: 21900 Loss: 2131.003173828125 Validation loss: 2305.31689453125 best Validation loss: 2293.44384765625\n",
      "Epoch: 22000 Loss: 2129.55322265625 Validation loss: 2304.138671875 best Validation loss: 2293.44384765625\n",
      "Epoch: 22100 Loss: 2130.471923828125 Validation loss: 2302.69384765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 22200 Loss: 2127.371826171875 Validation loss: 2303.70556640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 22300 Loss: 2126.361572265625 Validation loss: 2303.1884765625 best Validation loss: 2293.44384765625\n",
      "Epoch: 22400 Loss: 2125.216796875 Validation loss: 2304.520751953125 best Validation loss: 2293.44384765625\n",
      "Epoch: 22500 Loss: 2124.180419921875 Validation loss: 2303.435546875 best Validation loss: 2293.44384765625\n",
      "Epoch: 22600 Loss: 2122.946533203125 Validation loss: 2302.43603515625 best Validation loss: 2293.44384765625\n",
      "Epoch: 22700 Loss: 2121.906005859375 Validation loss: 2303.0107421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 22800 Loss: 2121.41259765625 Validation loss: 2302.155517578125 best Validation loss: 2293.44384765625\n",
      "Epoch: 22900 Loss: 2120.080322265625 Validation loss: 2302.831298828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 23000 Loss: 2118.937255859375 Validation loss: 2302.35107421875 best Validation loss: 2293.44384765625\n",
      "Epoch: 23100 Loss: 2118.0498046875 Validation loss: 2300.7109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 23200 Loss: 2117.332763671875 Validation loss: 2302.679931640625 best Validation loss: 2293.44384765625\n",
      "Epoch: 23300 Loss: 2115.9384765625 Validation loss: 2300.362548828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 23400 Loss: 2115.36328125 Validation loss: 2300.048828125 best Validation loss: 2293.44384765625\n",
      "Epoch: 23500 Loss: 2114.752197265625 Validation loss: 2304.290283203125 best Validation loss: 2293.44384765625\n",
      "Epoch: 23600 Loss: 2113.277099609375 Validation loss: 2299.91015625 best Validation loss: 2293.44384765625\n",
      "Epoch: 23700 Loss: 2112.403564453125 Validation loss: 2298.769287109375 best Validation loss: 2293.44384765625\n",
      "Epoch: 23800 Loss: 2110.855224609375 Validation loss: 2298.04443359375 best Validation loss: 2293.44384765625\n",
      "Epoch: 23900 Loss: 2109.525390625 Validation loss: 2297.826416015625 best Validation loss: 2293.44384765625\n",
      "Epoch: 24000 Loss: 2108.396240234375 Validation loss: 2297.640380859375 best Validation loss: 2293.44384765625\n",
      "Epoch: 24100 Loss: 2107.886962890625 Validation loss: 2297.263671875 best Validation loss: 2293.44384765625\n",
      "Epoch: 24200 Loss: 2106.192626953125 Validation loss: 2296.542724609375 best Validation loss: 2293.44384765625\n",
      "Epoch: 24300 Loss: 2104.685791015625 Validation loss: 2296.209228515625 best Validation loss: 2293.44384765625\n",
      "Epoch: 24400 Loss: 2103.34326171875 Validation loss: 2297.111328125 best Validation loss: 2293.44384765625\n",
      "Epoch: 24500 Loss: 2102.05712890625 Validation loss: 2294.197021484375 best Validation loss: 2293.22216796875\n",
      "Epoch: 24600 Loss: 2101.56201171875 Validation loss: 2293.62060546875 best Validation loss: 2293.04443359375\n",
      "Epoch: 24700 Loss: 2100.828125 Validation loss: 2293.60791015625 best Validation loss: 2293.02734375\n",
      "Epoch: 24800 Loss: 2099.904296875 Validation loss: 2295.83447265625 best Validation loss: 2293.02734375\n",
      "Epoch: 24900 Loss: 2097.993896484375 Validation loss: 2295.29541015625 best Validation loss: 2293.02734375\n",
      "Epoch: 25000 Loss: 2097.37548828125 Validation loss: 2293.58203125 best Validation loss: 2292.9765625\n",
      "Epoch: 25100 Loss: 2096.419677734375 Validation loss: 2296.730712890625 best Validation loss: 2292.9765625\n",
      "Epoch: 25200 Loss: 2094.97265625 Validation loss: 2293.673095703125 best Validation loss: 2292.75634765625\n",
      "Epoch: 25300 Loss: 2093.381591796875 Validation loss: 2294.122802734375 best Validation loss: 2292.373291015625\n",
      "Epoch: 25400 Loss: 2092.72802734375 Validation loss: 2294.634521484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 25500 Loss: 2091.966552734375 Validation loss: 2292.96435546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 25600 Loss: 2090.654052734375 Validation loss: 2294.079345703125 best Validation loss: 2291.566650390625\n",
      "Epoch: 25700 Loss: 2089.801513671875 Validation loss: 2293.08203125 best Validation loss: 2291.566650390625\n",
      "Epoch: 25800 Loss: 2089.3310546875 Validation loss: 2292.535888671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 25900 Loss: 2088.34375 Validation loss: 2296.28759765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 26000 Loss: 2086.817138671875 Validation loss: 2293.629638671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 26100 Loss: 2086.01171875 Validation loss: 2294.354248046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 26200 Loss: 2085.39013671875 Validation loss: 2296.36279296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 26300 Loss: 2084.04638671875 Validation loss: 2294.3759765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 26400 Loss: 2083.671142578125 Validation loss: 2293.23046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 26500 Loss: 2082.2158203125 Validation loss: 2293.17041015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 26600 Loss: 2081.3359375 Validation loss: 2293.79296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 26700 Loss: 2080.5400390625 Validation loss: 2293.588134765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 26800 Loss: 2079.669921875 Validation loss: 2293.222412109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 26900 Loss: 2078.689697265625 Validation loss: 2294.654052734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 27000 Loss: 2077.689208984375 Validation loss: 2294.5830078125 best Validation loss: 2291.566650390625\n",
      "Epoch: 27100 Loss: 2077.508544921875 Validation loss: 2293.586669921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 27200 Loss: 2076.05517578125 Validation loss: 2295.427734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 27300 Loss: 2075.198486328125 Validation loss: 2296.763671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 27400 Loss: 2074.32275390625 Validation loss: 2295.140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 27500 Loss: 2074.1875 Validation loss: 2299.47509765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 27600 Loss: 2072.428466796875 Validation loss: 2296.330810546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 27700 Loss: 2072.153564453125 Validation loss: 2298.51806640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 27800 Loss: 2071.41162109375 Validation loss: 2298.614990234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 27900 Loss: 2070.655029296875 Validation loss: 2295.582763671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 28000 Loss: 2069.445556640625 Validation loss: 2298.508544921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 28100 Loss: 2068.4287109375 Validation loss: 2298.18798828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 28200 Loss: 2067.795654296875 Validation loss: 2296.512939453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 28300 Loss: 2066.744384765625 Validation loss: 2296.514404296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 28400 Loss: 2066.85888671875 Validation loss: 2296.38818359375 best Validation loss: 2291.566650390625\n",
      "Epoch: 28500 Loss: 2065.115478515625 Validation loss: 2297.0537109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 28600 Loss: 2064.263916015625 Validation loss: 2297.871826171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 28700 Loss: 2064.11865234375 Validation loss: 2296.84326171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 28800 Loss: 2062.80419921875 Validation loss: 2296.16162109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 28900 Loss: 2061.588134765625 Validation loss: 2296.30908203125 best Validation loss: 2291.566650390625\n",
      "Epoch: 29000 Loss: 2061.15234375 Validation loss: 2295.57958984375 best Validation loss: 2291.566650390625\n",
      "Epoch: 29100 Loss: 2059.797607421875 Validation loss: 2296.2421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 29200 Loss: 2059.404541015625 Validation loss: 2297.220947265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 29300 Loss: 2058.1015625 Validation loss: 2298.63818359375 best Validation loss: 2291.566650390625\n",
      "Epoch: 29400 Loss: 2058.183349609375 Validation loss: 2296.92626953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 29500 Loss: 2056.854248046875 Validation loss: 2297.96630859375 best Validation loss: 2291.566650390625\n",
      "Epoch: 29600 Loss: 2055.88330078125 Validation loss: 2299.309814453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 29700 Loss: 2054.70849609375 Validation loss: 2297.9697265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 29800 Loss: 2054.509521484375 Validation loss: 2297.245361328125 best Validation loss: 2291.566650390625\n",
      "Epoch: 29900 Loss: 2053.07666015625 Validation loss: 2297.498291015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 30000 Loss: 2052.221923828125 Validation loss: 2296.956298828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 30100 Loss: 2051.853271484375 Validation loss: 2301.3037109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 30200 Loss: 2050.451416015625 Validation loss: 2299.090576171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 30300 Loss: 2049.48876953125 Validation loss: 2300.34765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 30400 Loss: 2048.97265625 Validation loss: 2300.0400390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 30500 Loss: 2047.9169921875 Validation loss: 2299.668701171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 30600 Loss: 2048.684326171875 Validation loss: 2303.485107421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 30700 Loss: 2048.587158203125 Validation loss: 2303.04052734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 30800 Loss: 2046.289794921875 Validation loss: 2301.535888671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 30900 Loss: 2044.802734375 Validation loss: 2299.21337890625 best Validation loss: 2291.566650390625\n",
      "Epoch: 31000 Loss: 2044.847412109375 Validation loss: 2302.05712890625 best Validation loss: 2291.566650390625\n",
      "Epoch: 31100 Loss: 2043.1536865234375 Validation loss: 2298.950927734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 31200 Loss: 2043.703857421875 Validation loss: 2298.521240234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 31300 Loss: 2041.257568359375 Validation loss: 2298.860107421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 31400 Loss: 2041.966064453125 Validation loss: 2301.975830078125 best Validation loss: 2291.566650390625\n",
      "Epoch: 31500 Loss: 2039.9820556640625 Validation loss: 2297.82568359375 best Validation loss: 2291.566650390625\n",
      "Epoch: 31600 Loss: 2038.71923828125 Validation loss: 2299.13427734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 31700 Loss: 2038.4725341796875 Validation loss: 2300.82568359375 best Validation loss: 2291.566650390625\n",
      "Epoch: 31800 Loss: 2037.485107421875 Validation loss: 2298.48974609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 31900 Loss: 2036.9649658203125 Validation loss: 2297.849365234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 32000 Loss: 2035.79248046875 Validation loss: 2299.30322265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 32100 Loss: 2034.91259765625 Validation loss: 2299.18115234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 32200 Loss: 2034.83349609375 Validation loss: 2302.527099609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 32300 Loss: 2033.545166015625 Validation loss: 2299.1513671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 32400 Loss: 2032.504150390625 Validation loss: 2300.59326171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 32500 Loss: 2032.805908203125 Validation loss: 2302.973388671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 32600 Loss: 2031.803955078125 Validation loss: 2301.373046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 32700 Loss: 2030.7135009765625 Validation loss: 2304.009765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 32800 Loss: 2029.532958984375 Validation loss: 2300.4892578125 best Validation loss: 2291.566650390625\n",
      "Epoch: 32900 Loss: 2029.609130859375 Validation loss: 2301.224609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 33000 Loss: 2028.291015625 Validation loss: 2302.802001953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 33100 Loss: 2026.95556640625 Validation loss: 2304.281982421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 33200 Loss: 2027.2215576171875 Validation loss: 2304.100341796875 best Validation loss: 2291.566650390625\n",
      "Epoch: 33300 Loss: 2026.34765625 Validation loss: 2305.110595703125 best Validation loss: 2291.566650390625\n",
      "Epoch: 33400 Loss: 2025.3199462890625 Validation loss: 2308.736572265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 33500 Loss: 2024.0963134765625 Validation loss: 2308.17822265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 33600 Loss: 2023.0423583984375 Validation loss: 2308.5546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 33700 Loss: 2022.7724609375 Validation loss: 2307.86572265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 33800 Loss: 2022.6922607421875 Validation loss: 2309.051513671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 33900 Loss: 2021.673828125 Validation loss: 2309.455810546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 34000 Loss: 2020.5445556640625 Validation loss: 2312.38671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 34100 Loss: 2019.405029296875 Validation loss: 2312.59130859375 best Validation loss: 2291.566650390625\n",
      "Epoch: 34200 Loss: 2019.2232666015625 Validation loss: 2312.28662109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 34300 Loss: 2018.0902099609375 Validation loss: 2315.270751953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 34400 Loss: 2017.859375 Validation loss: 2315.895751953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 34500 Loss: 2016.6365966796875 Validation loss: 2312.5068359375 best Validation loss: 2291.566650390625\n",
      "Epoch: 34600 Loss: 2016.00244140625 Validation loss: 2313.51025390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 34700 Loss: 2016.215087890625 Validation loss: 2318.851806640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 34800 Loss: 2014.840576171875 Validation loss: 2318.593017578125 best Validation loss: 2291.566650390625\n",
      "Epoch: 34900 Loss: 2013.1114501953125 Validation loss: 2315.156982421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 35000 Loss: 2012.3363037109375 Validation loss: 2315.68408203125 best Validation loss: 2291.566650390625\n",
      "Epoch: 35100 Loss: 2013.0816650390625 Validation loss: 2319.683837890625 best Validation loss: 2291.566650390625\n",
      "Epoch: 35200 Loss: 2010.4398193359375 Validation loss: 2317.720947265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 35300 Loss: 2009.818115234375 Validation loss: 2316.82177734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 35400 Loss: 2009.785888671875 Validation loss: 2321.723388671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 35500 Loss: 2008.1475830078125 Validation loss: 2321.781005859375 best Validation loss: 2291.566650390625\n",
      "Epoch: 35600 Loss: 2006.3743896484375 Validation loss: 2318.739501953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 35700 Loss: 2005.240234375 Validation loss: 2317.31884765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 35800 Loss: 2004.787353515625 Validation loss: 2320.194091796875 best Validation loss: 2291.566650390625\n",
      "Epoch: 35900 Loss: 2002.6474609375 Validation loss: 2316.2685546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 36000 Loss: 2000.9195556640625 Validation loss: 2313.00830078125 best Validation loss: 2291.566650390625\n",
      "Epoch: 36100 Loss: 2001.122802734375 Validation loss: 2312.5517578125 best Validation loss: 2291.566650390625\n",
      "Epoch: 36200 Loss: 1998.8287353515625 Validation loss: 2314.470703125 best Validation loss: 2291.566650390625\n",
      "Epoch: 36300 Loss: 1998.179443359375 Validation loss: 2313.780029296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 36400 Loss: 1998.1297607421875 Validation loss: 2309.07275390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 36500 Loss: 1995.6787109375 Validation loss: 2311.179931640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 36600 Loss: 1994.6676025390625 Validation loss: 2309.911376953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 36700 Loss: 1993.7840576171875 Validation loss: 2308.682373046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 36800 Loss: 1992.7786865234375 Validation loss: 2310.086181640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 36900 Loss: 1995.5181884765625 Validation loss: 2315.219482421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 37000 Loss: 1990.9913330078125 Validation loss: 2310.119384765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 37100 Loss: 1990.0338134765625 Validation loss: 2308.777099609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 37200 Loss: 1989.1661376953125 Validation loss: 2308.38671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 37300 Loss: 1988.4246826171875 Validation loss: 2308.69189453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 37400 Loss: 1988.354736328125 Validation loss: 2311.138916015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 37500 Loss: 1986.6531982421875 Validation loss: 2307.50341796875 best Validation loss: 2291.566650390625\n",
      "Epoch: 37600 Loss: 1986.38671875 Validation loss: 2310.39501953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 37700 Loss: 1985.356689453125 Validation loss: 2307.331787109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 37800 Loss: 1984.2847900390625 Validation loss: 2306.36669921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 37900 Loss: 1984.0379638671875 Validation loss: 2309.79736328125 best Validation loss: 2291.566650390625\n",
      "Epoch: 38000 Loss: 1985.8441162109375 Validation loss: 2306.13623046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 38100 Loss: 1981.68505859375 Validation loss: 2307.493896484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 38200 Loss: 1982.9456787109375 Validation loss: 2305.99462890625 best Validation loss: 2291.566650390625\n",
      "Epoch: 38300 Loss: 1980.2613525390625 Validation loss: 2308.6884765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 38400 Loss: 1978.931884765625 Validation loss: 2307.514892578125 best Validation loss: 2291.566650390625\n",
      "Epoch: 38500 Loss: 1978.0164794921875 Validation loss: 2307.57080078125 best Validation loss: 2291.566650390625\n",
      "Epoch: 38600 Loss: 1977.3365478515625 Validation loss: 2308.158447265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 38700 Loss: 1977.4649658203125 Validation loss: 2307.51513671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 38800 Loss: 1975.7569580078125 Validation loss: 2307.634521484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 38900 Loss: 1974.450439453125 Validation loss: 2309.817626953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 39000 Loss: 1973.6094970703125 Validation loss: 2309.42724609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 39100 Loss: 1975.0958251953125 Validation loss: 2313.888671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 39200 Loss: 1972.557373046875 Validation loss: 2308.894775390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 39300 Loss: 1971.92431640625 Validation loss: 2313.1572265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 39400 Loss: 1970.78564453125 Validation loss: 2311.9267578125 best Validation loss: 2291.566650390625\n",
      "Epoch: 39500 Loss: 1969.4527587890625 Validation loss: 2308.60791015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 39600 Loss: 1968.5755615234375 Validation loss: 2310.0810546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 39700 Loss: 1968.7188720703125 Validation loss: 2312.84423828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 39800 Loss: 1966.9580078125 Validation loss: 2311.06982421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 39900 Loss: 1966.58447265625 Validation loss: 2314.323974609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 40000 Loss: 1966.5394287109375 Validation loss: 2315.47509765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 40100 Loss: 1964.7752685546875 Validation loss: 2309.931884765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 40200 Loss: 1964.630615234375 Validation loss: 2309.69677734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 40300 Loss: 1962.8267822265625 Validation loss: 2311.120361328125 best Validation loss: 2291.566650390625\n",
      "Epoch: 40400 Loss: 1962.7099609375 Validation loss: 2310.380126953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 40500 Loss: 1965.4007568359375 Validation loss: 2319.91796875 best Validation loss: 2291.566650390625\n",
      "Epoch: 40600 Loss: 1960.7716064453125 Validation loss: 2309.833740234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 40700 Loss: 1960.9027099609375 Validation loss: 2310.22119140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 40800 Loss: 1959.0286865234375 Validation loss: 2315.739501953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 40900 Loss: 1957.431640625 Validation loss: 2310.731201171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 41000 Loss: 1958.0767822265625 Validation loss: 2309.589599609375 best Validation loss: 2291.566650390625\n",
      "Epoch: 41100 Loss: 1955.962646484375 Validation loss: 2310.361328125 best Validation loss: 2291.566650390625\n",
      "Epoch: 41200 Loss: 1954.253662109375 Validation loss: 2310.962158203125 best Validation loss: 2291.566650390625\n",
      "Epoch: 41300 Loss: 1957.9520263671875 Validation loss: 2311.780517578125 best Validation loss: 2291.566650390625\n",
      "Epoch: 41400 Loss: 1953.493896484375 Validation loss: 2310.484130859375 best Validation loss: 2291.566650390625\n",
      "Epoch: 41500 Loss: 1953.42236328125 Validation loss: 2316.94091796875 best Validation loss: 2291.566650390625\n",
      "Epoch: 41600 Loss: 1950.8065185546875 Validation loss: 2313.751708984375 best Validation loss: 2291.566650390625\n",
      "Epoch: 41700 Loss: 1952.07080078125 Validation loss: 2311.6494140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 41800 Loss: 1949.12548828125 Validation loss: 2317.43310546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 41900 Loss: 1948.1573486328125 Validation loss: 2315.336181640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 42000 Loss: 1947.3934326171875 Validation loss: 2316.67822265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 42100 Loss: 1948.485107421875 Validation loss: 2314.7265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 42200 Loss: 1947.4290771484375 Validation loss: 2322.577880859375 best Validation loss: 2291.566650390625\n",
      "Epoch: 42300 Loss: 1945.8743896484375 Validation loss: 2320.16357421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 42400 Loss: 1945.465576171875 Validation loss: 2318.0693359375 best Validation loss: 2291.566650390625\n",
      "Epoch: 42500 Loss: 1944.02685546875 Validation loss: 2316.426513671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 42600 Loss: 1943.4427490234375 Validation loss: 2317.150146484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 42700 Loss: 1942.2100830078125 Validation loss: 2322.6376953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 42800 Loss: 1945.5531005859375 Validation loss: 2325.61669921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 42900 Loss: 1940.578857421875 Validation loss: 2317.307373046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 43000 Loss: 1940.831787109375 Validation loss: 2318.20556640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 43100 Loss: 1943.0379638671875 Validation loss: 2319.0048828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 43200 Loss: 1941.7471923828125 Validation loss: 2326.94873046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 43300 Loss: 1939.4705810546875 Validation loss: 2326.951416015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 43400 Loss: 1939.5042724609375 Validation loss: 2328.685791015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 43500 Loss: 1936.9716796875 Validation loss: 2319.846923828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 43600 Loss: 1935.76220703125 Validation loss: 2323.360107421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 43700 Loss: 1935.95166015625 Validation loss: 2324.911865234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 43800 Loss: 1938.7344970703125 Validation loss: 2325.8525390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 43900 Loss: 1933.9007568359375 Validation loss: 2319.637451171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 44000 Loss: 1932.88916015625 Validation loss: 2324.496337890625 best Validation loss: 2291.566650390625\n",
      "Epoch: 44100 Loss: 1935.243408203125 Validation loss: 2325.375244140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 44200 Loss: 1931.5040283203125 Validation loss: 2323.7412109375 best Validation loss: 2291.566650390625\n",
      "Epoch: 44300 Loss: 1932.329833984375 Validation loss: 2326.36865234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 44400 Loss: 1933.124755859375 Validation loss: 2321.226806640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 44500 Loss: 1929.564453125 Validation loss: 2321.3515625 best Validation loss: 2291.566650390625\n",
      "Epoch: 44600 Loss: 1929.05224609375 Validation loss: 2321.750244140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 44700 Loss: 1927.92822265625 Validation loss: 2324.428955078125 best Validation loss: 2291.566650390625\n",
      "Epoch: 44800 Loss: 1928.7291259765625 Validation loss: 2323.283447265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 44900 Loss: 1929.7474365234375 Validation loss: 2327.10009765625 best Validation loss: 2291.566650390625\n",
      "Epoch: 45000 Loss: 1927.950439453125 Validation loss: 2323.923828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 45100 Loss: 1927.4208984375 Validation loss: 2328.76904296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 45200 Loss: 1924.253662109375 Validation loss: 2328.78125 best Validation loss: 2291.566650390625\n",
      "Epoch: 45300 Loss: 1923.562744140625 Validation loss: 2328.391357421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 45400 Loss: 1926.7003173828125 Validation loss: 2324.647216796875 best Validation loss: 2291.566650390625\n",
      "Epoch: 45500 Loss: 1924.02001953125 Validation loss: 2330.900146484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 45600 Loss: 1922.108154296875 Validation loss: 2329.862548828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 45700 Loss: 1922.8297119140625 Validation loss: 2332.98388671875 best Validation loss: 2291.566650390625\n",
      "Epoch: 45800 Loss: 1921.92333984375 Validation loss: 2330.217041015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 45900 Loss: 1922.0321044921875 Validation loss: 2325.890869140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 46000 Loss: 1919.46728515625 Validation loss: 2326.575439453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 46100 Loss: 1918.4659423828125 Validation loss: 2327.333251953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 46200 Loss: 1917.940185546875 Validation loss: 2327.6298828125 best Validation loss: 2291.566650390625\n",
      "Epoch: 46300 Loss: 1921.0269775390625 Validation loss: 2324.138916015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 46400 Loss: 1921.9271240234375 Validation loss: 2333.05712890625 best Validation loss: 2291.566650390625\n",
      "Epoch: 46500 Loss: 1917.1810302734375 Validation loss: 2324.1396484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 46600 Loss: 1915.01806640625 Validation loss: 2325.785400390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 46700 Loss: 1915.5792236328125 Validation loss: 2325.778564453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 46800 Loss: 1914.2335205078125 Validation loss: 2325.8232421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 46900 Loss: 1913.2498779296875 Validation loss: 2327.271240234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 47000 Loss: 1912.64990234375 Validation loss: 2326.248291015625 best Validation loss: 2291.566650390625\n",
      "Epoch: 47100 Loss: 1913.0404052734375 Validation loss: 2324.030029296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 47200 Loss: 1912.2381591796875 Validation loss: 2325.85302734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 47300 Loss: 1911.8504638671875 Validation loss: 2327.046875 best Validation loss: 2291.566650390625\n",
      "Epoch: 47400 Loss: 1911.2310791015625 Validation loss: 2326.328369140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 47500 Loss: 1909.087890625 Validation loss: 2325.66650390625 best Validation loss: 2291.566650390625\n",
      "Epoch: 47600 Loss: 1908.6619873046875 Validation loss: 2327.002197265625 best Validation loss: 2291.566650390625\n",
      "Epoch: 47700 Loss: 1907.708740234375 Validation loss: 2327.56982421875 best Validation loss: 2291.566650390625\n",
      "Epoch: 47800 Loss: 1907.6539306640625 Validation loss: 2329.93701171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 47900 Loss: 1906.8314208984375 Validation loss: 2326.59130859375 best Validation loss: 2291.566650390625\n",
      "Epoch: 48000 Loss: 1906.14111328125 Validation loss: 2328.087646484375 best Validation loss: 2291.566650390625\n",
      "Epoch: 48100 Loss: 1909.2978515625 Validation loss: 2333.6865234375 best Validation loss: 2291.566650390625\n",
      "Epoch: 48200 Loss: 1908.146240234375 Validation loss: 2337.795654296875 best Validation loss: 2291.566650390625\n",
      "Epoch: 48300 Loss: 1907.2684326171875 Validation loss: 2336.3876953125 best Validation loss: 2291.566650390625\n",
      "Epoch: 48400 Loss: 1905.8968505859375 Validation loss: 2331.69970703125 best Validation loss: 2291.566650390625\n",
      "Epoch: 48500 Loss: 1905.86328125 Validation loss: 2330.357177734375 best Validation loss: 2291.566650390625\n",
      "Epoch: 48600 Loss: 1903.308837890625 Validation loss: 2329.419921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 48700 Loss: 1904.004150390625 Validation loss: 2327.828369140625 best Validation loss: 2291.566650390625\n",
      "Epoch: 48800 Loss: 1903.1800537109375 Validation loss: 2329.5439453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 48900 Loss: 1902.343505859375 Validation loss: 2328.674560546875 best Validation loss: 2291.566650390625\n",
      "Epoch: 49000 Loss: 1903.8494873046875 Validation loss: 2329.679931640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 49100 Loss: 1903.352783203125 Validation loss: 2329.236083984375 best Validation loss: 2291.566650390625\n",
      "Epoch: 49200 Loss: 1900.0460205078125 Validation loss: 2334.3544921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 49300 Loss: 1899.290283203125 Validation loss: 2329.78076171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 49400 Loss: 1904.236083984375 Validation loss: 2329.918701171875 best Validation loss: 2291.566650390625\n",
      "Epoch: 49500 Loss: 1899.12744140625 Validation loss: 2330.321044921875 best Validation loss: 2291.566650390625\n",
      "Epoch: 49600 Loss: 1898.03173828125 Validation loss: 2330.36083984375 best Validation loss: 2291.566650390625\n",
      "Epoch: 49700 Loss: 1897.9105224609375 Validation loss: 2332.98681640625 best Validation loss: 2291.566650390625\n",
      "Epoch: 49800 Loss: 1899.5189208984375 Validation loss: 2339.50439453125 best Validation loss: 2291.566650390625\n",
      "Epoch: 49900 Loss: 1896.613525390625 Validation loss: 2327.86669921875 best Validation loss: 2291.566650390625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "last_val_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for n in range(NUMBER_OF_EPOCH):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)[:, 0]\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    training_loss = np.append(training_loss, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = model(X_val)[:, 0]\n",
    "    val_loss = loss_fn(y_pred, y_val)\n",
    "    validation_loss = np.append(validation_loss, val_loss.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "    torch.save(model, 'mlp_model.pth')\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(f'Epoch: {n} Loss: {loss.item()}'f' Validation loss: {val_loss.item()}'f' best Validation loss: {best_val_loss}')\n",
    "        \n",
    "    if last_val_loss < val_loss:\n",
    "        count += 1\n",
    "        if count == 50:\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "    last_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x150113d90>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(training_loss.shape, validation_loss.shape)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/cy9kvd894_bfkfb71dsbxt2w0000gn/T/ipykernel_61623/228361571.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mlp_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "model = torch.load('mlp_model.pth')\n",
    "model.eval()\n",
    "\n",
    "index = df['id']\n",
    "df = df.drop(['id'], axis=1)\n",
    "X = pre.encoder(df)\n",
    "X = pre.standardize(X, scaler=pickle.load(open('scaler.pkl', 'rb')))\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_pred = model(X)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(y_pred.detach().numpy(), columns=['price'])\n",
    "df = pd.concat([index, df], axis=1)\n",
    "df.rename(columns={'price': 'answer'}, inplace=True)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
