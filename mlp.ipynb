{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCH = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import preprossesing as pre\n",
    "import math\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(df):\n",
    "    \n",
    "    # df = pre.standardize(df)\n",
    "    df = pre.encoder(df)\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    train, val = pre.test_validation_split(df)\n",
    "    \n",
    "    y_train = torch.tensor(train['price'].values, dtype=torch.float32)\n",
    "    X_train = train.drop(['price'], axis=1)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    \n",
    "    y_val = torch.tensor(val['price'].values, dtype=torch.float32)\n",
    "    X_val = val.drop(['price'], axis=1)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=171, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(171, 256),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Linear(128, 1) \n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layers(x)\n",
    "\n",
    "model = mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_train, y_train, X_val, y_val = prepare_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 18018.087890625 Validation loss: 18092.79296875 best Validation loss: 18092.79296875\n",
      "Epoch: 100 Loss: 6941.8642578125 Validation loss: 7089.32568359375 best Validation loss: 7089.32568359375\n",
      "Epoch: 200 Loss: 6845.24951171875 Validation loss: 6994.78076171875 best Validation loss: 6994.78076171875\n",
      "Epoch: 300 Loss: 6800.666015625 Validation loss: 6949.02392578125 best Validation loss: 6949.02392578125\n",
      "Epoch: 400 Loss: 6744.74853515625 Validation loss: 6892.39306640625 best Validation loss: 6892.39306640625\n",
      "Epoch: 500 Loss: 6664.37744140625 Validation loss: 6811.27392578125 best Validation loss: 6811.27392578125\n",
      "Epoch: 600 Loss: 6536.5546875 Validation loss: 6688.25048828125 best Validation loss: 6688.25048828125\n",
      "Epoch: 700 Loss: 6350.93701171875 Validation loss: 6505.86181640625 best Validation loss: 6497.380859375\n",
      "Epoch: 800 Loss: 6041.82177734375 Validation loss: 6204.19677734375 best Validation loss: 6204.12890625\n",
      "Epoch: 900 Loss: 5753.22705078125 Validation loss: 5924.00341796875 best Validation loss: 5878.21044921875\n",
      "Epoch: 1000 Loss: 5334.32373046875 Validation loss: 5562.58056640625 best Validation loss: 5504.9833984375\n",
      "Epoch: 1100 Loss: 5063.517578125 Validation loss: 5203.62158203125 best Validation loss: 5120.763671875\n",
      "Epoch: 1200 Loss: 4578.76953125 Validation loss: 4731.740234375 best Validation loss: 4731.740234375\n",
      "Epoch: 1300 Loss: 4235.5302734375 Validation loss: 4371.81689453125 best Validation loss: 4371.81689453125\n",
      "Epoch: 1400 Loss: 3926.234619140625 Validation loss: 4065.778564453125 best Validation loss: 4065.778564453125\n",
      "Epoch: 1500 Loss: 3687.864501953125 Validation loss: 3882.663330078125 best Validation loss: 3829.6767578125\n",
      "Epoch: 1600 Loss: 3517.8828125 Validation loss: 3700.141845703125 best Validation loss: 3652.021728515625\n",
      "Epoch: 1700 Loss: 3415.11328125 Validation loss: 3591.97900390625 best Validation loss: 3500.395751953125\n",
      "Epoch: 1800 Loss: 3450.696044921875 Validation loss: 3476.707275390625 best Validation loss: 3388.635498046875\n",
      "Epoch: 1900 Loss: 3252.531982421875 Validation loss: 3325.351318359375 best Validation loss: 3306.97509765625\n",
      "Epoch: 2000 Loss: 3173.9169921875 Validation loss: 3397.18603515625 best Validation loss: 3253.2060546875\n",
      "Epoch: 2100 Loss: 3150.029541015625 Validation loss: 3337.4462890625 best Validation loss: 3185.738037109375\n",
      "Epoch: 2200 Loss: 3216.968505859375 Validation loss: 3241.101318359375 best Validation loss: 3140.62060546875\n",
      "Epoch: 2300 Loss: 3049.83349609375 Validation loss: 3131.781005859375 best Validation loss: 3098.762451171875\n",
      "Epoch: 2400 Loss: 2952.91455078125 Validation loss: 3057.812744140625 best Validation loss: 3055.4208984375\n",
      "Epoch: 2500 Loss: 3007.31396484375 Validation loss: 3180.11181640625 best Validation loss: 3025.852294921875\n",
      "Epoch: 2600 Loss: 2916.730712890625 Validation loss: 3003.22265625 best Validation loss: 2972.716552734375\n",
      "Epoch: 2700 Loss: 3056.075927734375 Validation loss: 3233.52978515625 best Validation loss: 2959.812744140625\n",
      "Epoch: 2800 Loss: 2875.48681640625 Validation loss: 2994.444091796875 best Validation loss: 2914.961669921875\n",
      "Epoch: 2900 Loss: 2789.583740234375 Validation loss: 2912.5244140625 best Validation loss: 2882.859619140625\n",
      "Epoch: 3000 Loss: 3990.014404296875 Validation loss: 3600.111328125 best Validation loss: 2873.01220703125\n",
      "Epoch: 3100 Loss: 2858.119873046875 Validation loss: 2956.021728515625 best Validation loss: 2873.01220703125\n",
      "Epoch: 3200 Loss: 2907.8544921875 Validation loss: 2967.77880859375 best Validation loss: 2873.01220703125\n",
      "Epoch: 3300 Loss: 2862.780517578125 Validation loss: 2886.09716796875 best Validation loss: 2859.26025390625\n",
      "Epoch: 3400 Loss: 2875.858642578125 Validation loss: 2898.519287109375 best Validation loss: 2819.93505859375\n",
      "Epoch: 3500 Loss: 2687.579833984375 Validation loss: 2787.97607421875 best Validation loss: 2787.54833984375\n",
      "Epoch: 3600 Loss: 2720.3720703125 Validation loss: 2899.668701171875 best Validation loss: 2783.424560546875\n",
      "Epoch: 3700 Loss: 2796.740234375 Validation loss: 3006.477294921875 best Validation loss: 2761.26318359375\n",
      "Epoch: 3800 Loss: 2814.86376953125 Validation loss: 2853.403076171875 best Validation loss: 2750.559814453125\n",
      "Epoch: 3900 Loss: 2725.5078125 Validation loss: 2747.383544921875 best Validation loss: 2720.11474609375\n",
      "Epoch: 4000 Loss: 2602.30859375 Validation loss: 2751.713134765625 best Validation loss: 2711.943603515625\n",
      "Epoch: 4100 Loss: 2604.739013671875 Validation loss: 2748.819091796875 best Validation loss: 2698.38427734375\n",
      "Epoch: 4200 Loss: 2765.344970703125 Validation loss: 2745.676513671875 best Validation loss: 2677.106689453125\n",
      "Epoch: 4300 Loss: 2556.894775390625 Validation loss: 2689.822265625 best Validation loss: 2668.864501953125\n",
      "Epoch: 4400 Loss: 2542.758544921875 Validation loss: 2679.2021484375 best Validation loss: 2655.22265625\n",
      "Epoch: 4500 Loss: 2633.099365234375 Validation loss: 2777.25537109375 best Validation loss: 2645.237060546875\n",
      "Epoch: 4600 Loss: 2652.04638671875 Validation loss: 2698.475341796875 best Validation loss: 2631.598388671875\n",
      "Epoch: 4700 Loss: 2493.128662109375 Validation loss: 2631.773681640625 best Validation loss: 2614.400634765625\n",
      "Epoch: 4800 Loss: 2829.490234375 Validation loss: 2737.333251953125 best Validation loss: 2606.800048828125\n",
      "Epoch: 4900 Loss: 2493.722412109375 Validation loss: 2606.95751953125 best Validation loss: 2604.13427734375\n",
      "Epoch: 5000 Loss: 2476.399658203125 Validation loss: 2621.9111328125 best Validation loss: 2586.7666015625\n",
      "Epoch: 5100 Loss: 2514.252197265625 Validation loss: 2593.42822265625 best Validation loss: 2577.936279296875\n",
      "Epoch: 5200 Loss: 2526.8935546875 Validation loss: 2689.26708984375 best Validation loss: 2567.061279296875\n",
      "Epoch: 5300 Loss: 2457.56640625 Validation loss: 2550.17333984375 best Validation loss: 2548.51025390625\n",
      "Epoch: 5400 Loss: 2420.02001953125 Validation loss: 2587.1220703125 best Validation loss: 2540.728515625\n",
      "Epoch: 5500 Loss: 2432.77587890625 Validation loss: 2640.353271484375 best Validation loss: 2532.113525390625\n",
      "Epoch: 5600 Loss: 2399.447998046875 Validation loss: 2543.22119140625 best Validation loss: 2523.916259765625\n",
      "Epoch: 5700 Loss: 2393.793212890625 Validation loss: 2557.70166015625 best Validation loss: 2507.610107421875\n",
      "Epoch: 5800 Loss: 2386.322998046875 Validation loss: 2556.88037109375 best Validation loss: 2499.16796875\n",
      "Epoch: 5900 Loss: 2378.74560546875 Validation loss: 2540.522705078125 best Validation loss: 2492.429931640625\n",
      "Epoch: 6000 Loss: 2441.39501953125 Validation loss: 2609.4326171875 best Validation loss: 2488.398193359375\n",
      "Epoch: 6100 Loss: 2484.39794921875 Validation loss: 2797.30517578125 best Validation loss: 2481.4443359375\n",
      "Epoch: 6200 Loss: 2471.446044921875 Validation loss: 2511.83056640625 best Validation loss: 2476.30126953125\n",
      "Epoch: 6300 Loss: 2449.607421875 Validation loss: 2692.265869140625 best Validation loss: 2469.79541015625\n",
      "Epoch: 6400 Loss: 2595.776611328125 Validation loss: 2645.949951171875 best Validation loss: 2463.916748046875\n",
      "Epoch: 6500 Loss: 2333.279052734375 Validation loss: 2460.27685546875 best Validation loss: 2457.673095703125\n",
      "Epoch: 6600 Loss: 2490.61767578125 Validation loss: 2681.722900390625 best Validation loss: 2448.8310546875\n",
      "Epoch: 6700 Loss: 2301.37451171875 Validation loss: 2452.571044921875 best Validation loss: 2448.667236328125\n",
      "Epoch: 6800 Loss: 2486.15185546875 Validation loss: 2785.336181640625 best Validation loss: 2445.173095703125\n",
      "Epoch: 6900 Loss: 2283.90576171875 Validation loss: 2449.56982421875 best Validation loss: 2434.76513671875\n",
      "Epoch: 7000 Loss: 2304.476806640625 Validation loss: 2538.416015625 best Validation loss: 2430.147216796875\n",
      "Epoch: 7100 Loss: 2309.27880859375 Validation loss: 2527.833251953125 best Validation loss: 2428.052734375\n",
      "Epoch: 7200 Loss: 2437.104736328125 Validation loss: 2487.355224609375 best Validation loss: 2427.216552734375\n",
      "Epoch: 7300 Loss: 2268.535888671875 Validation loss: 2430.90087890625 best Validation loss: 2420.241943359375\n",
      "Epoch: 7400 Loss: 2261.0556640625 Validation loss: 2422.509033203125 best Validation loss: 2412.11328125\n",
      "Epoch: 7500 Loss: 2331.56591796875 Validation loss: 2625.9033203125 best Validation loss: 2409.49462890625\n",
      "Epoch: 7600 Loss: 2371.0390625 Validation loss: 2431.009521484375 best Validation loss: 2409.255126953125\n",
      "Epoch: 7700 Loss: 2464.994140625 Validation loss: 2520.884521484375 best Validation loss: 2400.81396484375\n",
      "Epoch: 7800 Loss: 2432.514404296875 Validation loss: 2542.4697265625 best Validation loss: 2400.582763671875\n",
      "Epoch: 7900 Loss: 2402.5009765625 Validation loss: 2536.226806640625 best Validation loss: 2394.150634765625\n",
      "Epoch: 8000 Loss: 2251.530029296875 Validation loss: 2398.384765625 best Validation loss: 2394.150634765625\n",
      "Epoch: 8100 Loss: 2405.03759765625 Validation loss: 2576.519775390625 best Validation loss: 2391.8095703125\n",
      "Epoch: 8200 Loss: 2354.611328125 Validation loss: 2442.624267578125 best Validation loss: 2389.945068359375\n",
      "Epoch: 8300 Loss: 2312.546142578125 Validation loss: 2634.57958984375 best Validation loss: 2383.939697265625\n",
      "Epoch: 8400 Loss: 2236.7529296875 Validation loss: 2493.21044921875 best Validation loss: 2383.939697265625\n",
      "Epoch: 8500 Loss: 2282.13427734375 Validation loss: 2399.48583984375 best Validation loss: 2380.873779296875\n",
      "Epoch: 8600 Loss: 2383.412353515625 Validation loss: 2450.84765625 best Validation loss: 2380.873779296875\n",
      "Epoch: 8700 Loss: 2260.42724609375 Validation loss: 2404.159423828125 best Validation loss: 2380.873779296875\n",
      "Epoch: 8800 Loss: 2230.2294921875 Validation loss: 2409.633056640625 best Validation loss: 2380.873779296875\n",
      "Epoch: 8900 Loss: 2220.3857421875 Validation loss: 2407.408447265625 best Validation loss: 2380.873779296875\n",
      "Epoch: 9000 Loss: 2345.1064453125 Validation loss: 2488.27490234375 best Validation loss: 2378.0244140625\n",
      "Epoch: 9100 Loss: 2374.894287109375 Validation loss: 2445.1064453125 best Validation loss: 2377.349365234375\n",
      "Epoch: 9200 Loss: 2336.0751953125 Validation loss: 2458.416259765625 best Validation loss: 2372.984619140625\n",
      "Epoch: 9300 Loss: 2203.524169921875 Validation loss: 2388.9912109375 best Validation loss: 2371.27490234375\n",
      "Epoch: 9400 Loss: 2332.43212890625 Validation loss: 2492.590576171875 best Validation loss: 2369.734375\n",
      "Epoch: 9500 Loss: 2264.337158203125 Validation loss: 2391.646728515625 best Validation loss: 2366.53369140625\n",
      "Epoch: 9600 Loss: 2357.43896484375 Validation loss: 2595.55419921875 best Validation loss: 2366.33544921875\n",
      "Epoch: 9700 Loss: 2185.66259765625 Validation loss: 2396.173095703125 best Validation loss: 2361.021484375\n",
      "Epoch: 9800 Loss: 2290.068603515625 Validation loss: 2622.927001953125 best Validation loss: 2360.141845703125\n",
      "Epoch: 9900 Loss: 2182.419921875 Validation loss: 2379.946533203125 best Validation loss: 2356.2001953125\n",
      "Epoch: 10000 Loss: 2385.623291015625 Validation loss: 2449.06298828125 best Validation loss: 2356.2001953125\n",
      "Epoch: 10100 Loss: 2224.15673828125 Validation loss: 2446.54150390625 best Validation loss: 2356.2001953125\n",
      "Epoch: 10200 Loss: 2184.232177734375 Validation loss: 2385.917724609375 best Validation loss: 2355.4345703125\n",
      "Epoch: 10300 Loss: 2186.517578125 Validation loss: 2372.448974609375 best Validation loss: 2352.689453125\n",
      "Epoch: 10400 Loss: 2269.832763671875 Validation loss: 2617.578125 best Validation loss: 2352.689453125\n",
      "Epoch: 10500 Loss: 2181.9072265625 Validation loss: 2354.5498046875 best Validation loss: 2350.681396484375\n",
      "Epoch: 10600 Loss: 2308.370361328125 Validation loss: 2507.884521484375 best Validation loss: 2349.676513671875\n",
      "Epoch: 10700 Loss: 2213.495361328125 Validation loss: 2526.416015625 best Validation loss: 2349.676513671875\n",
      "Epoch: 10800 Loss: 2160.61474609375 Validation loss: 2373.161376953125 best Validation loss: 2347.617431640625\n",
      "Epoch: 10900 Loss: 2160.431396484375 Validation loss: 2415.916015625 best Validation loss: 2344.75146484375\n",
      "Epoch: 11000 Loss: 2151.5947265625 Validation loss: 2380.261962890625 best Validation loss: 2344.75146484375\n",
      "Epoch: 11100 Loss: 2249.779052734375 Validation loss: 2354.65771484375 best Validation loss: 2343.73046875\n",
      "Epoch: 11200 Loss: 2382.544677734375 Validation loss: 2454.0478515625 best Validation loss: 2343.41455078125\n",
      "Epoch: 11300 Loss: 2220.57666015625 Validation loss: 2454.114990234375 best Validation loss: 2343.41455078125\n",
      "Epoch: 11400 Loss: 2146.305908203125 Validation loss: 2358.46630859375 best Validation loss: 2341.518310546875\n",
      "Epoch: 11500 Loss: 2145.76318359375 Validation loss: 2370.693115234375 best Validation loss: 2341.518310546875\n",
      "Epoch: 11600 Loss: 2282.37548828125 Validation loss: 2530.6962890625 best Validation loss: 2341.518310546875\n",
      "Epoch: 11700 Loss: 2151.93798828125 Validation loss: 2431.7568359375 best Validation loss: 2338.092041015625\n",
      "Epoch: 11800 Loss: 2233.447021484375 Validation loss: 2455.8515625 best Validation loss: 2337.4326171875\n",
      "Epoch: 11900 Loss: 2222.913818359375 Validation loss: 2456.455322265625 best Validation loss: 2337.4326171875\n",
      "Epoch: 12000 Loss: 2132.557861328125 Validation loss: 2347.314208984375 best Validation loss: 2337.4326171875\n",
      "Epoch: 12100 Loss: 2129.45068359375 Validation loss: 2354.442626953125 best Validation loss: 2337.4326171875\n",
      "Epoch: 12200 Loss: 2211.147705078125 Validation loss: 2380.4072265625 best Validation loss: 2334.86181640625\n",
      "Epoch: 12300 Loss: 2243.265869140625 Validation loss: 2446.2578125 best Validation loss: 2334.86181640625\n",
      "Epoch: 12400 Loss: 2297.48046875 Validation loss: 2472.896240234375 best Validation loss: 2334.86181640625\n",
      "Epoch: 12500 Loss: 2318.629638671875 Validation loss: 2454.04443359375 best Validation loss: 2334.1572265625\n",
      "Epoch: 12600 Loss: 2123.727294921875 Validation loss: 2373.38134765625 best Validation loss: 2331.8515625\n",
      "Epoch: 12700 Loss: 2154.16357421875 Validation loss: 2392.50634765625 best Validation loss: 2331.8515625\n",
      "Epoch: 12800 Loss: 2118.528564453125 Validation loss: 2369.748291015625 best Validation loss: 2331.406005859375\n",
      "Epoch: 12900 Loss: 2127.90087890625 Validation loss: 2393.61328125 best Validation loss: 2329.629638671875\n",
      "Epoch: 13000 Loss: 2142.595947265625 Validation loss: 2338.620849609375 best Validation loss: 2328.381591796875\n",
      "Epoch: 13100 Loss: 2126.600830078125 Validation loss: 2373.099853515625 best Validation loss: 2327.944091796875\n",
      "Epoch: 13200 Loss: 2098.8544921875 Validation loss: 2353.312744140625 best Validation loss: 2327.577392578125\n",
      "Epoch: 13300 Loss: 2243.513916015625 Validation loss: 2460.60986328125 best Validation loss: 2327.577392578125\n",
      "Epoch: 13400 Loss: 2199.275390625 Validation loss: 2444.616943359375 best Validation loss: 2327.577392578125\n",
      "Epoch: 13500 Loss: 2197.1171875 Validation loss: 2465.720703125 best Validation loss: 2325.90673828125\n",
      "Epoch: 13600 Loss: 2133.248291015625 Validation loss: 2326.1337890625 best Validation loss: 2325.90673828125\n",
      "Epoch: 13700 Loss: 2207.704345703125 Validation loss: 2436.3720703125 best Validation loss: 2325.8212890625\n",
      "Epoch: 13800 Loss: 2323.120849609375 Validation loss: 2406.9443359375 best Validation loss: 2323.3662109375\n",
      "Epoch: 13900 Loss: 2238.296875 Validation loss: 2374.3037109375 best Validation loss: 2323.3662109375\n",
      "Epoch: 14000 Loss: 2250.583740234375 Validation loss: 2427.41943359375 best Validation loss: 2323.3662109375\n",
      "Epoch: 14100 Loss: 2268.325927734375 Validation loss: 2424.676513671875 best Validation loss: 2323.3662109375\n",
      "Epoch: 14200 Loss: 2111.0126953125 Validation loss: 2327.283203125 best Validation loss: 2323.3662109375\n",
      "Epoch: 14300 Loss: 2224.818603515625 Validation loss: 2523.348876953125 best Validation loss: 2323.3662109375\n",
      "Epoch: 14400 Loss: 2171.348388671875 Validation loss: 2323.6728515625 best Validation loss: 2320.852783203125\n",
      "Epoch: 14500 Loss: 2083.14013671875 Validation loss: 2331.8115234375 best Validation loss: 2319.590087890625\n",
      "Epoch: 14600 Loss: 2101.032470703125 Validation loss: 2328.555908203125 best Validation loss: 2318.554931640625\n",
      "Epoch: 14700 Loss: 2104.63427734375 Validation loss: 2333.61474609375 best Validation loss: 2318.554931640625\n",
      "Epoch: 14800 Loss: 2103.27978515625 Validation loss: 2332.57666015625 best Validation loss: 2318.554931640625\n",
      "Epoch: 14900 Loss: 2256.854736328125 Validation loss: 2432.301025390625 best Validation loss: 2318.554931640625\n",
      "Epoch: 15000 Loss: 2112.2646484375 Validation loss: 2444.448486328125 best Validation loss: 2318.554931640625\n",
      "Epoch: 15100 Loss: 2285.539794921875 Validation loss: 2424.366943359375 best Validation loss: 2318.554931640625\n",
      "Epoch: 15200 Loss: 2118.506591796875 Validation loss: 2429.956787109375 best Validation loss: 2315.219970703125\n",
      "Epoch: 15300 Loss: 2175.172607421875 Validation loss: 2528.547607421875 best Validation loss: 2315.219970703125\n",
      "Epoch: 15400 Loss: 2264.584228515625 Validation loss: 2401.77783203125 best Validation loss: 2315.219970703125\n",
      "Epoch: 15500 Loss: 2149.48388671875 Validation loss: 2340.875244140625 best Validation loss: 2315.219970703125\n",
      "Epoch: 15600 Loss: 2254.770263671875 Validation loss: 2482.219482421875 best Validation loss: 2313.282958984375\n",
      "Epoch: 15700 Loss: 2055.162109375 Validation loss: 2318.962646484375 best Validation loss: 2310.69873046875\n",
      "Epoch: 15800 Loss: 2138.1337890625 Validation loss: 2478.735595703125 best Validation loss: 2310.69873046875\n",
      "Epoch: 15900 Loss: 2053.590087890625 Validation loss: 2320.537841796875 best Validation loss: 2310.69873046875\n",
      "Epoch: 16000 Loss: 2070.2685546875 Validation loss: 2312.869384765625 best Validation loss: 2310.69873046875\n",
      "Epoch: 16100 Loss: 2078.624267578125 Validation loss: 2368.312744140625 best Validation loss: 2309.11865234375\n",
      "Epoch: 16200 Loss: 2123.264892578125 Validation loss: 2373.218505859375 best Validation loss: 2307.25341796875\n",
      "Epoch: 16300 Loss: 2156.330078125 Validation loss: 2463.476318359375 best Validation loss: 2307.25341796875\n",
      "Epoch: 16400 Loss: 2073.2109375 Validation loss: 2312.64697265625 best Validation loss: 2307.25341796875\n",
      "Epoch: 16500 Loss: 2137.79833984375 Validation loss: 2409.02001953125 best Validation loss: 2307.25341796875\n",
      "Epoch: 16600 Loss: 2055.70654296875 Validation loss: 2335.47412109375 best Validation loss: 2307.25341796875\n",
      "Epoch: 16700 Loss: 2062.650634765625 Validation loss: 2310.233154296875 best Validation loss: 2307.172607421875\n",
      "Epoch: 16800 Loss: 2163.323486328125 Validation loss: 2321.693359375 best Validation loss: 2304.788330078125\n",
      "Epoch: 16900 Loss: 2274.21923828125 Validation loss: 2404.337646484375 best Validation loss: 2302.821533203125\n",
      "Epoch: 17000 Loss: 2211.7451171875 Validation loss: 2397.0146484375 best Validation loss: 2302.821533203125\n",
      "Epoch: 17100 Loss: 2138.59521484375 Validation loss: 2339.57958984375 best Validation loss: 2302.821533203125\n",
      "Epoch: 17200 Loss: 2039.4771728515625 Validation loss: 2315.164306640625 best Validation loss: 2302.821533203125\n",
      "Epoch: 17300 Loss: 2311.0146484375 Validation loss: 2470.847412109375 best Validation loss: 2299.53662109375\n",
      "Epoch: 17400 Loss: 2033.8236083984375 Validation loss: 2304.014404296875 best Validation loss: 2298.53369140625\n",
      "Epoch: 17500 Loss: 2040.449462890625 Validation loss: 2302.820068359375 best Validation loss: 2298.53369140625\n",
      "Epoch: 17600 Loss: 2032.81396484375 Validation loss: 2305.908203125 best Validation loss: 2298.53369140625\n",
      "Epoch: 17700 Loss: 2044.0748291015625 Validation loss: 2319.41552734375 best Validation loss: 2295.896484375\n",
      "Epoch: 17800 Loss: 2056.767578125 Validation loss: 2308.25048828125 best Validation loss: 2293.37451171875\n",
      "Epoch: 17900 Loss: 2030.491455078125 Validation loss: 2317.861328125 best Validation loss: 2293.37451171875\n",
      "Epoch: 18000 Loss: 2175.130859375 Validation loss: 2340.150146484375 best Validation loss: 2293.37451171875\n",
      "Epoch: 18100 Loss: 2086.521240234375 Validation loss: 2374.41845703125 best Validation loss: 2293.37451171875\n",
      "Epoch: 18200 Loss: 2026.875244140625 Validation loss: 2296.81201171875 best Validation loss: 2293.37451171875\n",
      "Epoch: 18300 Loss: 2035.4241943359375 Validation loss: 2301.442138671875 best Validation loss: 2293.37451171875\n",
      "Epoch: 18400 Loss: 2044.52392578125 Validation loss: 2326.638427734375 best Validation loss: 2293.37451171875\n",
      "Epoch: 18500 Loss: 2088.209228515625 Validation loss: 2432.8740234375 best Validation loss: 2293.37451171875\n",
      "Epoch: 18600 Loss: 2089.876953125 Validation loss: 2308.377197265625 best Validation loss: 2293.37451171875\n",
      "Epoch: 18700 Loss: 2048.5693359375 Validation loss: 2332.007568359375 best Validation loss: 2292.04052734375\n",
      "Epoch: 18800 Loss: 2108.65576171875 Validation loss: 2412.27734375 best Validation loss: 2291.995361328125\n",
      "Epoch: 18900 Loss: 2305.55126953125 Validation loss: 2646.237060546875 best Validation loss: 2287.353271484375\n",
      "Epoch: 19000 Loss: 2050.4189453125 Validation loss: 2328.515380859375 best Validation loss: 2287.353271484375\n",
      "Epoch: 19100 Loss: 2097.4873046875 Validation loss: 2442.8388671875 best Validation loss: 2287.353271484375\n",
      "Epoch: 19200 Loss: 2041.96435546875 Validation loss: 2296.894287109375 best Validation loss: 2287.353271484375\n",
      "Epoch: 19300 Loss: 2086.61669921875 Validation loss: 2426.972900390625 best Validation loss: 2287.353271484375\n",
      "Epoch: 19400 Loss: 2125.636474609375 Validation loss: 2331.159912109375 best Validation loss: 2287.353271484375\n",
      "Epoch: 19500 Loss: 2040.1893310546875 Validation loss: 2325.5966796875 best Validation loss: 2284.14208984375\n",
      "Epoch: 19600 Loss: 2155.39111328125 Validation loss: 2377.41552734375 best Validation loss: 2284.14208984375\n",
      "Epoch: 19700 Loss: 2083.15869140625 Validation loss: 2425.8115234375 best Validation loss: 2284.14208984375\n",
      "Epoch: 19800 Loss: 2011.8741455078125 Validation loss: 2298.4521484375 best Validation loss: 2284.12841796875\n",
      "Epoch: 19900 Loss: 2081.031005859375 Validation loss: 2460.70556640625 best Validation loss: 2281.409423828125\n",
      "Epoch: 20000 Loss: 2146.873046875 Validation loss: 2313.90234375 best Validation loss: 2281.409423828125\n",
      "Epoch: 20100 Loss: 2128.34326171875 Validation loss: 2478.951171875 best Validation loss: 2280.1513671875\n",
      "Epoch: 20200 Loss: 2045.8232421875 Validation loss: 2300.177978515625 best Validation loss: 2279.42919921875\n",
      "Epoch: 20300 Loss: 2063.9599609375 Validation loss: 2351.928466796875 best Validation loss: 2279.42919921875\n",
      "Epoch: 20400 Loss: 2028.3477783203125 Validation loss: 2346.253173828125 best Validation loss: 2279.42919921875\n",
      "Epoch: 20500 Loss: 2146.535400390625 Validation loss: 2334.712890625 best Validation loss: 2279.42919921875\n",
      "Epoch: 20600 Loss: 2079.369384765625 Validation loss: 2299.175048828125 best Validation loss: 2279.42919921875\n",
      "Epoch: 20700 Loss: 2107.517333984375 Validation loss: 2310.466552734375 best Validation loss: 2279.42919921875\n",
      "Epoch: 20800 Loss: 2035.1370849609375 Validation loss: 2326.280517578125 best Validation loss: 2279.42919921875\n",
      "Epoch: 20900 Loss: 2064.921630859375 Validation loss: 2299.8505859375 best Validation loss: 2279.42919921875\n",
      "Epoch: 21000 Loss: 2030.0595703125 Validation loss: 2296.43505859375 best Validation loss: 2279.42919921875\n",
      "Epoch: 21100 Loss: 2123.90966796875 Validation loss: 2464.138671875 best Validation loss: 2279.42919921875\n",
      "Epoch: 21200 Loss: 2125.08544921875 Validation loss: 2384.9013671875 best Validation loss: 2279.42919921875\n",
      "Epoch: 21300 Loss: 2126.291015625 Validation loss: 2446.18359375 best Validation loss: 2279.42919921875\n",
      "Epoch: 21400 Loss: 2049.54345703125 Validation loss: 2397.795654296875 best Validation loss: 2279.42919921875\n",
      "Epoch: 21500 Loss: 2099.517333984375 Validation loss: 2385.868408203125 best Validation loss: 2279.42919921875\n",
      "Epoch: 21600 Loss: 1996.078857421875 Validation loss: 2285.421142578125 best Validation loss: 2279.42919921875\n",
      "Epoch: 21700 Loss: 2015.9527587890625 Validation loss: 2295.05908203125 best Validation loss: 2276.76806640625\n",
      "Epoch: 21800 Loss: 2196.070068359375 Validation loss: 2402.23779296875 best Validation loss: 2276.76806640625\n",
      "Epoch: 21900 Loss: 2204.746826171875 Validation loss: 2344.849853515625 best Validation loss: 2276.76806640625\n",
      "Epoch: 22000 Loss: 2134.494384765625 Validation loss: 2450.259033203125 best Validation loss: 2276.577392578125\n",
      "Epoch: 22100 Loss: 1991.944580078125 Validation loss: 2302.134765625 best Validation loss: 2276.577392578125\n",
      "Epoch: 22200 Loss: 2191.829833984375 Validation loss: 2418.39404296875 best Validation loss: 2276.577392578125\n",
      "Epoch: 22300 Loss: 1992.2193603515625 Validation loss: 2290.603759765625 best Validation loss: 2276.577392578125\n",
      "Epoch: 22400 Loss: 2129.05078125 Validation loss: 2512.768310546875 best Validation loss: 2276.577392578125\n",
      "Epoch: 22500 Loss: 1988.2589111328125 Validation loss: 2277.445556640625 best Validation loss: 2276.577392578125\n",
      "Epoch: 22600 Loss: 2132.3017578125 Validation loss: 2289.478759765625 best Validation loss: 2275.2861328125\n",
      "Epoch: 22700 Loss: 2042.112060546875 Validation loss: 2295.27734375 best Validation loss: 2275.2861328125\n",
      "Epoch: 22800 Loss: 1982.59326171875 Validation loss: 2277.4794921875 best Validation loss: 2272.170166015625\n",
      "Epoch: 22900 Loss: 1998.457275390625 Validation loss: 2280.166015625 best Validation loss: 2272.170166015625\n",
      "Epoch: 23000 Loss: 2021.237060546875 Validation loss: 2367.19677734375 best Validation loss: 2272.170166015625\n",
      "Epoch: 23100 Loss: 2066.466796875 Validation loss: 2371.058837890625 best Validation loss: 2272.170166015625\n",
      "Epoch: 23200 Loss: 2132.825927734375 Validation loss: 2420.8896484375 best Validation loss: 2272.170166015625\n",
      "Epoch: 23300 Loss: 2015.56982421875 Validation loss: 2328.25341796875 best Validation loss: 2272.158203125\n",
      "Epoch: 23400 Loss: 1976.9439697265625 Validation loss: 2286.1796875 best Validation loss: 2271.11328125\n",
      "Epoch: 23500 Loss: 2201.1943359375 Validation loss: 2361.186279296875 best Validation loss: 2271.11328125\n",
      "Epoch: 23600 Loss: 1975.662109375 Validation loss: 2274.5439453125 best Validation loss: 2270.267822265625\n",
      "Epoch: 23700 Loss: 2003.7401123046875 Validation loss: 2355.531005859375 best Validation loss: 2268.712158203125\n",
      "Epoch: 23800 Loss: 2081.848876953125 Validation loss: 2301.308837890625 best Validation loss: 2268.712158203125\n",
      "Epoch: 23900 Loss: 1986.9344482421875 Validation loss: 2297.664794921875 best Validation loss: 2266.6728515625\n",
      "Epoch: 24000 Loss: 2118.901123046875 Validation loss: 2415.5849609375 best Validation loss: 2265.914794921875\n",
      "Epoch: 24100 Loss: 1965.88232421875 Validation loss: 2266.0791015625 best Validation loss: 2265.914794921875\n",
      "Epoch: 24200 Loss: 2134.74609375 Validation loss: 2463.98583984375 best Validation loss: 2264.829345703125\n",
      "Epoch: 24300 Loss: 1997.927734375 Validation loss: 2283.832763671875 best Validation loss: 2264.829345703125\n",
      "Epoch: 24400 Loss: 1999.31396484375 Validation loss: 2280.9306640625 best Validation loss: 2264.829345703125\n",
      "Epoch: 24500 Loss: 2071.3984375 Validation loss: 2294.511962890625 best Validation loss: 2263.946044921875\n",
      "Epoch: 24600 Loss: 1982.3302001953125 Validation loss: 2265.934326171875 best Validation loss: 2263.836669921875\n",
      "Epoch: 24700 Loss: 1970.7830810546875 Validation loss: 2267.105712890625 best Validation loss: 2262.97509765625\n",
      "Epoch: 24800 Loss: 2001.6776123046875 Validation loss: 2336.656005859375 best Validation loss: 2262.97509765625\n",
      "Epoch: 24900 Loss: 2105.736083984375 Validation loss: 2377.306396484375 best Validation loss: 2262.97509765625\n",
      "Epoch: 25000 Loss: 2123.5078125 Validation loss: 2403.615234375 best Validation loss: 2262.540771484375\n",
      "Epoch: 25100 Loss: 2015.503662109375 Validation loss: 2348.930908203125 best Validation loss: 2262.540771484375\n",
      "Epoch: 25200 Loss: 1999.1224365234375 Validation loss: 2276.824951171875 best Validation loss: 2262.033447265625\n",
      "Epoch: 25300 Loss: 2088.98974609375 Validation loss: 2571.422119140625 best Validation loss: 2259.02587890625\n",
      "Epoch: 25400 Loss: 2045.7833251953125 Validation loss: 2268.3564453125 best Validation loss: 2259.02587890625\n",
      "Epoch: 25500 Loss: 1955.007080078125 Validation loss: 2262.282958984375 best Validation loss: 2259.02587890625\n",
      "Epoch: 25600 Loss: 1968.1734619140625 Validation loss: 2268.6240234375 best Validation loss: 2259.02587890625\n",
      "Epoch: 25700 Loss: 1988.2672119140625 Validation loss: 2343.002197265625 best Validation loss: 2258.5615234375\n",
      "Epoch: 25800 Loss: 2050.981689453125 Validation loss: 2283.9111328125 best Validation loss: 2256.255615234375\n",
      "Epoch: 25900 Loss: 1962.2535400390625 Validation loss: 2293.98583984375 best Validation loss: 2256.255615234375\n",
      "Epoch: 26000 Loss: 2026.1900634765625 Validation loss: 2344.80078125 best Validation loss: 2255.912109375\n",
      "Epoch: 26100 Loss: 2052.522216796875 Validation loss: 2420.45947265625 best Validation loss: 2255.912109375\n",
      "Epoch: 26200 Loss: 2010.3251953125 Validation loss: 2378.799560546875 best Validation loss: 2252.548095703125\n",
      "Epoch: 26300 Loss: 4384.529296875 Validation loss: 3060.595947265625 best Validation loss: 2249.364013671875\n",
      "Epoch: 26400 Loss: 2378.399169921875 Validation loss: 2507.878173828125 best Validation loss: 2249.364013671875\n",
      "Epoch: 26500 Loss: 2141.769287109375 Validation loss: 2359.103759765625 best Validation loss: 2249.364013671875\n",
      "Epoch: 26600 Loss: 2117.676513671875 Validation loss: 2385.4091796875 best Validation loss: 2249.364013671875\n",
      "Epoch: 26700 Loss: 2154.41650390625 Validation loss: 2428.6923828125 best Validation loss: 2249.364013671875\n",
      "Epoch: 26800 Loss: 2070.802490234375 Validation loss: 2310.184814453125 best Validation loss: 2249.364013671875\n",
      "Epoch: 26900 Loss: 2049.3974609375 Validation loss: 2299.1728515625 best Validation loss: 2249.364013671875\n",
      "Epoch: 27000 Loss: 2040.479736328125 Validation loss: 2273.05224609375 best Validation loss: 2249.364013671875\n",
      "Epoch: 27100 Loss: 2036.6116943359375 Validation loss: 2292.756103515625 best Validation loss: 2249.364013671875\n",
      "Epoch: 27200 Loss: 2030.0648193359375 Validation loss: 2265.300048828125 best Validation loss: 2249.364013671875\n",
      "Epoch: 27300 Loss: 2162.617431640625 Validation loss: 2387.2109375 best Validation loss: 2249.364013671875\n",
      "Epoch: 27400 Loss: 2142.953369140625 Validation loss: 2334.711669921875 best Validation loss: 2249.364013671875\n",
      "Epoch: 27500 Loss: 2127.795654296875 Validation loss: 2303.558349609375 best Validation loss: 2249.364013671875\n",
      "Epoch: 27600 Loss: 2026.5567626953125 Validation loss: 2264.37353515625 best Validation loss: 2249.364013671875\n",
      "Epoch: 27700 Loss: 2077.380126953125 Validation loss: 2372.2060546875 best Validation loss: 2249.364013671875\n",
      "Epoch: 27800 Loss: 2100.455810546875 Validation loss: 2273.288818359375 best Validation loss: 2249.364013671875\n",
      "Epoch: 27900 Loss: 2089.138671875 Validation loss: 2384.068115234375 best Validation loss: 2249.364013671875\n",
      "Epoch: 28000 Loss: 2015.943603515625 Validation loss: 2266.2041015625 best Validation loss: 2249.364013671875\n",
      "Epoch: 28100 Loss: 2024.5107421875 Validation loss: 2290.6953125 best Validation loss: 2249.364013671875\n",
      "Epoch: 28200 Loss: 2122.434326171875 Validation loss: 2370.842041015625 best Validation loss: 2249.364013671875\n",
      "Epoch: 28300 Loss: 2089.014404296875 Validation loss: 2335.844482421875 best Validation loss: 2249.364013671875\n",
      "Epoch: 28400 Loss: 2100.1513671875 Validation loss: 2437.67578125 best Validation loss: 2249.364013671875\n",
      "Epoch: 28500 Loss: 2122.82373046875 Validation loss: 2398.58740234375 best Validation loss: 2249.364013671875\n",
      "Epoch: 28600 Loss: 2007.305419921875 Validation loss: 2261.556884765625 best Validation loss: 2249.364013671875\n",
      "Epoch: 28700 Loss: 2114.2294921875 Validation loss: 2376.864013671875 best Validation loss: 2249.364013671875\n",
      "Epoch: 28800 Loss: 2027.0450439453125 Validation loss: 2295.03662109375 best Validation loss: 2249.364013671875\n",
      "Epoch: 28900 Loss: 2029.9156494140625 Validation loss: 2260.920654296875 best Validation loss: 2249.364013671875\n",
      "Epoch: 29000 Loss: 2026.2864990234375 Validation loss: 2304.6513671875 best Validation loss: 2249.364013671875\n",
      "Epoch: 29100 Loss: 2026.990478515625 Validation loss: 2264.900634765625 best Validation loss: 2249.364013671875\n",
      "Epoch: 29200 Loss: 2000.4273681640625 Validation loss: 2258.72900390625 best Validation loss: 2249.364013671875\n",
      "Epoch: 29300 Loss: 2174.9404296875 Validation loss: 2441.312744140625 best Validation loss: 2249.364013671875\n",
      "Epoch: 29400 Loss: 2116.472900390625 Validation loss: 2340.656982421875 best Validation loss: 2249.364013671875\n",
      "Epoch: 29500 Loss: 2094.43994140625 Validation loss: 2344.219482421875 best Validation loss: 2249.364013671875\n",
      "Epoch: 29600 Loss: 2054.958984375 Validation loss: 2321.89697265625 best Validation loss: 2249.364013671875\n",
      "Epoch: 29700 Loss: 2023.593017578125 Validation loss: 2307.549560546875 best Validation loss: 2249.364013671875\n",
      "Epoch: 29800 Loss: 2085.620849609375 Validation loss: 2333.835693359375 best Validation loss: 2243.838134765625\n",
      "Epoch: 29900 Loss: 2082.8291015625 Validation loss: 2335.86767578125 best Validation loss: 2243.838134765625\n",
      "Epoch: 30000 Loss: 2039.1583251953125 Validation loss: 2394.738037109375 best Validation loss: 2243.838134765625\n",
      "Epoch: 30100 Loss: 2010.3216552734375 Validation loss: 2255.0234375 best Validation loss: 2243.838134765625\n",
      "Epoch: 30200 Loss: 1988.9454345703125 Validation loss: 2253.56005859375 best Validation loss: 2243.838134765625\n",
      "Epoch: 30300 Loss: 2032.4581298828125 Validation loss: 2351.34130859375 best Validation loss: 2243.838134765625\n",
      "Epoch: 30400 Loss: 1992.03076171875 Validation loss: 2251.4208984375 best Validation loss: 2243.838134765625\n",
      "Epoch: 30500 Loss: 1991.464111328125 Validation loss: 2251.419921875 best Validation loss: 2243.838134765625\n",
      "Epoch: 30600 Loss: 2010.3489990234375 Validation loss: 2317.46533203125 best Validation loss: 2243.838134765625\n",
      "Epoch: 30700 Loss: 1986.00048828125 Validation loss: 2253.7509765625 best Validation loss: 2243.838134765625\n",
      "Epoch: 30800 Loss: 2111.835693359375 Validation loss: 2334.5166015625 best Validation loss: 2243.838134765625\n",
      "Epoch: 30900 Loss: 2079.670166015625 Validation loss: 2335.18115234375 best Validation loss: 2243.838134765625\n",
      "Epoch: 31000 Loss: 2029.7611083984375 Validation loss: 2278.025146484375 best Validation loss: 2243.838134765625\n",
      "Epoch: 31100 Loss: 1986.010498046875 Validation loss: 2253.343017578125 best Validation loss: 2243.838134765625\n",
      "Epoch: 31200 Loss: 2076.53466796875 Validation loss: 2278.7509765625 best Validation loss: 2243.838134765625\n",
      "Epoch: 31300 Loss: 2185.909423828125 Validation loss: 2342.216552734375 best Validation loss: 2243.838134765625\n",
      "Epoch: 31400 Loss: 1992.74072265625 Validation loss: 2269.912109375 best Validation loss: 2243.050048828125\n",
      "Epoch: 31500 Loss: 2019.0177001953125 Validation loss: 2351.81982421875 best Validation loss: 2243.050048828125\n",
      "Epoch: 31600 Loss: 2067.338134765625 Validation loss: 2413.121826171875 best Validation loss: 2243.050048828125\n",
      "Epoch: 31700 Loss: 1999.3956298828125 Validation loss: 2271.22607421875 best Validation loss: 2243.050048828125\n",
      "Epoch: 31800 Loss: 1976.1265869140625 Validation loss: 2247.03759765625 best Validation loss: 2243.050048828125\n",
      "Epoch: 31900 Loss: 1976.8023681640625 Validation loss: 2249.0556640625 best Validation loss: 2243.050048828125\n",
      "Epoch: 32000 Loss: 2009.78369140625 Validation loss: 2247.906982421875 best Validation loss: 2243.050048828125\n",
      "Epoch: 32100 Loss: 1998.0777587890625 Validation loss: 2256.975341796875 best Validation loss: 2243.050048828125\n",
      "Epoch: 32200 Loss: 2004.365478515625 Validation loss: 2317.6083984375 best Validation loss: 2243.050048828125\n",
      "Epoch: 32300 Loss: 2050.021484375 Validation loss: 2272.8544921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 32400 Loss: 1989.000732421875 Validation loss: 2250.434326171875 best Validation loss: 2236.73681640625\n",
      "Epoch: 32500 Loss: 2004.573974609375 Validation loss: 2274.095947265625 best Validation loss: 2236.73681640625\n",
      "Epoch: 32600 Loss: 1996.53515625 Validation loss: 2270.849609375 best Validation loss: 2236.73681640625\n",
      "Epoch: 32700 Loss: 2054.5673828125 Validation loss: 2290.125 best Validation loss: 2236.73681640625\n",
      "Epoch: 32800 Loss: 2095.13330078125 Validation loss: 2430.095703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 32900 Loss: 2009.7978515625 Validation loss: 2320.1982421875 best Validation loss: 2236.73681640625\n",
      "Epoch: 33000 Loss: 2123.643310546875 Validation loss: 2393.48779296875 best Validation loss: 2236.73681640625\n",
      "Epoch: 33100 Loss: 1965.3992919921875 Validation loss: 2252.1953125 best Validation loss: 2236.73681640625\n",
      "Epoch: 33200 Loss: 1965.1448974609375 Validation loss: 2253.402587890625 best Validation loss: 2236.73681640625\n",
      "Epoch: 33300 Loss: 1998.2523193359375 Validation loss: 2252.112060546875 best Validation loss: 2236.73681640625\n",
      "Epoch: 33400 Loss: 2103.16943359375 Validation loss: 2339.6201171875 best Validation loss: 2236.73681640625\n",
      "Epoch: 33500 Loss: 1963.9676513671875 Validation loss: 2250.623779296875 best Validation loss: 2236.73681640625\n",
      "Epoch: 33600 Loss: 2007.7825927734375 Validation loss: 2259.41357421875 best Validation loss: 2236.73681640625\n",
      "Epoch: 33700 Loss: 1979.153564453125 Validation loss: 2251.03759765625 best Validation loss: 2236.73681640625\n",
      "Epoch: 33800 Loss: 2083.8837890625 Validation loss: 2369.552978515625 best Validation loss: 2236.73681640625\n",
      "Epoch: 33900 Loss: 1959.712890625 Validation loss: 2248.2685546875 best Validation loss: 2236.73681640625\n",
      "Epoch: 34000 Loss: 1956.8697509765625 Validation loss: 2241.841064453125 best Validation loss: 2236.73681640625\n",
      "Epoch: 34100 Loss: 2079.577392578125 Validation loss: 2325.352294921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 34200 Loss: 1958.0023193359375 Validation loss: 2243.180908203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 34300 Loss: 2045.73876953125 Validation loss: 2398.759033203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 34400 Loss: 1973.75439453125 Validation loss: 2250.672607421875 best Validation loss: 2236.73681640625\n",
      "Epoch: 34500 Loss: 1981.994873046875 Validation loss: 2280.7783203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 34600 Loss: 1956.5513916015625 Validation loss: 2249.0146484375 best Validation loss: 2236.73681640625\n",
      "Epoch: 34700 Loss: 2023.3787841796875 Validation loss: 2340.651123046875 best Validation loss: 2236.73681640625\n",
      "Epoch: 34800 Loss: 2019.0938720703125 Validation loss: 2311.270751953125 best Validation loss: 2236.73681640625\n",
      "Epoch: 34900 Loss: 1955.2091064453125 Validation loss: 2244.568115234375 best Validation loss: 2236.73681640625\n",
      "Epoch: 35000 Loss: 2041.4964599609375 Validation loss: 2276.621337890625 best Validation loss: 2236.73681640625\n",
      "Epoch: 35100 Loss: 1995.126953125 Validation loss: 2343.89599609375 best Validation loss: 2236.73681640625\n",
      "Epoch: 35200 Loss: 1956.424072265625 Validation loss: 2249.885986328125 best Validation loss: 2236.73681640625\n",
      "Epoch: 35300 Loss: 2072.324951171875 Validation loss: 2291.12451171875 best Validation loss: 2236.73681640625\n",
      "Epoch: 35400 Loss: 2054.400390625 Validation loss: 2319.640869140625 best Validation loss: 2236.73681640625\n",
      "Epoch: 35500 Loss: 1972.947509765625 Validation loss: 2247.234130859375 best Validation loss: 2236.73681640625\n",
      "Epoch: 35600 Loss: 1949.162353515625 Validation loss: 2250.298095703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 35700 Loss: 1952.0084228515625 Validation loss: 2245.73828125 best Validation loss: 2236.73681640625\n",
      "Epoch: 35800 Loss: 2033.88671875 Validation loss: 2278.065673828125 best Validation loss: 2236.73681640625\n",
      "Epoch: 35900 Loss: 2028.4835205078125 Validation loss: 2374.051513671875 best Validation loss: 2236.73681640625\n",
      "Epoch: 36000 Loss: 1946.143310546875 Validation loss: 2244.0234375 best Validation loss: 2236.73681640625\n",
      "Epoch: 36100 Loss: 1953.5999755859375 Validation loss: 2244.384033203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 36200 Loss: 2062.5615234375 Validation loss: 2327.177734375 best Validation loss: 2236.73681640625\n",
      "Epoch: 36300 Loss: 2000.90673828125 Validation loss: 2261.972900390625 best Validation loss: 2236.73681640625\n",
      "Epoch: 36400 Loss: 1951.240478515625 Validation loss: 2243.891357421875 best Validation loss: 2236.73681640625\n",
      "Epoch: 36500 Loss: 1957.13037109375 Validation loss: 2258.4970703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 36600 Loss: 1968.169921875 Validation loss: 2272.796630859375 best Validation loss: 2236.73681640625\n",
      "Epoch: 36700 Loss: 2051.63720703125 Validation loss: 2314.616943359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 36800 Loss: 1940.269775390625 Validation loss: 2247.65966796875 best Validation loss: 2236.73681640625\n",
      "Epoch: 36900 Loss: 2071.47314453125 Validation loss: 2344.28759765625 best Validation loss: 2236.73681640625\n",
      "Epoch: 37000 Loss: 1958.1336669921875 Validation loss: 2315.713134765625 best Validation loss: 2236.73681640625\n",
      "Epoch: 37100 Loss: 2038.855712890625 Validation loss: 2319.28125 best Validation loss: 2236.73681640625\n",
      "Epoch: 37200 Loss: 2125.427001953125 Validation loss: 2294.760498046875 best Validation loss: 2236.73681640625\n",
      "Epoch: 37300 Loss: 1975.504150390625 Validation loss: 2332.932861328125 best Validation loss: 2236.73681640625\n",
      "Epoch: 37400 Loss: 2061.7978515625 Validation loss: 2301.2646484375 best Validation loss: 2236.73681640625\n",
      "Epoch: 37500 Loss: 2112.47412109375 Validation loss: 2303.734619140625 best Validation loss: 2236.73681640625\n",
      "Epoch: 37600 Loss: 2032.9593505859375 Validation loss: 2384.521484375 best Validation loss: 2236.73681640625\n",
      "Epoch: 37700 Loss: 2034.248291015625 Validation loss: 2331.50439453125 best Validation loss: 2236.73681640625\n",
      "Epoch: 37800 Loss: 2154.0068359375 Validation loss: 2328.465087890625 best Validation loss: 2236.73681640625\n",
      "Epoch: 37900 Loss: 2044.3582763671875 Validation loss: 2318.79638671875 best Validation loss: 2236.73681640625\n",
      "Epoch: 38000 Loss: 1932.6868896484375 Validation loss: 2243.623291015625 best Validation loss: 2236.73681640625\n",
      "Epoch: 38100 Loss: 1944.4310302734375 Validation loss: 2248.72705078125 best Validation loss: 2236.73681640625\n",
      "Epoch: 38200 Loss: 2035.3076171875 Validation loss: 2374.19873046875 best Validation loss: 2236.73681640625\n",
      "Epoch: 38300 Loss: 1986.719482421875 Validation loss: 2251.681884765625 best Validation loss: 2236.73681640625\n",
      "Epoch: 38400 Loss: 2058.97802734375 Validation loss: 2334.524169921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 38500 Loss: 1937.7403564453125 Validation loss: 2272.547607421875 best Validation loss: 2236.73681640625\n",
      "Epoch: 38600 Loss: 1959.419921875 Validation loss: 2273.962158203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 38700 Loss: 1943.539794921875 Validation loss: 2249.2509765625 best Validation loss: 2236.73681640625\n",
      "Epoch: 38800 Loss: 2091.748779296875 Validation loss: 2349.11376953125 best Validation loss: 2236.73681640625\n",
      "Epoch: 38900 Loss: 1926.3441162109375 Validation loss: 2248.832275390625 best Validation loss: 2236.73681640625\n",
      "Epoch: 39000 Loss: 2077.285400390625 Validation loss: 2377.574462890625 best Validation loss: 2236.73681640625\n",
      "Epoch: 39100 Loss: 1925.567138671875 Validation loss: 2245.5732421875 best Validation loss: 2236.73681640625\n",
      "Epoch: 39200 Loss: 1949.7889404296875 Validation loss: 2250.017333984375 best Validation loss: 2236.73681640625\n",
      "Epoch: 39300 Loss: 1922.8543701171875 Validation loss: 2247.0927734375 best Validation loss: 2236.73681640625\n",
      "Epoch: 39400 Loss: 2041.2757568359375 Validation loss: 2314.241455078125 best Validation loss: 2236.73681640625\n",
      "Epoch: 39500 Loss: 1926.9031982421875 Validation loss: 2265.552734375 best Validation loss: 2236.73681640625\n",
      "Epoch: 39600 Loss: 1957.6983642578125 Validation loss: 2254.581787109375 best Validation loss: 2236.73681640625\n",
      "Epoch: 39700 Loss: 1919.5054931640625 Validation loss: 2244.00048828125 best Validation loss: 2236.73681640625\n",
      "Epoch: 39800 Loss: 1920.1806640625 Validation loss: 2242.892578125 best Validation loss: 2236.73681640625\n",
      "Epoch: 39900 Loss: 2078.9853515625 Validation loss: 2306.075927734375 best Validation loss: 2236.73681640625\n",
      "Epoch: 40000 Loss: 1928.47802734375 Validation loss: 2244.41845703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 40100 Loss: 1917.848388671875 Validation loss: 2246.204345703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 40200 Loss: 1923.87646484375 Validation loss: 2247.765869140625 best Validation loss: 2236.73681640625\n",
      "Epoch: 40300 Loss: 2032.1585693359375 Validation loss: 2367.700439453125 best Validation loss: 2236.73681640625\n",
      "Epoch: 40400 Loss: 1915.8909912109375 Validation loss: 2248.134033203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 40500 Loss: 1946.0166015625 Validation loss: 2271.460693359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 40600 Loss: 2028.9259033203125 Validation loss: 2349.862548828125 best Validation loss: 2236.73681640625\n",
      "Epoch: 40700 Loss: 1923.020263671875 Validation loss: 2256.8984375 best Validation loss: 2236.73681640625\n",
      "Epoch: 40800 Loss: 1918.3802490234375 Validation loss: 2261.038818359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 40900 Loss: 1923.208251953125 Validation loss: 2249.258056640625 best Validation loss: 2236.73681640625\n",
      "Epoch: 41000 Loss: 1928.1126708984375 Validation loss: 2255.032470703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 41100 Loss: 1912.5059814453125 Validation loss: 2247.186767578125 best Validation loss: 2236.73681640625\n",
      "Epoch: 41200 Loss: 1920.5224609375 Validation loss: 2257.8681640625 best Validation loss: 2236.73681640625\n",
      "Epoch: 41300 Loss: 1912.57470703125 Validation loss: 2259.76171875 best Validation loss: 2236.73681640625\n",
      "Epoch: 41400 Loss: 1918.1025390625 Validation loss: 2258.883544921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 41500 Loss: 2005.6717529296875 Validation loss: 2273.08544921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 41600 Loss: 1927.723388671875 Validation loss: 2260.968505859375 best Validation loss: 2236.73681640625\n",
      "Epoch: 41700 Loss: 1959.33544921875 Validation loss: 2354.417724609375 best Validation loss: 2236.73681640625\n",
      "Epoch: 41800 Loss: 1908.98486328125 Validation loss: 2251.048095703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 41900 Loss: 2039.140625 Validation loss: 2338.96240234375 best Validation loss: 2236.73681640625\n",
      "Epoch: 42000 Loss: 1905.06591796875 Validation loss: 2248.899169921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 42100 Loss: 1916.0133056640625 Validation loss: 2249.048583984375 best Validation loss: 2236.73681640625\n",
      "Epoch: 42200 Loss: 1999.12353515625 Validation loss: 2389.90966796875 best Validation loss: 2236.73681640625\n",
      "Epoch: 42300 Loss: 1923.4166259765625 Validation loss: 2251.972412109375 best Validation loss: 2236.73681640625\n",
      "Epoch: 42400 Loss: 1903.71435546875 Validation loss: 2246.642333984375 best Validation loss: 2236.73681640625\n",
      "Epoch: 42500 Loss: 1941.4342041015625 Validation loss: 2259.709716796875 best Validation loss: 2236.73681640625\n",
      "Epoch: 42600 Loss: 1903.27099609375 Validation loss: 2251.5078125 best Validation loss: 2236.73681640625\n",
      "Epoch: 42700 Loss: 2032.17333984375 Validation loss: 2332.392822265625 best Validation loss: 2236.73681640625\n",
      "Epoch: 42800 Loss: 2008.27392578125 Validation loss: 2282.1640625 best Validation loss: 2236.73681640625\n",
      "Epoch: 42900 Loss: 1980.382080078125 Validation loss: 2397.620361328125 best Validation loss: 2236.73681640625\n",
      "Epoch: 43000 Loss: 1899.3489990234375 Validation loss: 2246.924072265625 best Validation loss: 2236.73681640625\n",
      "Epoch: 43100 Loss: 2022.2467041015625 Validation loss: 2339.359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 43200 Loss: 1945.2821044921875 Validation loss: 2273.2919921875 best Validation loss: 2236.73681640625\n",
      "Epoch: 43300 Loss: 1976.75244140625 Validation loss: 2332.450927734375 best Validation loss: 2236.73681640625\n",
      "Epoch: 43400 Loss: 1903.04443359375 Validation loss: 2252.9091796875 best Validation loss: 2236.73681640625\n",
      "Epoch: 43500 Loss: 1924.5640869140625 Validation loss: 2272.937744140625 best Validation loss: 2236.73681640625\n",
      "Epoch: 43600 Loss: 1910.5726318359375 Validation loss: 2256.46875 best Validation loss: 2236.73681640625\n",
      "Epoch: 43700 Loss: 2061.99267578125 Validation loss: 2318.33349609375 best Validation loss: 2236.73681640625\n",
      "Epoch: 43800 Loss: 2114.77490234375 Validation loss: 2325.32568359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 43900 Loss: 1938.73779296875 Validation loss: 2293.020263671875 best Validation loss: 2236.73681640625\n",
      "Epoch: 44000 Loss: 1940.44482421875 Validation loss: 2253.907958984375 best Validation loss: 2236.73681640625\n",
      "Epoch: 44100 Loss: 1981.16455078125 Validation loss: 2333.279541015625 best Validation loss: 2236.73681640625\n",
      "Epoch: 44200 Loss: 1924.860595703125 Validation loss: 2267.30908203125 best Validation loss: 2236.73681640625\n",
      "Epoch: 44300 Loss: 1893.2279052734375 Validation loss: 2247.98583984375 best Validation loss: 2236.73681640625\n",
      "Epoch: 44400 Loss: 1944.898681640625 Validation loss: 2262.869140625 best Validation loss: 2236.73681640625\n",
      "Epoch: 44500 Loss: 1898.044921875 Validation loss: 2264.510498046875 best Validation loss: 2236.73681640625\n",
      "Epoch: 44600 Loss: 1895.9083251953125 Validation loss: 2251.26318359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 44700 Loss: 1894.8760986328125 Validation loss: 2259.288818359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 44800 Loss: 1918.5745849609375 Validation loss: 2270.246826171875 best Validation loss: 2236.73681640625\n",
      "Epoch: 44900 Loss: 1889.8016357421875 Validation loss: 2251.709228515625 best Validation loss: 2236.73681640625\n",
      "Epoch: 45000 Loss: 1889.569091796875 Validation loss: 2249.134521484375 best Validation loss: 2236.73681640625\n",
      "Epoch: 45100 Loss: 1902.5074462890625 Validation loss: 2256.630859375 best Validation loss: 2236.73681640625\n",
      "Epoch: 45200 Loss: 1984.0380859375 Validation loss: 2410.39453125 best Validation loss: 2236.73681640625\n",
      "Epoch: 45300 Loss: 1957.94384765625 Validation loss: 2277.398193359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 45400 Loss: 1902.328369140625 Validation loss: 2266.34716796875 best Validation loss: 2236.73681640625\n",
      "Epoch: 45500 Loss: 1908.372802734375 Validation loss: 2256.945068359375 best Validation loss: 2236.73681640625\n",
      "Epoch: 45600 Loss: 2033.258056640625 Validation loss: 2344.018798828125 best Validation loss: 2236.73681640625\n",
      "Epoch: 45700 Loss: 1890.6087646484375 Validation loss: 2261.6484375 best Validation loss: 2236.73681640625\n",
      "Epoch: 45800 Loss: 2028.9664306640625 Validation loss: 2358.786865234375 best Validation loss: 2236.73681640625\n",
      "Epoch: 45900 Loss: 1909.80322265625 Validation loss: 2280.235595703125 best Validation loss: 2236.73681640625\n",
      "Epoch: 46000 Loss: 1923.5565185546875 Validation loss: 2340.128662109375 best Validation loss: 2236.73681640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUMBER_OF_EPOCH):\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 10\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[1;32m     12\u001b[0m     training_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(training_loss, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mmlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "last_val_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for n in range(NUMBER_OF_EPOCH):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)[:, 0]\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    training_loss = np.append(training_loss, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = model(X_val)[:, 0]\n",
    "    val_loss = loss_fn(y_pred, y_val)\n",
    "    validation_loss = np.append(validation_loss, val_loss.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model, 'mlp_model.pth')\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(f'Epoch: {n} Loss: {loss.item()}'f' Validation loss: {val_loss.item()}'f' best Validation loss: {best_val_loss}')\n",
    "        \n",
    "    if last_val_loss < val_loss:\n",
    "        count += 1\n",
    "        if count == 25:\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "    last_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x311e3cb50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/cy9kvd894_bfkfb71dsbxt2w0000gn/T/ipykernel_16962/3527652370.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mlp_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "model = torch.load('mlp_model.pth')\n",
    "model.eval()\n",
    "\n",
    "index = df['id']\n",
    "df = df.drop(['id'], axis=1)\n",
    "X = pre.encoder(df)\n",
    "# X = pre.standardize(X, scaler=pickle.load(open('scaler.pkl', 'rb')))\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_pred = model(X)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(y_pred.detach().numpy(), columns=['price'])\n",
    "df = pd.concat([index, df], axis=1)\n",
    "df.rename(columns={'price': 'answer'}, inplace=True)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
