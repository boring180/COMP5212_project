{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCH = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import preprossesing as pre\n",
    "import math\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(df):\n",
    "    \n",
    "    df = pre.standardize(df)\n",
    "    df = pre.encoder(df)\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    train, val = pre.test_validation_split(df)\n",
    "    \n",
    "    y_train = torch.tensor(train['price'].values, dtype=torch.float32)\n",
    "    X_train = train.drop(['price'], axis=1)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    \n",
    "    y_val = torch.tensor(val['price'].values, dtype=torch.float32)\n",
    "    X_val = val.drop(['price'], axis=1)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (manufacturers): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=143, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (gearbox_type): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fuel_type): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (registration_fees): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (engine_capacity): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(9, 64),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(64, 16),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(16, 1) \n",
    "        )\n",
    "        \n",
    "        self.manufacturers = nn.Sequential(\n",
    "        nn.Linear(9, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(143, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.gearbox_type = nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fuel_type = nn.Sequential(\n",
    "        nn.Linear(4, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.registration_fees = nn.Sequential(\n",
    "        nn.Linear(12, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.engine_capacity = nn.Sequential(\n",
    "        nn.Linear(15, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        manufacutre_output = self.manufacturers(x[:, 3:12])\n",
    "        model_output = self.model(x[:, 12:155])\n",
    "        gearbox_output = self.gearbox_type(x[:, 155:158])\n",
    "        fuel_output = self.fuel_type(x[:, 158:162])\n",
    "        registration_fees_output = self.registration_fees(x[:, 162:174])\n",
    "        engine_capacity_output = self.engine_capacity(x[:, 174:189])\n",
    "        x = torch.cat((x[:, :3], manufacutre_output, model_output, gearbox_output, fuel_output, registration_fees_output, engine_capacity_output), 1)\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "model = mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_train, y_train, X_val, y_val = prepare_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 19184.474609375 Validation loss: 18926.70703125 best Validation loss: 18926.70703125\n",
      "Epoch: 100 Loss: 19175.939453125 Validation loss: 18917.77734375 best Validation loss: 18917.77734375\n",
      "Epoch: 200 Loss: 19045.326171875 Validation loss: 18782.59765625 best Validation loss: 18782.59765625\n",
      "Epoch: 300 Loss: 18231.7265625 Validation loss: 17950.734375 best Validation loss: 17950.734375\n",
      "Epoch: 400 Loss: 15748.951171875 Validation loss: 15427.95703125 best Validation loss: 15427.95703125\n",
      "Epoch: 500 Loss: 11135.9638671875 Validation loss: 10772.216796875 best Validation loss: 10772.216796875\n",
      "Epoch: 600 Loss: 7836.1484375 Validation loss: 7545.3173828125 best Validation loss: 7545.3173828125\n",
      "Epoch: 700 Loss: 6877.78271484375 Validation loss: 6612.80126953125 best Validation loss: 6612.80126953125\n",
      "Epoch: 800 Loss: 6167.3798828125 Validation loss: 5904.58251953125 best Validation loss: 5904.58251953125\n",
      "Epoch: 900 Loss: 5562.28076171875 Validation loss: 5301.61572265625 best Validation loss: 5301.61572265625\n",
      "Epoch: 1000 Loss: 5061.234375 Validation loss: 4805.4248046875 best Validation loss: 4805.4248046875\n",
      "Epoch: 1100 Loss: 4681.19091796875 Validation loss: 4434.06005859375 best Validation loss: 4434.06005859375\n",
      "Epoch: 1200 Loss: 4418.1669921875 Validation loss: 4183.15576171875 best Validation loss: 4183.15576171875\n",
      "Epoch: 1300 Loss: 4236.763671875 Validation loss: 4015.7333984375 best Validation loss: 4015.7333984375\n",
      "Epoch: 1400 Loss: 4097.4541015625 Validation loss: 3890.6513671875 best Validation loss: 3890.6513671875\n",
      "Epoch: 1500 Loss: 3978.204345703125 Validation loss: 3785.33349609375 best Validation loss: 3785.33349609375\n",
      "Epoch: 1600 Loss: 3871.49853515625 Validation loss: 3692.181396484375 best Validation loss: 3692.181396484375\n",
      "Epoch: 1700 Loss: 3776.345947265625 Validation loss: 3610.38525390625 best Validation loss: 3610.38525390625\n",
      "Epoch: 1800 Loss: 3693.107421875 Validation loss: 3539.632568359375 best Validation loss: 3539.632568359375\n",
      "Epoch: 1900 Loss: 3620.9228515625 Validation loss: 3478.4296875 best Validation loss: 3478.4296875\n",
      "Epoch: 2000 Loss: 3560.697021484375 Validation loss: 3427.202880859375 best Validation loss: 3427.202880859375\n",
      "Epoch: 2100 Loss: 3510.3271484375 Validation loss: 3384.02197265625 best Validation loss: 3384.02197265625\n",
      "Epoch: 2200 Loss: 3468.193359375 Validation loss: 3347.392333984375 best Validation loss: 3347.392333984375\n",
      "Epoch: 2300 Loss: 3432.807373046875 Validation loss: 3316.08544921875 best Validation loss: 3316.08544921875\n",
      "Epoch: 2400 Loss: 3402.93603515625 Validation loss: 3288.705322265625 best Validation loss: 3288.705322265625\n",
      "Epoch: 2500 Loss: 3377.011962890625 Validation loss: 3264.361572265625 best Validation loss: 3264.361572265625\n",
      "Epoch: 2600 Loss: 3353.70263671875 Validation loss: 3242.04541015625 best Validation loss: 3242.04541015625\n",
      "Epoch: 2700 Loss: 3331.97607421875 Validation loss: 3220.9169921875 best Validation loss: 3220.9169921875\n",
      "Epoch: 2800 Loss: 3311.511474609375 Validation loss: 3200.595947265625 best Validation loss: 3200.595947265625\n",
      "Epoch: 2900 Loss: 3291.970947265625 Validation loss: 3181.57421875 best Validation loss: 3181.57421875\n",
      "Epoch: 3000 Loss: 3272.975830078125 Validation loss: 3163.30322265625 best Validation loss: 3163.30322265625\n",
      "Epoch: 3100 Loss: 3254.413330078125 Validation loss: 3145.27783203125 best Validation loss: 3145.27783203125\n",
      "Epoch: 3200 Loss: 3235.970703125 Validation loss: 3127.190673828125 best Validation loss: 3127.190673828125\n",
      "Epoch: 3300 Loss: 3217.98388671875 Validation loss: 3109.453857421875 best Validation loss: 3109.453857421875\n",
      "Epoch: 3400 Loss: 3200.499267578125 Validation loss: 3092.1357421875 best Validation loss: 3092.1357421875\n",
      "Epoch: 3500 Loss: 3183.80712890625 Validation loss: 3075.656005859375 best Validation loss: 3075.656005859375\n",
      "Epoch: 3600 Loss: 3167.342529296875 Validation loss: 3059.241943359375 best Validation loss: 3059.241943359375\n",
      "Epoch: 3700 Loss: 3150.94482421875 Validation loss: 3042.59375 best Validation loss: 3042.59375\n",
      "Epoch: 3800 Loss: 3134.496337890625 Validation loss: 3025.958740234375 best Validation loss: 3025.958740234375\n",
      "Epoch: 3900 Loss: 3118.228515625 Validation loss: 3009.159423828125 best Validation loss: 3009.159423828125\n",
      "Epoch: 4000 Loss: 3101.748779296875 Validation loss: 2991.61767578125 best Validation loss: 2991.61767578125\n",
      "Epoch: 4100 Loss: 3085.104248046875 Validation loss: 2973.914306640625 best Validation loss: 2973.914306640625\n",
      "Epoch: 4200 Loss: 3068.20166015625 Validation loss: 2955.158935546875 best Validation loss: 2955.158935546875\n",
      "Epoch: 4300 Loss: 3050.916748046875 Validation loss: 2935.83984375 best Validation loss: 2935.83984375\n",
      "Epoch: 4400 Loss: 3032.68896484375 Validation loss: 2915.338623046875 best Validation loss: 2915.338623046875\n",
      "Epoch: 4500 Loss: 3012.54736328125 Validation loss: 2892.169189453125 best Validation loss: 2892.169189453125\n",
      "Epoch: 4600 Loss: 2990.723388671875 Validation loss: 2866.485107421875 best Validation loss: 2866.485107421875\n",
      "Epoch: 4700 Loss: 2968.244873046875 Validation loss: 2839.966796875 best Validation loss: 2839.966796875\n",
      "Epoch: 4800 Loss: 2945.64599609375 Validation loss: 2813.142333984375 best Validation loss: 2813.142333984375\n",
      "Epoch: 4900 Loss: 2923.5693359375 Validation loss: 2786.939697265625 best Validation loss: 2786.939697265625\n",
      "Epoch: 5000 Loss: 2902.208740234375 Validation loss: 2761.40966796875 best Validation loss: 2761.40966796875\n",
      "Epoch: 5100 Loss: 2882.148193359375 Validation loss: 2737.101806640625 best Validation loss: 2737.101806640625\n",
      "Epoch: 5200 Loss: 2863.52685546875 Validation loss: 2715.134521484375 best Validation loss: 2715.134521484375\n",
      "Epoch: 5300 Loss: 2845.2060546875 Validation loss: 2694.550537109375 best Validation loss: 2694.550537109375\n",
      "Epoch: 5400 Loss: 2828.589111328125 Validation loss: 2676.1064453125 best Validation loss: 2676.1064453125\n",
      "Epoch: 5500 Loss: 2813.74072265625 Validation loss: 2659.23486328125 best Validation loss: 2659.23486328125\n",
      "Epoch: 5600 Loss: 2799.609375 Validation loss: 2644.485595703125 best Validation loss: 2644.485595703125\n",
      "Epoch: 5700 Loss: 2786.516845703125 Validation loss: 2631.625244140625 best Validation loss: 2631.625244140625\n",
      "Epoch: 5800 Loss: 2774.287109375 Validation loss: 2620.0341796875 best Validation loss: 2620.0341796875\n",
      "Epoch: 5900 Loss: 2762.8720703125 Validation loss: 2609.21337890625 best Validation loss: 2609.21337890625\n",
      "Epoch: 6000 Loss: 2751.912109375 Validation loss: 2598.5380859375 best Validation loss: 2598.5380859375\n",
      "Epoch: 6100 Loss: 2741.322265625 Validation loss: 2588.32421875 best Validation loss: 2588.32421875\n",
      "Epoch: 6200 Loss: 2731.523193359375 Validation loss: 2578.752685546875 best Validation loss: 2578.752685546875\n",
      "Epoch: 6300 Loss: 2722.1298828125 Validation loss: 2569.704833984375 best Validation loss: 2569.704833984375\n",
      "Epoch: 6400 Loss: 2713.0361328125 Validation loss: 2561.391845703125 best Validation loss: 2561.391845703125\n",
      "Epoch: 6500 Loss: 2703.475341796875 Validation loss: 2552.971923828125 best Validation loss: 2552.971923828125\n",
      "Epoch: 6600 Loss: 2693.9423828125 Validation loss: 2545.008544921875 best Validation loss: 2545.008544921875\n",
      "Epoch: 6700 Loss: 2684.925048828125 Validation loss: 2536.354736328125 best Validation loss: 2536.354736328125\n",
      "Epoch: 6800 Loss: 2676.314697265625 Validation loss: 2528.093994140625 best Validation loss: 2528.093994140625\n",
      "Epoch: 6900 Loss: 2667.5947265625 Validation loss: 2519.674072265625 best Validation loss: 2519.674072265625\n",
      "Epoch: 7000 Loss: 2659.039306640625 Validation loss: 2510.880859375 best Validation loss: 2510.880859375\n",
      "Epoch: 7100 Loss: 2651.158447265625 Validation loss: 2503.02685546875 best Validation loss: 2503.02685546875\n",
      "Epoch: 7200 Loss: 2643.697998046875 Validation loss: 2495.931884765625 best Validation loss: 2495.931884765625\n",
      "Epoch: 7300 Loss: 2635.963134765625 Validation loss: 2488.73828125 best Validation loss: 2488.73828125\n",
      "Epoch: 7400 Loss: 2628.591064453125 Validation loss: 2482.42626953125 best Validation loss: 2482.42626953125\n",
      "Epoch: 7500 Loss: 2621.844482421875 Validation loss: 2477.25830078125 best Validation loss: 2477.25830078125\n",
      "Epoch: 7600 Loss: 2615.29736328125 Validation loss: 2472.83935546875 best Validation loss: 2472.83935546875\n",
      "Epoch: 7700 Loss: 2608.92138671875 Validation loss: 2468.3330078125 best Validation loss: 2468.3330078125\n",
      "Epoch: 7800 Loss: 2602.984130859375 Validation loss: 2463.963134765625 best Validation loss: 2463.963134765625\n",
      "Epoch: 7900 Loss: 2596.960205078125 Validation loss: 2459.78369140625 best Validation loss: 2459.78369140625\n",
      "Epoch: 8000 Loss: 2590.899658203125 Validation loss: 2454.99609375 best Validation loss: 2454.99609375\n",
      "Epoch: 8100 Loss: 2585.07275390625 Validation loss: 2451.506103515625 best Validation loss: 2451.506103515625\n",
      "Epoch: 8200 Loss: 2579.64501953125 Validation loss: 2448.169921875 best Validation loss: 2448.169921875\n",
      "Epoch: 8300 Loss: 2574.300048828125 Validation loss: 2445.549072265625 best Validation loss: 2445.549072265625\n",
      "Epoch: 8400 Loss: 2568.727783203125 Validation loss: 2442.609375 best Validation loss: 2442.609375\n",
      "Epoch: 8500 Loss: 2562.921142578125 Validation loss: 2437.68212890625 best Validation loss: 2437.665771484375\n",
      "Epoch: 8600 Loss: 2557.551513671875 Validation loss: 2433.536376953125 best Validation loss: 2433.536376953125\n",
      "Epoch: 8700 Loss: 2552.542236328125 Validation loss: 2430.1376953125 best Validation loss: 2430.1376953125\n",
      "Epoch: 8800 Loss: 2547.542724609375 Validation loss: 2425.88232421875 best Validation loss: 2425.88232421875\n",
      "Epoch: 8900 Loss: 2542.8525390625 Validation loss: 2420.9267578125 best Validation loss: 2420.9267578125\n",
      "Epoch: 9000 Loss: 2538.242431640625 Validation loss: 2418.557373046875 best Validation loss: 2418.556396484375\n",
      "Epoch: 9100 Loss: 2533.532958984375 Validation loss: 2415.48681640625 best Validation loss: 2415.4375\n",
      "Epoch: 9200 Loss: 2528.506591796875 Validation loss: 2414.379638671875 best Validation loss: 2414.201171875\n",
      "Epoch: 9300 Loss: 2523.5830078125 Validation loss: 2413.622802734375 best Validation loss: 2413.519775390625\n",
      "Epoch: 9400 Loss: 2518.3095703125 Validation loss: 2412.754150390625 best Validation loss: 2412.25\n",
      "Epoch: 9500 Loss: 2512.47607421875 Validation loss: 2410.20556640625 best Validation loss: 2410.077392578125\n",
      "Epoch: 9600 Loss: 2507.136474609375 Validation loss: 2406.76025390625 best Validation loss: 2406.755859375\n",
      "Epoch: 9700 Loss: 2502.090087890625 Validation loss: 2405.070068359375 best Validation loss: 2404.8173828125\n",
      "Epoch: 9800 Loss: 2496.497314453125 Validation loss: 2402.847900390625 best Validation loss: 2402.276611328125\n",
      "Epoch: 9900 Loss: 2490.5439453125 Validation loss: 2396.9814453125 best Validation loss: 2396.9814453125\n",
      "Epoch: 10000 Loss: 2485.1181640625 Validation loss: 2390.11474609375 best Validation loss: 2390.05126953125\n",
      "Epoch: 10100 Loss: 2479.76904296875 Validation loss: 2383.49267578125 best Validation loss: 2383.49267578125\n",
      "Epoch: 10200 Loss: 2474.844970703125 Validation loss: 2383.59033203125 best Validation loss: 2382.842041015625\n",
      "Epoch: 10300 Loss: 2470.68505859375 Validation loss: 2378.955322265625 best Validation loss: 2378.472900390625\n",
      "Epoch: 10400 Loss: 2466.934326171875 Validation loss: 2376.14404296875 best Validation loss: 2376.14404296875\n",
      "Epoch: 10500 Loss: 2462.530517578125 Validation loss: 2371.966796875 best Validation loss: 2371.966796875\n",
      "Epoch: 10600 Loss: 2459.1328125 Validation loss: 2368.700439453125 best Validation loss: 2368.700439453125\n",
      "Epoch: 10700 Loss: 2456.03173828125 Validation loss: 2364.91552734375 best Validation loss: 2364.737060546875\n",
      "Epoch: 10800 Loss: 2452.593017578125 Validation loss: 2360.31298828125 best Validation loss: 2360.2734375\n",
      "Epoch: 10900 Loss: 2449.590576171875 Validation loss: 2357.91552734375 best Validation loss: 2357.91552734375\n",
      "Epoch: 11000 Loss: 2446.8671875 Validation loss: 2359.235107421875 best Validation loss: 2356.75732421875\n",
      "Epoch: 11100 Loss: 2444.57373046875 Validation loss: 2360.275634765625 best Validation loss: 2356.75732421875\n",
      "Epoch: 11200 Loss: 2442.458251953125 Validation loss: 2359.958740234375 best Validation loss: 2356.75732421875\n",
      "Epoch: 11300 Loss: 2440.056640625 Validation loss: 2359.58056640625 best Validation loss: 2356.75732421875\n",
      "Epoch: 11400 Loss: 2437.9697265625 Validation loss: 2356.79931640625 best Validation loss: 2356.352783203125\n",
      "Epoch: 11500 Loss: 2436.1123046875 Validation loss: 2355.5244140625 best Validation loss: 2355.081298828125\n",
      "Epoch: 11600 Loss: 2434.46337890625 Validation loss: 2354.194091796875 best Validation loss: 2353.87890625\n",
      "Epoch: 11700 Loss: 2432.931396484375 Validation loss: 2353.84765625 best Validation loss: 2353.151123046875\n",
      "Epoch: 11800 Loss: 2431.48779296875 Validation loss: 2353.93701171875 best Validation loss: 2353.0869140625\n",
      "Epoch: 11900 Loss: 2430.03369140625 Validation loss: 2352.98876953125 best Validation loss: 2352.564697265625\n",
      "Epoch: 12000 Loss: 2428.049560546875 Validation loss: 2355.1318359375 best Validation loss: 2352.56298828125\n",
      "Epoch: 12100 Loss: 2426.63134765625 Validation loss: 2357.317626953125 best Validation loss: 2352.56298828125\n",
      "Epoch: 12200 Loss: 2425.4453125 Validation loss: 2358.373046875 best Validation loss: 2352.56298828125\n",
      "Epoch: 12300 Loss: 2424.2666015625 Validation loss: 2358.5400390625 best Validation loss: 2352.56298828125\n",
      "Epoch: 12400 Loss: 2423.1494140625 Validation loss: 2359.21044921875 best Validation loss: 2352.56298828125\n",
      "Epoch: 12500 Loss: 2421.92626953125 Validation loss: 2359.871826171875 best Validation loss: 2352.56298828125\n",
      "Epoch: 12600 Loss: 2420.76611328125 Validation loss: 2361.231201171875 best Validation loss: 2352.56298828125\n",
      "Epoch: 12700 Loss: 2419.69921875 Validation loss: 2360.0263671875 best Validation loss: 2352.56298828125\n",
      "Epoch: 12800 Loss: 2418.607421875 Validation loss: 2358.76611328125 best Validation loss: 2352.56298828125\n",
      "Epoch: 12900 Loss: 2417.22705078125 Validation loss: 2356.525390625 best Validation loss: 2352.56298828125\n",
      "Epoch: 13000 Loss: 2415.92333984375 Validation loss: 2352.669677734375 best Validation loss: 2352.359619140625\n",
      "Epoch: 13100 Loss: 2414.502197265625 Validation loss: 2349.83154296875 best Validation loss: 2349.74462890625\n",
      "Epoch: 13200 Loss: 2413.259765625 Validation loss: 2348.792236328125 best Validation loss: 2348.11328125\n",
      "Epoch: 13300 Loss: 2412.15771484375 Validation loss: 2348.001708984375 best Validation loss: 2347.400390625\n",
      "Epoch: 13400 Loss: 2411.13525390625 Validation loss: 2347.14892578125 best Validation loss: 2346.8408203125\n",
      "Epoch: 13500 Loss: 2410.20068359375 Validation loss: 2347.18603515625 best Validation loss: 2346.427734375\n",
      "Epoch: 13600 Loss: 2409.2236328125 Validation loss: 2346.07958984375 best Validation loss: 2345.62451171875\n",
      "Epoch: 13700 Loss: 2407.866455078125 Validation loss: 2345.451171875 best Validation loss: 2344.5791015625\n",
      "Epoch: 13800 Loss: 2406.6123046875 Validation loss: 2342.443115234375 best Validation loss: 2342.117919921875\n",
      "Epoch: 13900 Loss: 2403.17333984375 Validation loss: 2337.5478515625 best Validation loss: 2337.5478515625\n",
      "Epoch: 14000 Loss: 2399.33251953125 Validation loss: 2337.334228515625 best Validation loss: 2336.361328125\n",
      "Epoch: 14100 Loss: 2396.48681640625 Validation loss: 2337.482177734375 best Validation loss: 2336.361328125\n",
      "Epoch: 14200 Loss: 2394.362548828125 Validation loss: 2338.228515625 best Validation loss: 2336.361328125\n",
      "Epoch: 14300 Loss: 2392.414306640625 Validation loss: 2341.079345703125 best Validation loss: 2336.361328125\n",
      "Epoch: 14400 Loss: 2390.76513671875 Validation loss: 2342.5185546875 best Validation loss: 2336.361328125\n",
      "Epoch: 14500 Loss: 2389.158203125 Validation loss: 2343.4892578125 best Validation loss: 2336.361328125\n",
      "Epoch: 14600 Loss: 2387.681396484375 Validation loss: 2344.281005859375 best Validation loss: 2336.361328125\n",
      "Epoch: 14700 Loss: 2386.289306640625 Validation loss: 2345.9072265625 best Validation loss: 2336.361328125\n",
      "Epoch: 14800 Loss: 2384.86474609375 Validation loss: 2347.02783203125 best Validation loss: 2336.361328125\n",
      "Epoch: 14900 Loss: 2383.43017578125 Validation loss: 2346.129638671875 best Validation loss: 2336.361328125\n",
      "Epoch: 15000 Loss: 2382.034423828125 Validation loss: 2345.741455078125 best Validation loss: 2336.361328125\n",
      "Epoch: 15100 Loss: 2380.783447265625 Validation loss: 2345.3447265625 best Validation loss: 2336.361328125\n",
      "Epoch: 15200 Loss: 2379.588623046875 Validation loss: 2344.748779296875 best Validation loss: 2336.361328125\n",
      "Epoch: 15300 Loss: 2378.323974609375 Validation loss: 2344.550537109375 best Validation loss: 2336.361328125\n",
      "Epoch: 15400 Loss: 2377.085205078125 Validation loss: 2343.665771484375 best Validation loss: 2336.361328125\n",
      "Epoch: 15500 Loss: 2375.643310546875 Validation loss: 2345.702880859375 best Validation loss: 2336.361328125\n",
      "Epoch: 15600 Loss: 2374.501953125 Validation loss: 2346.978515625 best Validation loss: 2336.361328125\n",
      "Epoch: 15700 Loss: 2373.288330078125 Validation loss: 2348.451416015625 best Validation loss: 2336.361328125\n",
      "Epoch: 15800 Loss: 2372.118408203125 Validation loss: 2349.101318359375 best Validation loss: 2336.361328125\n",
      "Epoch: 15900 Loss: 2370.83740234375 Validation loss: 2351.2109375 best Validation loss: 2336.361328125\n",
      "Epoch: 16000 Loss: 2369.574951171875 Validation loss: 2351.5029296875 best Validation loss: 2336.361328125\n",
      "Epoch: 16100 Loss: 2368.400634765625 Validation loss: 2352.56787109375 best Validation loss: 2336.361328125\n",
      "Epoch: 16200 Loss: 2367.093017578125 Validation loss: 2353.649658203125 best Validation loss: 2336.361328125\n",
      "Epoch: 16300 Loss: 2365.87353515625 Validation loss: 2356.365234375 best Validation loss: 2336.361328125\n",
      "Epoch: 16400 Loss: 2364.57666015625 Validation loss: 2356.117919921875 best Validation loss: 2336.361328125\n",
      "Epoch: 16500 Loss: 2363.5283203125 Validation loss: 2357.882080078125 best Validation loss: 2336.361328125\n",
      "Epoch: 16600 Loss: 2362.6083984375 Validation loss: 2359.503173828125 best Validation loss: 2336.361328125\n",
      "Epoch: 16700 Loss: 2361.568603515625 Validation loss: 2360.297607421875 best Validation loss: 2336.361328125\n",
      "Epoch: 16800 Loss: 2360.647216796875 Validation loss: 2362.787841796875 best Validation loss: 2336.361328125\n",
      "Epoch: 16900 Loss: 2359.755615234375 Validation loss: 2365.387939453125 best Validation loss: 2336.361328125\n",
      "Epoch: 17000 Loss: 2357.064697265625 Validation loss: 2364.450439453125 best Validation loss: 2336.361328125\n",
      "Epoch: 17100 Loss: 2355.423583984375 Validation loss: 2367.54052734375 best Validation loss: 2336.361328125\n",
      "Epoch: 17200 Loss: 2354.169921875 Validation loss: 2369.68017578125 best Validation loss: 2336.361328125\n",
      "Epoch: 17300 Loss: 2353.11083984375 Validation loss: 2369.33447265625 best Validation loss: 2336.361328125\n",
      "Epoch: 17400 Loss: 2352.11083984375 Validation loss: 2369.432373046875 best Validation loss: 2336.361328125\n",
      "Epoch: 17500 Loss: 2351.015625 Validation loss: 2367.85546875 best Validation loss: 2336.361328125\n",
      "Epoch: 17600 Loss: 2350.029296875 Validation loss: 2366.64306640625 best Validation loss: 2336.361328125\n",
      "Epoch: 17700 Loss: 2349.1416015625 Validation loss: 2364.79345703125 best Validation loss: 2336.361328125\n",
      "Epoch: 17800 Loss: 2348.352294921875 Validation loss: 2363.95947265625 best Validation loss: 2336.361328125\n",
      "Epoch: 17900 Loss: 2347.443603515625 Validation loss: 2361.822509765625 best Validation loss: 2336.361328125\n",
      "Epoch: 18000 Loss: 2346.652587890625 Validation loss: 2361.64404296875 best Validation loss: 2336.361328125\n",
      "Epoch: 18100 Loss: 2345.839111328125 Validation loss: 2363.187744140625 best Validation loss: 2336.361328125\n",
      "Epoch: 18200 Loss: 2345.1083984375 Validation loss: 2363.8095703125 best Validation loss: 2336.361328125\n",
      "Epoch: 18300 Loss: 2344.36767578125 Validation loss: 2365.547119140625 best Validation loss: 2336.361328125\n",
      "Epoch: 18400 Loss: 2343.66064453125 Validation loss: 2367.1689453125 best Validation loss: 2336.361328125\n",
      "Epoch: 18500 Loss: 2342.802490234375 Validation loss: 2366.20703125 best Validation loss: 2336.361328125\n",
      "Epoch: 18600 Loss: 2342.173828125 Validation loss: 2366.069580078125 best Validation loss: 2336.361328125\n",
      "Epoch: 18700 Loss: 2341.447509765625 Validation loss: 2366.6416015625 best Validation loss: 2336.361328125\n",
      "Epoch: 18800 Loss: 2340.779052734375 Validation loss: 2366.998046875 best Validation loss: 2336.361328125\n",
      "Epoch: 18900 Loss: 2339.977783203125 Validation loss: 2368.378662109375 best Validation loss: 2336.361328125\n",
      "Epoch: 19000 Loss: 2339.374267578125 Validation loss: 2370.879638671875 best Validation loss: 2336.361328125\n",
      "Epoch: 19100 Loss: 2338.614013671875 Validation loss: 2371.658203125 best Validation loss: 2336.361328125\n",
      "Epoch: 19200 Loss: 2337.81396484375 Validation loss: 2373.94140625 best Validation loss: 2336.361328125\n",
      "Epoch: 19300 Loss: 2337.076416015625 Validation loss: 2376.242431640625 best Validation loss: 2336.361328125\n",
      "Epoch: 19400 Loss: 2336.26416015625 Validation loss: 2379.02490234375 best Validation loss: 2336.361328125\n",
      "Epoch: 19500 Loss: 2335.509765625 Validation loss: 2381.499755859375 best Validation loss: 2336.361328125\n",
      "Epoch: 19600 Loss: 2334.88134765625 Validation loss: 2382.34765625 best Validation loss: 2336.361328125\n",
      "Epoch: 19700 Loss: 2333.9423828125 Validation loss: 2371.76220703125 best Validation loss: 2336.361328125\n",
      "Epoch: 19800 Loss: 2332.91748046875 Validation loss: 2365.382080078125 best Validation loss: 2336.361328125\n",
      "Epoch: 19900 Loss: 2331.587646484375 Validation loss: 2356.247802734375 best Validation loss: 2336.361328125\n",
      "Epoch: 20000 Loss: 2330.721923828125 Validation loss: 2351.41064453125 best Validation loss: 2336.361328125\n",
      "Epoch: 20100 Loss: 2329.583251953125 Validation loss: 2343.625 best Validation loss: 2336.361328125\n",
      "Epoch: 20200 Loss: 2328.64306640625 Validation loss: 2342.330078125 best Validation loss: 2336.361328125\n",
      "Epoch: 20300 Loss: 2327.88427734375 Validation loss: 2342.18310546875 best Validation loss: 2336.361328125\n",
      "Epoch: 20400 Loss: 2327.1611328125 Validation loss: 2343.349853515625 best Validation loss: 2336.361328125\n",
      "Epoch: 20500 Loss: 2326.173828125 Validation loss: 2341.37646484375 best Validation loss: 2336.361328125\n",
      "Epoch: 20600 Loss: 2325.345947265625 Validation loss: 2340.814453125 best Validation loss: 2336.361328125\n",
      "Epoch: 20700 Loss: 2324.727783203125 Validation loss: 2340.318603515625 best Validation loss: 2336.361328125\n",
      "Epoch: 20800 Loss: 2323.963623046875 Validation loss: 2339.063232421875 best Validation loss: 2336.361328125\n",
      "Epoch: 20900 Loss: 2323.17822265625 Validation loss: 2338.240966796875 best Validation loss: 2336.361328125\n",
      "Epoch: 21000 Loss: 2322.3408203125 Validation loss: 2338.287353515625 best Validation loss: 2336.361328125\n",
      "Epoch: 21100 Loss: 2321.595947265625 Validation loss: 2336.66162109375 best Validation loss: 2336.228515625\n",
      "Epoch: 21200 Loss: 2320.9169921875 Validation loss: 2334.7412109375 best Validation loss: 2334.209228515625\n",
      "Epoch: 21300 Loss: 2320.1240234375 Validation loss: 2333.064208984375 best Validation loss: 2332.811767578125\n",
      "Epoch: 21400 Loss: 2319.42529296875 Validation loss: 2332.993896484375 best Validation loss: 2332.136474609375\n",
      "Epoch: 21500 Loss: 2318.666259765625 Validation loss: 2331.80126953125 best Validation loss: 2331.08740234375\n",
      "Epoch: 21600 Loss: 2317.924072265625 Validation loss: 2328.274169921875 best Validation loss: 2327.240234375\n",
      "Epoch: 21700 Loss: 2317.188720703125 Validation loss: 2326.200439453125 best Validation loss: 2325.6162109375\n",
      "Epoch: 21800 Loss: 2316.512451171875 Validation loss: 2325.306884765625 best Validation loss: 2324.73583984375\n",
      "Epoch: 21900 Loss: 2315.55322265625 Validation loss: 2320.97607421875 best Validation loss: 2320.2744140625\n",
      "Epoch: 22000 Loss: 2314.45361328125 Validation loss: 2315.552978515625 best Validation loss: 2315.2822265625\n",
      "Epoch: 22100 Loss: 2313.203369140625 Validation loss: 2312.25634765625 best Validation loss: 2311.987548828125\n",
      "Epoch: 22200 Loss: 2312.10888671875 Validation loss: 2309.68994140625 best Validation loss: 2309.177490234375\n",
      "Epoch: 22300 Loss: 2311.037841796875 Validation loss: 2306.35205078125 best Validation loss: 2305.7607421875\n",
      "Epoch: 22400 Loss: 2310.058349609375 Validation loss: 2303.592041015625 best Validation loss: 2303.15380859375\n",
      "Epoch: 22500 Loss: 2308.89013671875 Validation loss: 2302.5478515625 best Validation loss: 2302.470947265625\n",
      "Epoch: 22600 Loss: 2307.590576171875 Validation loss: 2300.38427734375 best Validation loss: 2299.279541015625\n",
      "Epoch: 22700 Loss: 2306.6513671875 Validation loss: 2299.856201171875 best Validation loss: 2298.767822265625\n",
      "Epoch: 22800 Loss: 2305.896728515625 Validation loss: 2300.253173828125 best Validation loss: 2298.767822265625\n",
      "Epoch: 22900 Loss: 2304.926513671875 Validation loss: 2293.262939453125 best Validation loss: 2293.262939453125\n",
      "Epoch: 23000 Loss: 2303.973388671875 Validation loss: 2289.079833984375 best Validation loss: 2288.42724609375\n",
      "Epoch: 23100 Loss: 2303.232177734375 Validation loss: 2289.958740234375 best Validation loss: 2288.42724609375\n",
      "Epoch: 23200 Loss: 2302.43798828125 Validation loss: 2288.154052734375 best Validation loss: 2287.609375\n",
      "Epoch: 23300 Loss: 2301.6669921875 Validation loss: 2287.697509765625 best Validation loss: 2287.147705078125\n",
      "Epoch: 23400 Loss: 2300.86865234375 Validation loss: 2287.127685546875 best Validation loss: 2287.058837890625\n",
      "Epoch: 23500 Loss: 2300.145751953125 Validation loss: 2287.03515625 best Validation loss: 2286.039794921875\n",
      "Epoch: 23600 Loss: 2299.33544921875 Validation loss: 2289.1806640625 best Validation loss: 2285.97802734375\n",
      "Epoch: 23700 Loss: 2298.67236328125 Validation loss: 2290.8349609375 best Validation loss: 2285.97802734375\n",
      "Epoch: 23800 Loss: 2297.455078125 Validation loss: 2288.733154296875 best Validation loss: 2285.97802734375\n",
      "Epoch: 23900 Loss: 2296.72802734375 Validation loss: 2289.9892578125 best Validation loss: 2285.97802734375\n",
      "Epoch: 24000 Loss: 2296.0126953125 Validation loss: 2289.407958984375 best Validation loss: 2285.97802734375\n",
      "Epoch: 24100 Loss: 2295.048095703125 Validation loss: 2290.551025390625 best Validation loss: 2285.97802734375\n",
      "Epoch: 24200 Loss: 2294.246337890625 Validation loss: 2289.325927734375 best Validation loss: 2285.97802734375\n",
      "Epoch: 24300 Loss: 2293.5166015625 Validation loss: 2288.309326171875 best Validation loss: 2285.97802734375\n",
      "Epoch: 24400 Loss: 2292.8623046875 Validation loss: 2288.715087890625 best Validation loss: 2285.97802734375\n",
      "Epoch: 24500 Loss: 2292.164794921875 Validation loss: 2287.949462890625 best Validation loss: 2285.97802734375\n",
      "Epoch: 24600 Loss: 2291.53515625 Validation loss: 2288.722900390625 best Validation loss: 2285.97802734375\n",
      "Epoch: 24700 Loss: 2290.958251953125 Validation loss: 2287.302978515625 best Validation loss: 2285.97802734375\n",
      "Epoch: 24800 Loss: 2290.362060546875 Validation loss: 2289.135498046875 best Validation loss: 2285.97802734375\n",
      "Epoch: 24900 Loss: 2289.82568359375 Validation loss: 2288.47412109375 best Validation loss: 2285.97802734375\n",
      "Epoch: 25000 Loss: 2289.2783203125 Validation loss: 2289.0 best Validation loss: 2285.97802734375\n",
      "Epoch: 25100 Loss: 2288.7724609375 Validation loss: 2290.009033203125 best Validation loss: 2285.97802734375\n",
      "Epoch: 25200 Loss: 2288.190673828125 Validation loss: 2290.429443359375 best Validation loss: 2285.97802734375\n",
      "Epoch: 25300 Loss: 2287.68994140625 Validation loss: 2290.822998046875 best Validation loss: 2285.97802734375\n",
      "Epoch: 25400 Loss: 2287.186279296875 Validation loss: 2290.473388671875 best Validation loss: 2285.97802734375\n",
      "Epoch: 25500 Loss: 2286.733642578125 Validation loss: 2291.457763671875 best Validation loss: 2285.97802734375\n",
      "Epoch: 25600 Loss: 2286.284423828125 Validation loss: 2290.650390625 best Validation loss: 2285.97802734375\n",
      "Epoch: 25700 Loss: 2285.48876953125 Validation loss: 2289.09033203125 best Validation loss: 2285.97802734375\n",
      "Epoch: 25800 Loss: 2284.90869140625 Validation loss: 2289.27783203125 best Validation loss: 2285.97802734375\n",
      "Epoch: 25900 Loss: 2284.568359375 Validation loss: 2288.929443359375 best Validation loss: 2285.97802734375\n",
      "Epoch: 26000 Loss: 2284.161376953125 Validation loss: 2287.642822265625 best Validation loss: 2285.97802734375\n",
      "Epoch: 26100 Loss: 2283.829833984375 Validation loss: 2288.5947265625 best Validation loss: 2285.97802734375\n",
      "Epoch: 26200 Loss: 2283.449462890625 Validation loss: 2288.30419921875 best Validation loss: 2285.97802734375\n",
      "Epoch: 26300 Loss: 2283.130859375 Validation loss: 2288.2255859375 best Validation loss: 2285.97802734375\n",
      "Epoch: 26400 Loss: 2282.66455078125 Validation loss: 2288.855712890625 best Validation loss: 2285.97802734375\n",
      "Epoch: 26500 Loss: 2282.20458984375 Validation loss: 2289.005126953125 best Validation loss: 2285.97802734375\n",
      "Epoch: 26600 Loss: 2281.81494140625 Validation loss: 2288.042724609375 best Validation loss: 2285.97802734375\n",
      "Epoch: 26700 Loss: 2281.406982421875 Validation loss: 2287.23291015625 best Validation loss: 2285.97802734375\n",
      "Epoch: 26800 Loss: 2280.97314453125 Validation loss: 2285.341552734375 best Validation loss: 2284.638427734375\n",
      "Epoch: 26900 Loss: 2280.580322265625 Validation loss: 2286.11279296875 best Validation loss: 2284.428466796875\n",
      "Epoch: 27000 Loss: 2280.160888671875 Validation loss: 2283.348876953125 best Validation loss: 2283.206787109375\n",
      "Epoch: 27100 Loss: 2279.767333984375 Validation loss: 2282.62158203125 best Validation loss: 2282.62158203125\n",
      "Epoch: 27200 Loss: 2279.343994140625 Validation loss: 2283.6396484375 best Validation loss: 2281.56787109375\n",
      "Epoch: 27300 Loss: 2278.871826171875 Validation loss: 2283.828369140625 best Validation loss: 2281.56787109375\n",
      "Epoch: 27400 Loss: 2278.3310546875 Validation loss: 2286.25390625 best Validation loss: 2281.56787109375\n",
      "Epoch: 27500 Loss: 2277.88232421875 Validation loss: 2287.874755859375 best Validation loss: 2281.56787109375\n",
      "Epoch: 27600 Loss: 2277.42529296875 Validation loss: 2287.2421875 best Validation loss: 2281.56787109375\n",
      "Epoch: 27700 Loss: 2277.025146484375 Validation loss: 2287.071533203125 best Validation loss: 2281.56787109375\n",
      "Epoch: 27800 Loss: 2276.62646484375 Validation loss: 2287.36572265625 best Validation loss: 2281.56787109375\n",
      "Epoch: 27900 Loss: 2276.076416015625 Validation loss: 2291.031005859375 best Validation loss: 2281.56787109375\n",
      "Epoch: 28000 Loss: 2275.52685546875 Validation loss: 2294.947021484375 best Validation loss: 2281.56787109375\n",
      "Epoch: 28100 Loss: 2275.033935546875 Validation loss: 2298.537109375 best Validation loss: 2281.56787109375\n",
      "Epoch: 28200 Loss: 2274.638427734375 Validation loss: 2297.570556640625 best Validation loss: 2281.56787109375\n",
      "Epoch: 28300 Loss: 2274.30517578125 Validation loss: 2299.448486328125 best Validation loss: 2281.56787109375\n",
      "Epoch: 28400 Loss: 2273.94091796875 Validation loss: 2300.8388671875 best Validation loss: 2281.56787109375\n",
      "Epoch: 28500 Loss: 2273.68115234375 Validation loss: 2303.19873046875 best Validation loss: 2281.56787109375\n",
      "Epoch: 28600 Loss: 2273.36767578125 Validation loss: 2302.81396484375 best Validation loss: 2281.56787109375\n",
      "Epoch: 28700 Loss: 2273.068603515625 Validation loss: 2305.768310546875 best Validation loss: 2281.56787109375\n",
      "Epoch: 28800 Loss: 2272.41455078125 Validation loss: 2307.358154296875 best Validation loss: 2281.56787109375\n",
      "Epoch: 28900 Loss: 2272.071533203125 Validation loss: 2308.450927734375 best Validation loss: 2281.56787109375\n",
      "Epoch: 29000 Loss: 2271.760498046875 Validation loss: 2310.074951171875 best Validation loss: 2281.56787109375\n",
      "Epoch: 29100 Loss: 2271.537353515625 Validation loss: 2311.845458984375 best Validation loss: 2281.56787109375\n",
      "Epoch: 29200 Loss: 2271.263427734375 Validation loss: 2314.7919921875 best Validation loss: 2281.56787109375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUMBER_OF_EPOCH):\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 10\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[1;32m     12\u001b[0m     training_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(training_loss, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[248], line 55\u001b[0m, in \u001b[0;36mmlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m engine_capacity_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_capacity(x[:, \u001b[38;5;241m174\u001b[39m:\u001b[38;5;241m189\u001b[39m])\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x[:, :\u001b[38;5;241m3\u001b[39m], manufacutre_output, model_output, gearbox_output, fuel_output, registration_fees_output, engine_capacity_output), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch2.0/lib/python3.11/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "last_val_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for n in range(NUMBER_OF_EPOCH):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)[:, 0]\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    training_loss = np.append(training_loss, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = model(X_val)[:, 0]\n",
    "    val_loss = loss_fn(y_pred, y_val)\n",
    "    validation_loss = np.append(validation_loss, val_loss.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model, 'mlp_model.pth')\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(f'Epoch: {n} Loss: {loss.item()}'f' Validation loss: {val_loss.item()}'f' best Validation loss: {best_val_loss}')\n",
    "        \n",
    "    if last_val_loss < val_loss:\n",
    "        count += 1\n",
    "        if count == 50:\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "    last_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x152373d90>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(training_loss.shape, validation_loss.shape)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/cy9kvd894_bfkfb71dsbxt2w0000gn/T/ipykernel_61623/228361571.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mlp_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "model = torch.load('mlp_model.pth')\n",
    "model.eval()\n",
    "\n",
    "index = df['id']\n",
    "df = df.drop(['id'], axis=1)\n",
    "X = pre.encoder(df)\n",
    "X = pre.standardize(X, scaler=pickle.load(open('scaler.pkl', 'rb')))\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_pred = model(X)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(y_pred.detach().numpy(), columns=['price'])\n",
    "df = pd.concat([index, df], axis=1)\n",
    "df.rename(columns={'price': 'answer'}, inplace=True)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
