{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCH = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import preprossesing as pre\n",
    "import math\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(df):\n",
    "    \n",
    "    df = pre.standardize(df)\n",
    "    df = pre.encoder(df)\n",
    "    df = df.drop(['id'], axis=1)\n",
    "\n",
    "    train, val = pre.test_validation_split(df)\n",
    "    \n",
    "    y_train = torch.tensor(train['price'].values, dtype=torch.float32)\n",
    "    X_train = train.drop(['price'], axis=1)\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    \n",
    "    y_val = torch.tensor(val['price'].values, dtype=torch.float32)\n",
    "    X_val = val.drop(['price'], axis=1)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (manufacturers): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=151, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (gearbox_type): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fuel_type): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (registration_fees): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (engine_capacity): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(9, 64),\n",
    "        nn.ReLU(), \n",
    "        \n",
    "        nn.Linear(64, 1) \n",
    "        )\n",
    "        \n",
    "        self.manufacturers = nn.Sequential(\n",
    "        nn.Linear(9, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(151, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.gearbox_type = nn.Sequential(\n",
    "        nn.Linear(3, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fuel_type = nn.Sequential(\n",
    "        nn.Linear(4, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.registration_fees = nn.Sequential(\n",
    "        nn.Linear(6, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.engine_capacity = nn.Sequential(\n",
    "        nn.Linear(8, 1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        manufacutre_output = self.manufacturers(x[:, 3:12])\n",
    "        model_output = self.model(x[:, 12:163])\n",
    "        gearbox_output = self.gearbox_type(x[:, 163:166])\n",
    "        fuel_output = self.fuel_type(x[:, 166:170])\n",
    "        registration_fees_output = self.registration_fees(x[:, 170:176])\n",
    "        engine_capacity_output = self.engine_capacity(x[:, 176:184])\n",
    "        x = torch.cat((x[:, :3], manufacutre_output, model_output, gearbox_output, fuel_output, registration_fees_output, engine_capacity_output), 1)\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "model = mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_train, y_train, X_val, y_val = prepare_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 18173.0703125 Validation loss: 17865.427734375 best Validation loss: 17865.427734375\n",
      "Epoch: 100 Loss: 18168.533203125 Validation loss: 17860.8046875 best Validation loss: 17860.8046875\n",
      "Epoch: 200 Loss: 18139.87890625 Validation loss: 17831.78125 best Validation loss: 17831.78125\n",
      "Epoch: 300 Loss: 18054.423828125 Validation loss: 17745.623046875 best Validation loss: 17745.623046875\n",
      "Epoch: 400 Loss: 17882.400390625 Validation loss: 17572.404296875 best Validation loss: 17572.404296875\n",
      "Epoch: 500 Loss: 17596.0390625 Validation loss: 17284.271484375 best Validation loss: 17284.271484375\n",
      "Epoch: 600 Loss: 17175.62109375 Validation loss: 16861.384765625 best Validation loss: 16861.384765625\n",
      "Epoch: 700 Loss: 16605.6015625 Validation loss: 16288.0576171875 best Validation loss: 16288.0576171875\n",
      "Epoch: 800 Loss: 15876.0908203125 Validation loss: 15554.265625 best Validation loss: 15554.265625\n",
      "Epoch: 900 Loss: 14984.8837890625 Validation loss: 14657.6630859375 best Validation loss: 14657.6630859375\n",
      "Epoch: 1000 Loss: 13940.7666015625 Validation loss: 13606.970703125 best Validation loss: 13606.970703125\n",
      "Epoch: 1100 Loss: 12768.3134765625 Validation loss: 12426.810546875 best Validation loss: 12426.810546875\n",
      "Epoch: 1200 Loss: 11511.0263671875 Validation loss: 11161.2890625 best Validation loss: 11161.2890625\n",
      "Epoch: 1300 Loss: 10258.962890625 Validation loss: 9902.1455078125 best Validation loss: 9902.1455078125\n",
      "Epoch: 1400 Loss: 9131.5478515625 Validation loss: 8771.755859375 best Validation loss: 8771.755859375\n",
      "Epoch: 1500 Loss: 8237.53515625 Validation loss: 7881.7646484375 best Validation loss: 7881.7646484375\n",
      "Epoch: 1600 Loss: 7605.58154296875 Validation loss: 7259.970703125 best Validation loss: 7259.970703125\n",
      "Epoch: 1700 Loss: 7173.138671875 Validation loss: 6839.18017578125 best Validation loss: 6839.18017578125\n",
      "Epoch: 1800 Loss: 6855.7412109375 Validation loss: 6531.1162109375 best Validation loss: 6531.1162109375\n",
      "Epoch: 1900 Loss: 6596.3427734375 Validation loss: 6277.76025390625 best Validation loss: 6277.76025390625\n",
      "Epoch: 2000 Loss: 6366.16259765625 Validation loss: 6050.95751953125 best Validation loss: 6050.95751953125\n",
      "Epoch: 2100 Loss: 6152.25927734375 Validation loss: 5838.763671875 best Validation loss: 5838.763671875\n",
      "Epoch: 2200 Loss: 5948.73828125 Validation loss: 5636.13623046875 best Validation loss: 5636.13623046875\n",
      "Epoch: 2300 Loss: 5752.78125 Validation loss: 5440.83056640625 best Validation loss: 5440.83056640625\n",
      "Epoch: 2400 Loss: 5562.8857421875 Validation loss: 5251.68310546875 best Validation loss: 5251.68310546875\n",
      "Epoch: 2500 Loss: 5377.5703125 Validation loss: 5067.42626953125 best Validation loss: 5067.42626953125\n",
      "Epoch: 2600 Loss: 5197.8564453125 Validation loss: 4889.38525390625 best Validation loss: 4889.38525390625\n",
      "Epoch: 2700 Loss: 5024.7490234375 Validation loss: 4718.7431640625 best Validation loss: 4718.7431640625\n",
      "Epoch: 2800 Loss: 4859.47216796875 Validation loss: 4556.86865234375 best Validation loss: 4556.86865234375\n",
      "Epoch: 2900 Loss: 4703.4482421875 Validation loss: 4405.28955078125 best Validation loss: 4405.28955078125\n",
      "Epoch: 3000 Loss: 4558.11376953125 Validation loss: 4265.49755859375 best Validation loss: 4265.49755859375\n",
      "Epoch: 3100 Loss: 4424.6953125 Validation loss: 4138.64990234375 best Validation loss: 4138.64990234375\n",
      "Epoch: 3200 Loss: 4303.96875 Validation loss: 4025.39208984375 best Validation loss: 4025.39208984375\n",
      "Epoch: 3300 Loss: 4196.10400390625 Validation loss: 3925.6376953125 best Validation loss: 3925.6376953125\n",
      "Epoch: 3400 Loss: 4100.61474609375 Validation loss: 3838.60498046875 best Validation loss: 3838.60498046875\n",
      "Epoch: 3500 Loss: 4016.44677734375 Validation loss: 3762.9111328125 best Validation loss: 3762.9111328125\n",
      "Epoch: 3600 Loss: 3942.1650390625 Validation loss: 3696.840087890625 best Validation loss: 3696.840087890625\n",
      "Epoch: 3700 Loss: 3876.182373046875 Validation loss: 3638.568359375 best Validation loss: 3638.568359375\n",
      "Epoch: 3800 Loss: 3816.9560546875 Validation loss: 3586.44384765625 best Validation loss: 3586.44384765625\n",
      "Epoch: 3900 Loss: 3763.122802734375 Validation loss: 3539.0712890625 best Validation loss: 3539.0712890625\n",
      "Epoch: 4000 Loss: 3713.537353515625 Validation loss: 3495.31494140625 best Validation loss: 3495.31494140625\n",
      "Epoch: 4100 Loss: 3667.263916015625 Validation loss: 3454.28076171875 best Validation loss: 3454.28076171875\n",
      "Epoch: 4200 Loss: 3623.525634765625 Validation loss: 3415.2890625 best Validation loss: 3415.2890625\n",
      "Epoch: 4300 Loss: 3581.7021484375 Validation loss: 3377.78955078125 best Validation loss: 3377.78955078125\n",
      "Epoch: 4400 Loss: 3541.30126953125 Validation loss: 3341.361572265625 best Validation loss: 3341.361572265625\n",
      "Epoch: 4500 Loss: 3501.931396484375 Validation loss: 3305.62158203125 best Validation loss: 3305.62158203125\n",
      "Epoch: 4600 Loss: 3463.28662109375 Validation loss: 3270.3017578125 best Validation loss: 3270.3017578125\n",
      "Epoch: 4700 Loss: 3425.13818359375 Validation loss: 3235.18505859375 best Validation loss: 3235.18505859375\n",
      "Epoch: 4800 Loss: 3387.34375 Validation loss: 3200.16015625 best Validation loss: 3200.16015625\n",
      "Epoch: 4900 Loss: 3349.825439453125 Validation loss: 3165.188720703125 best Validation loss: 3165.188720703125\n",
      "Epoch: 5000 Loss: 3312.544921875 Validation loss: 3130.282470703125 best Validation loss: 3130.282470703125\n",
      "Epoch: 5100 Loss: 3275.537109375 Validation loss: 3095.4873046875 best Validation loss: 3095.4873046875\n",
      "Epoch: 5200 Loss: 3238.85791015625 Validation loss: 3060.884765625 best Validation loss: 3060.884765625\n",
      "Epoch: 5300 Loss: 3202.590576171875 Validation loss: 3026.591552734375 best Validation loss: 3026.591552734375\n",
      "Epoch: 5400 Loss: 3166.759521484375 Validation loss: 2992.661865234375 best Validation loss: 2992.661865234375\n",
      "Epoch: 5500 Loss: 3131.451416015625 Validation loss: 2959.201904296875 best Validation loss: 2959.201904296875\n",
      "Epoch: 5600 Loss: 3096.897216796875 Validation loss: 2926.43798828125 best Validation loss: 2926.43798828125\n",
      "Epoch: 5700 Loss: 3063.2548828125 Validation loss: 2894.485595703125 best Validation loss: 2894.485595703125\n",
      "Epoch: 5800 Loss: 3030.659423828125 Validation loss: 2863.466064453125 best Validation loss: 2863.466064453125\n",
      "Epoch: 5900 Loss: 2999.2021484375 Validation loss: 2833.492919921875 best Validation loss: 2833.492919921875\n",
      "Epoch: 6000 Loss: 2968.87451171875 Validation loss: 2804.642578125 best Validation loss: 2804.642578125\n",
      "Epoch: 6100 Loss: 2939.960205078125 Validation loss: 2777.280517578125 best Validation loss: 2777.280517578125\n",
      "Epoch: 6200 Loss: 2912.61083984375 Validation loss: 2751.572265625 best Validation loss: 2751.572265625\n",
      "Epoch: 6300 Loss: 2886.8603515625 Validation loss: 2727.5380859375 best Validation loss: 2727.5380859375\n",
      "Epoch: 6400 Loss: 2862.763427734375 Validation loss: 2705.258544921875 best Validation loss: 2705.258544921875\n",
      "Epoch: 6500 Loss: 2840.45263671875 Validation loss: 2684.905517578125 best Validation loss: 2684.905517578125\n",
      "Epoch: 6600 Loss: 2819.864990234375 Validation loss: 2666.447265625 best Validation loss: 2666.447265625\n",
      "Epoch: 6700 Loss: 2800.938720703125 Validation loss: 2649.758544921875 best Validation loss: 2649.758544921875\n",
      "Epoch: 6800 Loss: 2783.589599609375 Validation loss: 2634.859130859375 best Validation loss: 2634.859130859375\n",
      "Epoch: 6900 Loss: 2767.732421875 Validation loss: 2621.621826171875 best Validation loss: 2621.621826171875\n",
      "Epoch: 7000 Loss: 2753.26953125 Validation loss: 2609.90673828125 best Validation loss: 2609.90673828125\n",
      "Epoch: 7100 Loss: 2740.096435546875 Validation loss: 2599.596435546875 best Validation loss: 2599.596435546875\n",
      "Epoch: 7200 Loss: 2728.109619140625 Validation loss: 2590.529296875 best Validation loss: 2590.529296875\n",
      "Epoch: 7300 Loss: 2717.193359375 Validation loss: 2582.53955078125 best Validation loss: 2582.53955078125\n",
      "Epoch: 7400 Loss: 2707.219482421875 Validation loss: 2575.446533203125 best Validation loss: 2575.446533203125\n",
      "Epoch: 7500 Loss: 2698.086181640625 Validation loss: 2569.19677734375 best Validation loss: 2569.19677734375\n",
      "Epoch: 7600 Loss: 2689.68701171875 Validation loss: 2563.63330078125 best Validation loss: 2563.63330078125\n",
      "Epoch: 7700 Loss: 2681.898193359375 Validation loss: 2558.610595703125 best Validation loss: 2558.610595703125\n",
      "Epoch: 7800 Loss: 2674.62939453125 Validation loss: 2554.0830078125 best Validation loss: 2554.0830078125\n",
      "Epoch: 7900 Loss: 2667.756103515625 Validation loss: 2549.900146484375 best Validation loss: 2549.900146484375\n",
      "Epoch: 8000 Loss: 2661.185791015625 Validation loss: 2546.01953125 best Validation loss: 2546.01953125\n",
      "Epoch: 8100 Loss: 2654.835205078125 Validation loss: 2542.275390625 best Validation loss: 2542.275390625\n",
      "Epoch: 8200 Loss: 2648.6123046875 Validation loss: 2538.5166015625 best Validation loss: 2538.5166015625\n",
      "Epoch: 8300 Loss: 2642.419189453125 Validation loss: 2534.647216796875 best Validation loss: 2534.647216796875\n",
      "Epoch: 8400 Loss: 2635.953369140625 Validation loss: 2530.28955078125 best Validation loss: 2530.28955078125\n",
      "Epoch: 8500 Loss: 2629.07275390625 Validation loss: 2525.496826171875 best Validation loss: 2525.496826171875\n",
      "Epoch: 8600 Loss: 2621.815673828125 Validation loss: 2520.05126953125 best Validation loss: 2520.05126953125\n",
      "Epoch: 8700 Loss: 2614.0771484375 Validation loss: 2513.94873046875 best Validation loss: 2513.94873046875\n",
      "Epoch: 8800 Loss: 2605.853515625 Validation loss: 2507.371337890625 best Validation loss: 2507.371337890625\n",
      "Epoch: 8900 Loss: 2597.09033203125 Validation loss: 2500.321044921875 best Validation loss: 2500.321044921875\n",
      "Epoch: 9000 Loss: 2588.078857421875 Validation loss: 2492.862060546875 best Validation loss: 2492.862060546875\n",
      "Epoch: 9100 Loss: 2578.703369140625 Validation loss: 2484.838134765625 best Validation loss: 2484.838134765625\n",
      "Epoch: 9200 Loss: 2568.850830078125 Validation loss: 2476.28662109375 best Validation loss: 2476.28662109375\n",
      "Epoch: 9300 Loss: 2558.40283203125 Validation loss: 2466.92724609375 best Validation loss: 2466.92724609375\n",
      "Epoch: 9400 Loss: 2547.403076171875 Validation loss: 2456.644775390625 best Validation loss: 2456.644775390625\n",
      "Epoch: 9500 Loss: 2535.8115234375 Validation loss: 2445.946533203125 best Validation loss: 2445.946533203125\n",
      "Epoch: 9600 Loss: 2523.5322265625 Validation loss: 2434.339111328125 best Validation loss: 2434.339111328125\n",
      "Epoch: 9700 Loss: 2510.617431640625 Validation loss: 2422.10595703125 best Validation loss: 2422.10595703125\n",
      "Epoch: 9800 Loss: 2497.355224609375 Validation loss: 2409.75439453125 best Validation loss: 2409.75439453125\n",
      "Epoch: 9900 Loss: 2483.827880859375 Validation loss: 2396.79638671875 best Validation loss: 2396.79638671875\n",
      "Epoch: 10000 Loss: 2470.085205078125 Validation loss: 2383.3662109375 best Validation loss: 2383.3662109375\n",
      "Epoch: 10100 Loss: 2455.725830078125 Validation loss: 2369.439208984375 best Validation loss: 2369.439208984375\n",
      "Epoch: 10200 Loss: 2440.4482421875 Validation loss: 2354.30029296875 best Validation loss: 2354.30029296875\n",
      "Epoch: 10300 Loss: 2424.343505859375 Validation loss: 2338.341552734375 best Validation loss: 2338.341552734375\n",
      "Epoch: 10400 Loss: 2407.614501953125 Validation loss: 2321.884033203125 best Validation loss: 2321.884033203125\n",
      "Epoch: 10500 Loss: 2390.534912109375 Validation loss: 2305.552978515625 best Validation loss: 2305.552978515625\n",
      "Epoch: 10600 Loss: 2373.4365234375 Validation loss: 2289.319580078125 best Validation loss: 2289.319580078125\n",
      "Epoch: 10700 Loss: 2356.3662109375 Validation loss: 2273.5751953125 best Validation loss: 2273.5751953125\n",
      "Epoch: 10800 Loss: 2339.89794921875 Validation loss: 2258.03271484375 best Validation loss: 2258.03271484375\n",
      "Epoch: 10900 Loss: 2324.241943359375 Validation loss: 2243.186279296875 best Validation loss: 2243.186279296875\n",
      "Epoch: 11000 Loss: 2309.484375 Validation loss: 2229.178955078125 best Validation loss: 2229.178955078125\n",
      "Epoch: 11100 Loss: 2295.440673828125 Validation loss: 2215.7626953125 best Validation loss: 2215.7626953125\n",
      "Epoch: 11200 Loss: 2282.04248046875 Validation loss: 2202.8623046875 best Validation loss: 2202.8623046875\n",
      "Epoch: 11300 Loss: 2269.621826171875 Validation loss: 2191.18701171875 best Validation loss: 2191.18701171875\n",
      "Epoch: 11400 Loss: 2257.998291015625 Validation loss: 2180.314697265625 best Validation loss: 2180.314697265625\n",
      "Epoch: 11500 Loss: 2247.229248046875 Validation loss: 2170.069580078125 best Validation loss: 2170.069580078125\n",
      "Epoch: 11600 Loss: 2237.1796875 Validation loss: 2160.353759765625 best Validation loss: 2160.353759765625\n",
      "Epoch: 11700 Loss: 2227.788818359375 Validation loss: 2151.00048828125 best Validation loss: 2151.00048828125\n",
      "Epoch: 11800 Loss: 2218.745849609375 Validation loss: 2141.76708984375 best Validation loss: 2141.76708984375\n",
      "Epoch: 11900 Loss: 2210.171875 Validation loss: 2133.206298828125 best Validation loss: 2133.206298828125\n",
      "Epoch: 12000 Loss: 2202.11767578125 Validation loss: 2125.655029296875 best Validation loss: 2125.655029296875\n",
      "Epoch: 12100 Loss: 2194.73974609375 Validation loss: 2118.713134765625 best Validation loss: 2118.713134765625\n",
      "Epoch: 12200 Loss: 2187.895263671875 Validation loss: 2112.364990234375 best Validation loss: 2112.364990234375\n",
      "Epoch: 12300 Loss: 2181.480224609375 Validation loss: 2106.423583984375 best Validation loss: 2106.423583984375\n",
      "Epoch: 12400 Loss: 2175.572021484375 Validation loss: 2101.01220703125 best Validation loss: 2101.01220703125\n",
      "Epoch: 12500 Loss: 2170.14208984375 Validation loss: 2095.9306640625 best Validation loss: 2095.9306640625\n",
      "Epoch: 12600 Loss: 2165.074951171875 Validation loss: 2090.90234375 best Validation loss: 2090.90234375\n",
      "Epoch: 12700 Loss: 2160.262939453125 Validation loss: 2086.09033203125 best Validation loss: 2086.09033203125\n",
      "Epoch: 12800 Loss: 2155.582275390625 Validation loss: 2081.71533203125 best Validation loss: 2081.71533203125\n",
      "Epoch: 12900 Loss: 2150.933349609375 Validation loss: 2077.707763671875 best Validation loss: 2077.707763671875\n",
      "Epoch: 13000 Loss: 2146.588623046875 Validation loss: 2073.90478515625 best Validation loss: 2073.90478515625\n",
      "Epoch: 13100 Loss: 2142.392578125 Validation loss: 2069.923583984375 best Validation loss: 2069.923583984375\n",
      "Epoch: 13200 Loss: 2138.225830078125 Validation loss: 2065.845458984375 best Validation loss: 2065.845458984375\n",
      "Epoch: 13300 Loss: 2134.38916015625 Validation loss: 2062.19189453125 best Validation loss: 2062.19189453125\n",
      "Epoch: 13400 Loss: 2130.70654296875 Validation loss: 2058.751220703125 best Validation loss: 2058.751220703125\n",
      "Epoch: 13500 Loss: 2127.13671875 Validation loss: 2055.28759765625 best Validation loss: 2055.28759765625\n",
      "Epoch: 13600 Loss: 2123.703369140625 Validation loss: 2051.85986328125 best Validation loss: 2051.85986328125\n",
      "Epoch: 13700 Loss: 2120.341064453125 Validation loss: 2048.78515625 best Validation loss: 2048.78515625\n",
      "Epoch: 13800 Loss: 2117.1572265625 Validation loss: 2045.8701171875 best Validation loss: 2045.8701171875\n",
      "Epoch: 13900 Loss: 2114.288330078125 Validation loss: 2043.193603515625 best Validation loss: 2043.193603515625\n",
      "Epoch: 14000 Loss: 2111.39697265625 Validation loss: 2040.380126953125 best Validation loss: 2040.380126953125\n",
      "Epoch: 14100 Loss: 2108.317626953125 Validation loss: 2037.532958984375 best Validation loss: 2037.532958984375\n",
      "Epoch: 14200 Loss: 2104.961669921875 Validation loss: 2035.07861328125 best Validation loss: 2035.07861328125\n",
      "Epoch: 14300 Loss: 2101.90576171875 Validation loss: 2032.626220703125 best Validation loss: 2032.626220703125\n",
      "Epoch: 14400 Loss: 2099.045166015625 Validation loss: 2030.49267578125 best Validation loss: 2030.49267578125\n",
      "Epoch: 14500 Loss: 2096.301513671875 Validation loss: 2028.8602294921875 best Validation loss: 2028.8602294921875\n",
      "Epoch: 14600 Loss: 2093.63720703125 Validation loss: 2027.5948486328125 best Validation loss: 2027.5948486328125\n",
      "Epoch: 14700 Loss: 2090.865966796875 Validation loss: 2025.6533203125 best Validation loss: 2025.6533203125\n",
      "Epoch: 14800 Loss: 2088.438232421875 Validation loss: 2023.9202880859375 best Validation loss: 2023.9202880859375\n",
      "Epoch: 14900 Loss: 2086.0146484375 Validation loss: 2022.1943359375 best Validation loss: 2022.1729736328125\n",
      "Epoch: 15000 Loss: 2083.624755859375 Validation loss: 2020.6605224609375 best Validation loss: 2020.6605224609375\n",
      "Epoch: 15100 Loss: 2081.25341796875 Validation loss: 2018.9757080078125 best Validation loss: 2018.9757080078125\n",
      "Epoch: 15200 Loss: 2078.747314453125 Validation loss: 2017.0579833984375 best Validation loss: 2017.0579833984375\n",
      "Epoch: 15300 Loss: 2076.10302734375 Validation loss: 2015.3013916015625 best Validation loss: 2015.2740478515625\n",
      "Epoch: 15400 Loss: 2073.8310546875 Validation loss: 2013.587646484375 best Validation loss: 2013.587646484375\n",
      "Epoch: 15500 Loss: 2071.37939453125 Validation loss: 2011.943359375 best Validation loss: 2011.920654296875\n",
      "Epoch: 15600 Loss: 2068.968017578125 Validation loss: 2010.3983154296875 best Validation loss: 2010.3983154296875\n",
      "Epoch: 15700 Loss: 2066.685791015625 Validation loss: 2008.60791015625 best Validation loss: 2008.5911865234375\n",
      "Epoch: 15800 Loss: 2064.5703125 Validation loss: 2007.105712890625 best Validation loss: 2007.095947265625\n",
      "Epoch: 15900 Loss: 2062.510986328125 Validation loss: 2005.8218994140625 best Validation loss: 2005.7667236328125\n",
      "Epoch: 16000 Loss: 2060.456787109375 Validation loss: 2004.4722900390625 best Validation loss: 2004.453125\n",
      "Epoch: 16100 Loss: 2058.430908203125 Validation loss: 2003.06396484375 best Validation loss: 2003.045654296875\n",
      "Epoch: 16200 Loss: 2056.2548828125 Validation loss: 2002.096923828125 best Validation loss: 2002.096923828125\n",
      "Epoch: 16300 Loss: 2054.092041015625 Validation loss: 2001.282470703125 best Validation loss: 2001.2730712890625\n",
      "Epoch: 16400 Loss: 2052.16650390625 Validation loss: 2000.087646484375 best Validation loss: 2000.087646484375\n",
      "Epoch: 16500 Loss: 2050.282958984375 Validation loss: 1999.478271484375 best Validation loss: 1999.4593505859375\n",
      "Epoch: 16600 Loss: 2048.502197265625 Validation loss: 1998.258056640625 best Validation loss: 1998.258056640625\n",
      "Epoch: 16700 Loss: 2046.7000732421875 Validation loss: 1997.4080810546875 best Validation loss: 1997.3299560546875\n",
      "Epoch: 16800 Loss: 2044.87939453125 Validation loss: 1996.4173583984375 best Validation loss: 1996.4173583984375\n",
      "Epoch: 16900 Loss: 2043.1492919921875 Validation loss: 1995.721435546875 best Validation loss: 1995.66748046875\n",
      "Epoch: 17000 Loss: 2041.463623046875 Validation loss: 1994.820068359375 best Validation loss: 1994.820068359375\n",
      "Epoch: 17100 Loss: 2039.5604248046875 Validation loss: 1994.1251220703125 best Validation loss: 1994.1251220703125\n",
      "Epoch: 17200 Loss: 2037.682861328125 Validation loss: 1993.025390625 best Validation loss: 1993.025390625\n",
      "Epoch: 17300 Loss: 2036.137451171875 Validation loss: 1992.4466552734375 best Validation loss: 1992.418212890625\n",
      "Epoch: 17400 Loss: 2034.5733642578125 Validation loss: 1991.84326171875 best Validation loss: 1991.83837890625\n",
      "Epoch: 17500 Loss: 2032.9825439453125 Validation loss: 1991.269287109375 best Validation loss: 1991.2227783203125\n",
      "Epoch: 17600 Loss: 2031.2122802734375 Validation loss: 1990.4935302734375 best Validation loss: 1990.4935302734375\n",
      "Epoch: 17700 Loss: 2029.762451171875 Validation loss: 1989.884765625 best Validation loss: 1989.8607177734375\n",
      "Epoch: 17800 Loss: 2028.4742431640625 Validation loss: 1989.2197265625 best Validation loss: 1989.2098388671875\n",
      "Epoch: 17900 Loss: 2027.296142578125 Validation loss: 1988.2752685546875 best Validation loss: 1988.2618408203125\n",
      "Epoch: 18000 Loss: 2026.09033203125 Validation loss: 1987.7371826171875 best Validation loss: 1987.6768798828125\n",
      "Epoch: 18100 Loss: 2024.787841796875 Validation loss: 1987.10888671875 best Validation loss: 1987.10888671875\n",
      "Epoch: 18200 Loss: 2023.605712890625 Validation loss: 1986.9791259765625 best Validation loss: 1986.9476318359375\n",
      "Epoch: 18300 Loss: 2022.452392578125 Validation loss: 1986.4088134765625 best Validation loss: 1986.4088134765625\n",
      "Epoch: 18400 Loss: 2021.158935546875 Validation loss: 1985.8038330078125 best Validation loss: 1985.7890625\n",
      "Epoch: 18500 Loss: 2019.0057373046875 Validation loss: 1985.9656982421875 best Validation loss: 1985.216796875\n",
      "Epoch: 18600 Loss: 2017.4766845703125 Validation loss: 1985.954345703125 best Validation loss: 1985.216796875\n",
      "Epoch: 18700 Loss: 2016.166748046875 Validation loss: 1986.038330078125 best Validation loss: 1985.216796875\n",
      "Epoch: 18800 Loss: 2014.920654296875 Validation loss: 1985.58984375 best Validation loss: 1985.216796875\n",
      "Epoch: 18900 Loss: 2013.72705078125 Validation loss: 1985.1251220703125 best Validation loss: 1985.0897216796875\n",
      "Epoch: 19000 Loss: 2012.555908203125 Validation loss: 1984.7142333984375 best Validation loss: 1984.591064453125\n",
      "Epoch: 19100 Loss: 2011.4097900390625 Validation loss: 1984.24267578125 best Validation loss: 1984.2030029296875\n",
      "Epoch: 19200 Loss: 2010.0308837890625 Validation loss: 1983.378173828125 best Validation loss: 1983.3419189453125\n",
      "Epoch: 19300 Loss: 2008.8028564453125 Validation loss: 1983.00244140625 best Validation loss: 1982.968505859375\n",
      "Epoch: 19400 Loss: 2007.599609375 Validation loss: 1982.88037109375 best Validation loss: 1982.827392578125\n",
      "Epoch: 19500 Loss: 2006.474853515625 Validation loss: 1982.5640869140625 best Validation loss: 1982.53271484375\n",
      "Epoch: 19600 Loss: 2005.4256591796875 Validation loss: 1982.29150390625 best Validation loss: 1982.274169921875\n",
      "Epoch: 19700 Loss: 2004.314453125 Validation loss: 1981.8922119140625 best Validation loss: 1981.8922119140625\n",
      "Epoch: 19800 Loss: 2003.282958984375 Validation loss: 1981.5047607421875 best Validation loss: 1981.4608154296875\n",
      "Epoch: 19900 Loss: 2002.28564453125 Validation loss: 1981.32421875 best Validation loss: 1981.198486328125\n",
      "Epoch: 20000 Loss: 2001.2579345703125 Validation loss: 1980.79541015625 best Validation loss: 1980.79541015625\n",
      "Epoch: 20100 Loss: 2000.2412109375 Validation loss: 1980.118408203125 best Validation loss: 1980.0740966796875\n",
      "Epoch: 20200 Loss: 1999.2498779296875 Validation loss: 1979.506591796875 best Validation loss: 1979.506591796875\n",
      "Epoch: 20300 Loss: 1998.2855224609375 Validation loss: 1979.086669921875 best Validation loss: 1979.086669921875\n",
      "Epoch: 20400 Loss: 1997.375244140625 Validation loss: 1978.7196044921875 best Validation loss: 1978.6351318359375\n",
      "Epoch: 20500 Loss: 1996.45068359375 Validation loss: 1978.32958984375 best Validation loss: 1978.3212890625\n",
      "Epoch: 20600 Loss: 1995.523193359375 Validation loss: 1977.7154541015625 best Validation loss: 1977.682861328125\n",
      "Epoch: 20700 Loss: 1994.5224609375 Validation loss: 1977.4298095703125 best Validation loss: 1977.3050537109375\n",
      "Epoch: 20800 Loss: 1993.5926513671875 Validation loss: 1977.205810546875 best Validation loss: 1977.1634521484375\n",
      "Epoch: 20900 Loss: 1992.7064208984375 Validation loss: 1976.9534912109375 best Validation loss: 1976.9100341796875\n",
      "Epoch: 21000 Loss: 1991.8333740234375 Validation loss: 1976.8070068359375 best Validation loss: 1976.7091064453125\n",
      "Epoch: 21100 Loss: 1990.8743896484375 Validation loss: 1976.22802734375 best Validation loss: 1976.22802734375\n",
      "Epoch: 21200 Loss: 1990.0592041015625 Validation loss: 1976.305908203125 best Validation loss: 1976.2059326171875\n",
      "Epoch: 21300 Loss: 1989.28857421875 Validation loss: 1976.0634765625 best Validation loss: 1975.987060546875\n",
      "Epoch: 21400 Loss: 1988.4912109375 Validation loss: 1975.956298828125 best Validation loss: 1975.951416015625\n",
      "Epoch: 21500 Loss: 1987.7205810546875 Validation loss: 1975.8370361328125 best Validation loss: 1975.7791748046875\n",
      "Epoch: 21600 Loss: 1986.8497314453125 Validation loss: 1976.09521484375 best Validation loss: 1975.7791748046875\n",
      "Epoch: 21700 Loss: 1985.704833984375 Validation loss: 1976.544189453125 best Validation loss: 1975.7791748046875\n",
      "Epoch: 21800 Loss: 1984.7305908203125 Validation loss: 1976.2791748046875 best Validation loss: 1975.7791748046875\n",
      "Epoch: 21900 Loss: 1983.8668212890625 Validation loss: 1975.8751220703125 best Validation loss: 1975.7791748046875\n",
      "Epoch: 22000 Loss: 1983.025634765625 Validation loss: 1975.603271484375 best Validation loss: 1975.5218505859375\n",
      "Epoch: 22100 Loss: 1982.0587158203125 Validation loss: 1975.3775634765625 best Validation loss: 1975.13330078125\n",
      "Epoch: 22200 Loss: 1981.165283203125 Validation loss: 1975.580810546875 best Validation loss: 1975.1187744140625\n",
      "Epoch: 22300 Loss: 1980.220703125 Validation loss: 1975.4844970703125 best Validation loss: 1975.1187744140625\n",
      "Epoch: 22400 Loss: 1979.35009765625 Validation loss: 1975.143798828125 best Validation loss: 1975.1153564453125\n",
      "Epoch: 22500 Loss: 1978.529541015625 Validation loss: 1974.79638671875 best Validation loss: 1974.7564697265625\n",
      "Epoch: 22600 Loss: 1977.750732421875 Validation loss: 1974.2982177734375 best Validation loss: 1974.2982177734375\n",
      "Epoch: 22700 Loss: 1976.918701171875 Validation loss: 1973.81884765625 best Validation loss: 1973.7333984375\n",
      "Epoch: 22800 Loss: 1976.0765380859375 Validation loss: 1973.387451171875 best Validation loss: 1973.341552734375\n",
      "Epoch: 22900 Loss: 1975.22705078125 Validation loss: 1972.8665771484375 best Validation loss: 1972.7552490234375\n",
      "Epoch: 23000 Loss: 1974.3194580078125 Validation loss: 1972.9248046875 best Validation loss: 1972.7552490234375\n",
      "Epoch: 23100 Loss: 1973.2989501953125 Validation loss: 1972.652587890625 best Validation loss: 1972.54296875\n",
      "Epoch: 23200 Loss: 1972.3616943359375 Validation loss: 1972.22021484375 best Validation loss: 1972.18212890625\n",
      "Epoch: 23300 Loss: 1971.5263671875 Validation loss: 1972.4285888671875 best Validation loss: 1972.0440673828125\n",
      "Epoch: 23400 Loss: 1970.755859375 Validation loss: 1972.5849609375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 23500 Loss: 1969.9620361328125 Validation loss: 1972.19140625 best Validation loss: 1972.0440673828125\n",
      "Epoch: 23600 Loss: 1969.1259765625 Validation loss: 1972.279541015625 best Validation loss: 1972.0440673828125\n",
      "Epoch: 23700 Loss: 1968.2900390625 Validation loss: 1972.3206787109375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 23800 Loss: 1967.51318359375 Validation loss: 1972.5999755859375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 23900 Loss: 1966.7454833984375 Validation loss: 1972.72265625 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24000 Loss: 1966.01513671875 Validation loss: 1972.5965576171875 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24100 Loss: 1965.338134765625 Validation loss: 1972.4501953125 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24200 Loss: 1964.515380859375 Validation loss: 1972.7894287109375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24300 Loss: 1963.750244140625 Validation loss: 1972.5054931640625 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24400 Loss: 1962.970458984375 Validation loss: 1972.8955078125 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24500 Loss: 1962.179443359375 Validation loss: 1973.06298828125 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24600 Loss: 1961.397216796875 Validation loss: 1973.18701171875 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24700 Loss: 1960.5599365234375 Validation loss: 1973.14599609375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24800 Loss: 1959.742919921875 Validation loss: 1973.1585693359375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 24900 Loss: 1958.9090576171875 Validation loss: 1973.3785400390625 best Validation loss: 1972.0440673828125\n",
      "Epoch: 25000 Loss: 1957.9080810546875 Validation loss: 1972.4720458984375 best Validation loss: 1972.0440673828125\n",
      "Epoch: 25100 Loss: 1956.9647216796875 Validation loss: 1972.24951171875 best Validation loss: 1972.0440673828125\n",
      "Epoch: 25200 Loss: 1956.074462890625 Validation loss: 1972.2261962890625 best Validation loss: 1971.9530029296875\n",
      "Epoch: 25300 Loss: 1955.167236328125 Validation loss: 1972.20654296875 best Validation loss: 1971.9530029296875\n",
      "Epoch: 25400 Loss: 1954.235107421875 Validation loss: 1971.6973876953125 best Validation loss: 1971.56591796875\n",
      "Epoch: 25500 Loss: 1953.3643798828125 Validation loss: 1971.4365234375 best Validation loss: 1971.3292236328125\n",
      "Epoch: 25600 Loss: 1952.3057861328125 Validation loss: 1971.5555419921875 best Validation loss: 1971.216796875\n",
      "Epoch: 25700 Loss: 1951.02294921875 Validation loss: 1971.8404541015625 best Validation loss: 1971.216796875\n",
      "Epoch: 25800 Loss: 1950.134033203125 Validation loss: 1971.5152587890625 best Validation loss: 1971.216796875\n",
      "Epoch: 25900 Loss: 1949.2608642578125 Validation loss: 1971.3880615234375 best Validation loss: 1971.216796875\n",
      "Epoch: 26000 Loss: 1948.44775390625 Validation loss: 1971.0377197265625 best Validation loss: 1971.0140380859375\n",
      "Epoch: 26100 Loss: 1947.479736328125 Validation loss: 1970.481201171875 best Validation loss: 1970.3837890625\n",
      "Epoch: 26200 Loss: 1946.6309814453125 Validation loss: 1970.260498046875 best Validation loss: 1970.12890625\n",
      "Epoch: 26300 Loss: 1945.8094482421875 Validation loss: 1970.256591796875 best Validation loss: 1970.0386962890625\n",
      "Epoch: 26400 Loss: 1945.070556640625 Validation loss: 1969.908447265625 best Validation loss: 1969.8717041015625\n",
      "Epoch: 26500 Loss: 1944.34716796875 Validation loss: 1969.7342529296875 best Validation loss: 1969.7288818359375\n",
      "Epoch: 26600 Loss: 1943.61328125 Validation loss: 1969.8380126953125 best Validation loss: 1969.632080078125\n",
      "Epoch: 26700 Loss: 1942.9150390625 Validation loss: 1969.8326416015625 best Validation loss: 1969.632080078125\n",
      "Epoch: 26800 Loss: 1942.248046875 Validation loss: 1969.572509765625 best Validation loss: 1969.572509765625\n",
      "Epoch: 26900 Loss: 1941.57080078125 Validation loss: 1969.728759765625 best Validation loss: 1969.506591796875\n",
      "Epoch: 27000 Loss: 1940.84326171875 Validation loss: 1969.3370361328125 best Validation loss: 1969.1973876953125\n",
      "Epoch: 27100 Loss: 1939.9249267578125 Validation loss: 1968.033203125 best Validation loss: 1967.9820556640625\n",
      "Epoch: 27200 Loss: 1938.9881591796875 Validation loss: 1966.630615234375 best Validation loss: 1966.60009765625\n",
      "Epoch: 27300 Loss: 1937.6807861328125 Validation loss: 1965.7210693359375 best Validation loss: 1965.551513671875\n",
      "Epoch: 27400 Loss: 1936.4991455078125 Validation loss: 1965.7237548828125 best Validation loss: 1965.551513671875\n",
      "Epoch: 27500 Loss: 1935.073974609375 Validation loss: 1964.163330078125 best Validation loss: 1964.1326904296875\n",
      "Epoch: 27600 Loss: 1933.7464599609375 Validation loss: 1962.9268798828125 best Validation loss: 1962.8585205078125\n",
      "Epoch: 27700 Loss: 1932.761474609375 Validation loss: 1962.30810546875 best Validation loss: 1962.272705078125\n",
      "Epoch: 27800 Loss: 1931.7847900390625 Validation loss: 1962.3917236328125 best Validation loss: 1962.0968017578125\n",
      "Epoch: 27900 Loss: 1930.7755126953125 Validation loss: 1961.847412109375 best Validation loss: 1961.847412109375\n",
      "Epoch: 28000 Loss: 1929.7177734375 Validation loss: 1961.920166015625 best Validation loss: 1961.796142578125\n",
      "Epoch: 28100 Loss: 1928.6651611328125 Validation loss: 1961.735595703125 best Validation loss: 1961.72265625\n",
      "Epoch: 28200 Loss: 1927.5943603515625 Validation loss: 1961.4298095703125 best Validation loss: 1961.3033447265625\n",
      "Epoch: 28300 Loss: 1926.63623046875 Validation loss: 1961.18212890625 best Validation loss: 1961.0240478515625\n",
      "Epoch: 28400 Loss: 1925.7774658203125 Validation loss: 1960.8106689453125 best Validation loss: 1960.704345703125\n",
      "Epoch: 28500 Loss: 1924.9937744140625 Validation loss: 1960.5989990234375 best Validation loss: 1960.4722900390625\n",
      "Epoch: 28600 Loss: 1924.22802734375 Validation loss: 1960.22705078125 best Validation loss: 1960.169677734375\n",
      "Epoch: 28700 Loss: 1923.41796875 Validation loss: 1959.875244140625 best Validation loss: 1959.845947265625\n",
      "Epoch: 28800 Loss: 1922.61474609375 Validation loss: 1959.77392578125 best Validation loss: 1959.6768798828125\n",
      "Epoch: 28900 Loss: 1921.8424072265625 Validation loss: 1959.160888671875 best Validation loss: 1959.14697265625\n",
      "Epoch: 29000 Loss: 1921.1011962890625 Validation loss: 1959.048583984375 best Validation loss: 1958.951904296875\n",
      "Epoch: 29100 Loss: 1920.3792724609375 Validation loss: 1958.635009765625 best Validation loss: 1958.5528564453125\n",
      "Epoch: 29200 Loss: 1919.6953125 Validation loss: 1958.2064208984375 best Validation loss: 1958.1522216796875\n",
      "Epoch: 29300 Loss: 1919.0341796875 Validation loss: 1957.927734375 best Validation loss: 1957.788330078125\n",
      "Epoch: 29400 Loss: 1918.33251953125 Validation loss: 1957.261962890625 best Validation loss: 1957.150634765625\n",
      "Epoch: 29500 Loss: 1917.593994140625 Validation loss: 1956.39453125 best Validation loss: 1956.39453125\n",
      "Epoch: 29600 Loss: 1916.90625 Validation loss: 1956.1019287109375 best Validation loss: 1956.1019287109375\n",
      "Epoch: 29700 Loss: 1916.2366943359375 Validation loss: 1955.667724609375 best Validation loss: 1955.55712890625\n",
      "Epoch: 29800 Loss: 1915.576904296875 Validation loss: 1955.0950927734375 best Validation loss: 1955.089599609375\n",
      "Epoch: 29900 Loss: 1914.7904052734375 Validation loss: 1954.01416015625 best Validation loss: 1953.9610595703125\n",
      "Epoch: 30000 Loss: 1914.0533447265625 Validation loss: 1953.312744140625 best Validation loss: 1953.2880859375\n",
      "Epoch: 30100 Loss: 1913.3544921875 Validation loss: 1953.00732421875 best Validation loss: 1952.8974609375\n",
      "Epoch: 30200 Loss: 1912.6680908203125 Validation loss: 1952.6441650390625 best Validation loss: 1952.6441650390625\n",
      "Epoch: 30300 Loss: 1911.99853515625 Validation loss: 1952.262451171875 best Validation loss: 1952.09521484375\n",
      "Epoch: 30400 Loss: 1911.35205078125 Validation loss: 1952.042236328125 best Validation loss: 1951.986083984375\n",
      "Epoch: 30500 Loss: 1910.7479248046875 Validation loss: 1951.8560791015625 best Validation loss: 1951.796142578125\n",
      "Epoch: 30600 Loss: 1910.138916015625 Validation loss: 1951.930908203125 best Validation loss: 1951.6583251953125\n",
      "Epoch: 30700 Loss: 1909.5540771484375 Validation loss: 1951.859375 best Validation loss: 1951.6392822265625\n",
      "Epoch: 30800 Loss: 1908.99658203125 Validation loss: 1951.81298828125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 30900 Loss: 1908.451171875 Validation loss: 1952.0548095703125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31000 Loss: 1907.9151611328125 Validation loss: 1952.0286865234375 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31100 Loss: 1907.36474609375 Validation loss: 1951.9962158203125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31200 Loss: 1906.8453369140625 Validation loss: 1952.43408203125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31300 Loss: 1906.2491455078125 Validation loss: 1952.5799560546875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31400 Loss: 1905.721435546875 Validation loss: 1952.5032958984375 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31500 Loss: 1905.238525390625 Validation loss: 1952.3995361328125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31600 Loss: 1904.772705078125 Validation loss: 1952.366455078125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31700 Loss: 1904.328369140625 Validation loss: 1952.04638671875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31800 Loss: 1903.889892578125 Validation loss: 1952.2777099609375 best Validation loss: 1951.6392822265625\n",
      "Epoch: 31900 Loss: 1903.4481201171875 Validation loss: 1952.6207275390625 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32000 Loss: 1902.95166015625 Validation loss: 1952.5343017578125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32100 Loss: 1902.4566650390625 Validation loss: 1952.764404296875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32200 Loss: 1901.9913330078125 Validation loss: 1952.657470703125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32300 Loss: 1901.571533203125 Validation loss: 1952.821533203125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32400 Loss: 1901.1160888671875 Validation loss: 1952.70361328125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32500 Loss: 1900.6810302734375 Validation loss: 1952.5054931640625 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32600 Loss: 1900.2581787109375 Validation loss: 1952.505126953125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32700 Loss: 1899.825927734375 Validation loss: 1952.529296875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32800 Loss: 1899.3929443359375 Validation loss: 1952.033935546875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 32900 Loss: 1898.93701171875 Validation loss: 1951.80322265625 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33000 Loss: 1898.4906005859375 Validation loss: 1952.0919189453125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33100 Loss: 1898.0335693359375 Validation loss: 1951.8223876953125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33200 Loss: 1897.6002197265625 Validation loss: 1951.8397216796875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33300 Loss: 1897.1717529296875 Validation loss: 1952.006591796875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33400 Loss: 1896.7703857421875 Validation loss: 1952.0679931640625 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33500 Loss: 1896.3826904296875 Validation loss: 1952.13720703125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33600 Loss: 1896.0035400390625 Validation loss: 1952.2908935546875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33700 Loss: 1895.6259765625 Validation loss: 1952.4368896484375 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33800 Loss: 1895.264404296875 Validation loss: 1952.37060546875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 33900 Loss: 1894.9000244140625 Validation loss: 1952.0382080078125 best Validation loss: 1951.6392822265625\n",
      "Epoch: 34000 Loss: 1894.5218505859375 Validation loss: 1951.7518310546875 best Validation loss: 1951.6392822265625\n",
      "Epoch: 34100 Loss: 1894.1593017578125 Validation loss: 1951.4229736328125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34200 Loss: 1893.8077392578125 Validation loss: 1951.7493896484375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34300 Loss: 1893.306884765625 Validation loss: 1952.121337890625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34400 Loss: 1892.9041748046875 Validation loss: 1952.25 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34500 Loss: 1892.54248046875 Validation loss: 1952.5406494140625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34600 Loss: 1892.17138671875 Validation loss: 1952.646484375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34700 Loss: 1891.809814453125 Validation loss: 1953.00439453125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34800 Loss: 1891.356689453125 Validation loss: 1952.9912109375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 34900 Loss: 1890.959716796875 Validation loss: 1952.5455322265625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35000 Loss: 1890.512451171875 Validation loss: 1952.0587158203125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35100 Loss: 1890.1190185546875 Validation loss: 1952.1793212890625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35200 Loss: 1889.7745361328125 Validation loss: 1952.1102294921875 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35300 Loss: 1889.4595947265625 Validation loss: 1952.25537109375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35400 Loss: 1889.169677734375 Validation loss: 1952.3720703125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35500 Loss: 1888.8736572265625 Validation loss: 1952.3197021484375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35600 Loss: 1888.582763671875 Validation loss: 1952.4383544921875 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35700 Loss: 1888.2867431640625 Validation loss: 1952.462158203125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35800 Loss: 1887.9769287109375 Validation loss: 1952.6297607421875 best Validation loss: 1951.2757568359375\n",
      "Epoch: 35900 Loss: 1887.6827392578125 Validation loss: 1952.501220703125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36000 Loss: 1887.404541015625 Validation loss: 1952.598876953125 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36100 Loss: 1887.0989990234375 Validation loss: 1952.650146484375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36200 Loss: 1886.7947998046875 Validation loss: 1952.566650390625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36300 Loss: 1886.4642333984375 Validation loss: 1952.7593994140625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36400 Loss: 1886.1619873046875 Validation loss: 1952.724853515625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36500 Loss: 1885.88330078125 Validation loss: 1952.467529296875 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36600 Loss: 1885.584716796875 Validation loss: 1952.4022216796875 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36700 Loss: 1885.2706298828125 Validation loss: 1952.023681640625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36800 Loss: 1884.94580078125 Validation loss: 1951.89306640625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 36900 Loss: 1884.6412353515625 Validation loss: 1951.9195556640625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 37000 Loss: 1884.3319091796875 Validation loss: 1951.7601318359375 best Validation loss: 1951.2757568359375\n",
      "Epoch: 37100 Loss: 1883.997802734375 Validation loss: 1951.494140625 best Validation loss: 1951.2757568359375\n",
      "Epoch: 37200 Loss: 1883.696044921875 Validation loss: 1951.345703125 best Validation loss: 1951.270751953125\n",
      "Epoch: 37300 Loss: 1883.4107666015625 Validation loss: 1950.9820556640625 best Validation loss: 1950.9820556640625\n",
      "Epoch: 37400 Loss: 1883.1251220703125 Validation loss: 1951.0545654296875 best Validation loss: 1950.886474609375\n",
      "Epoch: 37500 Loss: 1882.848388671875 Validation loss: 1951.129638671875 best Validation loss: 1950.8154296875\n",
      "Epoch: 37600 Loss: 1882.5555419921875 Validation loss: 1950.5823974609375 best Validation loss: 1950.536865234375\n",
      "Epoch: 37700 Loss: 1882.2464599609375 Validation loss: 1950.17431640625 best Validation loss: 1950.1259765625\n",
      "Epoch: 37800 Loss: 1881.95263671875 Validation loss: 1949.9339599609375 best Validation loss: 1949.8612060546875\n",
      "Epoch: 37900 Loss: 1881.6588134765625 Validation loss: 1949.7542724609375 best Validation loss: 1949.624755859375\n",
      "Epoch: 38000 Loss: 1881.3665771484375 Validation loss: 1950.3580322265625 best Validation loss: 1949.624755859375\n",
      "Epoch: 38100 Loss: 1881.063232421875 Validation loss: 1950.22900390625 best Validation loss: 1949.624755859375\n",
      "Epoch: 38200 Loss: 1880.7750244140625 Validation loss: 1949.9595947265625 best Validation loss: 1949.624755859375\n",
      "Epoch: 38300 Loss: 1880.47265625 Validation loss: 1950.07177734375 best Validation loss: 1949.624755859375\n",
      "Epoch: 38400 Loss: 1880.1861572265625 Validation loss: 1949.941162109375 best Validation loss: 1949.624755859375\n",
      "Epoch: 38500 Loss: 1879.91552734375 Validation loss: 1949.6815185546875 best Validation loss: 1949.573974609375\n",
      "Epoch: 38600 Loss: 1879.6539306640625 Validation loss: 1949.5006103515625 best Validation loss: 1949.3680419921875\n",
      "Epoch: 38700 Loss: 1879.392333984375 Validation loss: 1949.255615234375 best Validation loss: 1949.246337890625\n",
      "Epoch: 38800 Loss: 1879.137451171875 Validation loss: 1948.8802490234375 best Validation loss: 1948.8536376953125\n",
      "Epoch: 38900 Loss: 1878.8824462890625 Validation loss: 1948.6759033203125 best Validation loss: 1948.6759033203125\n",
      "Epoch: 39000 Loss: 1878.62890625 Validation loss: 1948.6060791015625 best Validation loss: 1948.526123046875\n",
      "Epoch: 39100 Loss: 1878.369384765625 Validation loss: 1948.2889404296875 best Validation loss: 1948.2889404296875\n",
      "Epoch: 39200 Loss: 1878.1292724609375 Validation loss: 1948.228515625 best Validation loss: 1948.1424560546875\n",
      "Epoch: 39300 Loss: 1877.8546142578125 Validation loss: 1948.47265625 best Validation loss: 1948.1387939453125\n",
      "Epoch: 39400 Loss: 1877.606201171875 Validation loss: 1948.2344970703125 best Validation loss: 1948.0985107421875\n",
      "Epoch: 39500 Loss: 1877.3756103515625 Validation loss: 1948.31591796875 best Validation loss: 1948.0985107421875\n",
      "Epoch: 39600 Loss: 1877.1387939453125 Validation loss: 1948.2806396484375 best Validation loss: 1948.0413818359375\n",
      "Epoch: 39700 Loss: 1876.887939453125 Validation loss: 1948.218505859375 best Validation loss: 1948.0413818359375\n",
      "Epoch: 39800 Loss: 1876.6583251953125 Validation loss: 1947.986328125 best Validation loss: 1947.9080810546875\n",
      "Epoch: 39900 Loss: 1876.433349609375 Validation loss: 1947.82470703125 best Validation loss: 1947.7215576171875\n",
      "Epoch: 40000 Loss: 1876.2052001953125 Validation loss: 1947.7022705078125 best Validation loss: 1947.54541015625\n",
      "Epoch: 40100 Loss: 1875.9776611328125 Validation loss: 1947.4874267578125 best Validation loss: 1947.4267578125\n",
      "Epoch: 40200 Loss: 1875.7330322265625 Validation loss: 1947.42431640625 best Validation loss: 1947.17041015625\n",
      "Epoch: 40300 Loss: 1875.5093994140625 Validation loss: 1947.2293701171875 best Validation loss: 1947.0592041015625\n",
      "Epoch: 40400 Loss: 1875.292724609375 Validation loss: 1947.0411376953125 best Validation loss: 1946.97119140625\n",
      "Epoch: 40500 Loss: 1875.0780029296875 Validation loss: 1946.9124755859375 best Validation loss: 1946.7647705078125\n",
      "Epoch: 40600 Loss: 1874.8505859375 Validation loss: 1946.6978759765625 best Validation loss: 1946.659423828125\n",
      "Epoch: 40700 Loss: 1874.639892578125 Validation loss: 1946.513671875 best Validation loss: 1946.50830078125\n",
      "Epoch: 40800 Loss: 1874.4322509765625 Validation loss: 1946.6317138671875 best Validation loss: 1946.50830078125\n",
      "Epoch: 40900 Loss: 1874.24072265625 Validation loss: 1946.5557861328125 best Validation loss: 1946.4801025390625\n",
      "Epoch: 41000 Loss: 1874.04296875 Validation loss: 1946.4989013671875 best Validation loss: 1946.402099609375\n",
      "Epoch: 41100 Loss: 1873.8472900390625 Validation loss: 1946.40380859375 best Validation loss: 1946.335693359375\n",
      "Epoch: 41200 Loss: 1873.6614990234375 Validation loss: 1946.4937744140625 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41300 Loss: 1873.468017578125 Validation loss: 1946.6041259765625 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41400 Loss: 1873.24169921875 Validation loss: 1946.5037841796875 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41500 Loss: 1873.04296875 Validation loss: 1946.688720703125 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41600 Loss: 1872.8519287109375 Validation loss: 1946.7276611328125 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41700 Loss: 1872.6348876953125 Validation loss: 1946.6387939453125 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41800 Loss: 1872.309814453125 Validation loss: 1946.450927734375 best Validation loss: 1946.3311767578125\n",
      "Epoch: 41900 Loss: 1872.0479736328125 Validation loss: 1946.49365234375 best Validation loss: 1946.296630859375\n",
      "Epoch: 42000 Loss: 1871.782470703125 Validation loss: 1946.2325439453125 best Validation loss: 1946.1673583984375\n",
      "Epoch: 42100 Loss: 1871.5167236328125 Validation loss: 1945.91357421875 best Validation loss: 1945.826416015625\n",
      "Epoch: 42200 Loss: 1871.2265625 Validation loss: 1946.027099609375 best Validation loss: 1945.6500244140625\n",
      "Epoch: 42300 Loss: 1870.9693603515625 Validation loss: 1945.5697021484375 best Validation loss: 1945.4913330078125\n",
      "Epoch: 42400 Loss: 1870.728271484375 Validation loss: 1945.1224365234375 best Validation loss: 1945.0089111328125\n",
      "Epoch: 42500 Loss: 1870.489501953125 Validation loss: 1945.101318359375 best Validation loss: 1944.8846435546875\n",
      "Epoch: 42600 Loss: 1870.239501953125 Validation loss: 1944.8663330078125 best Validation loss: 1944.778076171875\n",
      "Epoch: 42700 Loss: 1870.0223388671875 Validation loss: 1944.914306640625 best Validation loss: 1944.76416015625\n",
      "Epoch: 42800 Loss: 1869.8160400390625 Validation loss: 1944.868896484375 best Validation loss: 1944.7393798828125\n",
      "Epoch: 42900 Loss: 1869.551513671875 Validation loss: 1944.9215087890625 best Validation loss: 1944.70166015625\n",
      "Epoch: 43000 Loss: 1869.2972412109375 Validation loss: 1944.823486328125 best Validation loss: 1944.693115234375\n",
      "Epoch: 43100 Loss: 1868.97607421875 Validation loss: 1945.03076171875 best Validation loss: 1944.68701171875\n",
      "Epoch: 43200 Loss: 1868.685791015625 Validation loss: 1945.1619873046875 best Validation loss: 1944.68701171875\n",
      "Epoch: 43300 Loss: 1868.4390869140625 Validation loss: 1945.1346435546875 best Validation loss: 1944.68701171875\n",
      "Epoch: 43400 Loss: 1868.1658935546875 Validation loss: 1945.016845703125 best Validation loss: 1944.68701171875\n",
      "Epoch: 43500 Loss: 1867.877685546875 Validation loss: 1945.2078857421875 best Validation loss: 1944.68701171875\n",
      "Epoch: 43600 Loss: 1867.612060546875 Validation loss: 1945.0948486328125 best Validation loss: 1944.68701171875\n",
      "Epoch: 43700 Loss: 1867.2801513671875 Validation loss: 1944.80517578125 best Validation loss: 1944.68701171875\n",
      "Epoch: 43800 Loss: 1867.0076904296875 Validation loss: 1944.8209228515625 best Validation loss: 1944.5941162109375\n",
      "Epoch: 43900 Loss: 1866.7900390625 Validation loss: 1944.7335205078125 best Validation loss: 1944.57763671875\n",
      "Epoch: 44000 Loss: 1866.5870361328125 Validation loss: 1944.9044189453125 best Validation loss: 1944.5584716796875\n",
      "Epoch: 44100 Loss: 1866.3504638671875 Validation loss: 1944.8670654296875 best Validation loss: 1944.5584716796875\n",
      "Epoch: 44200 Loss: 1866.11962890625 Validation loss: 1944.822998046875 best Validation loss: 1944.5584716796875\n",
      "Epoch: 44300 Loss: 1865.912353515625 Validation loss: 1944.6514892578125 best Validation loss: 1944.549072265625\n",
      "Epoch: 44400 Loss: 1865.7108154296875 Validation loss: 1944.413818359375 best Validation loss: 1944.2955322265625\n",
      "Epoch: 44500 Loss: 1865.5103759765625 Validation loss: 1944.2171630859375 best Validation loss: 1944.18701171875\n",
      "Epoch: 44600 Loss: 1865.3074951171875 Validation loss: 1944.5164794921875 best Validation loss: 1944.0616455078125\n",
      "Epoch: 44700 Loss: 1865.0538330078125 Validation loss: 1944.1036376953125 best Validation loss: 1944.0616455078125\n",
      "Epoch: 44800 Loss: 1864.84375 Validation loss: 1943.9127197265625 best Validation loss: 1943.716064453125\n",
      "Epoch: 44900 Loss: 1864.6461181640625 Validation loss: 1943.829345703125 best Validation loss: 1943.65185546875\n",
      "Epoch: 45000 Loss: 1864.4542236328125 Validation loss: 1943.647216796875 best Validation loss: 1943.5650634765625\n",
      "Epoch: 45100 Loss: 1864.2523193359375 Validation loss: 1943.4046630859375 best Validation loss: 1943.244140625\n",
      "Epoch: 45200 Loss: 1864.0843505859375 Validation loss: 1943.4671630859375 best Validation loss: 1943.178466796875\n",
      "Epoch: 45300 Loss: 1863.9149169921875 Validation loss: 1943.517822265625 best Validation loss: 1943.178466796875\n",
      "Epoch: 45400 Loss: 1863.75537109375 Validation loss: 1943.4453125 best Validation loss: 1943.178466796875\n",
      "Epoch: 45500 Loss: 1863.60791015625 Validation loss: 1943.4383544921875 best Validation loss: 1943.178466796875\n",
      "Epoch: 45600 Loss: 1863.4583740234375 Validation loss: 1943.45654296875 best Validation loss: 1943.178466796875\n",
      "Epoch: 45700 Loss: 1863.3150634765625 Validation loss: 1943.6064453125 best Validation loss: 1943.178466796875\n",
      "Epoch: 45800 Loss: 1863.16845703125 Validation loss: 1943.5963134765625 best Validation loss: 1943.178466796875\n",
      "Epoch: 45900 Loss: 1863.029296875 Validation loss: 1943.6566162109375 best Validation loss: 1943.178466796875\n",
      "Epoch: 46000 Loss: 1862.876220703125 Validation loss: 1943.2886962890625 best Validation loss: 1943.07958984375\n",
      "Epoch: 46100 Loss: 1862.7415771484375 Validation loss: 1943.2490234375 best Validation loss: 1943.07958984375\n",
      "Epoch: 46200 Loss: 1862.6063232421875 Validation loss: 1943.28076171875 best Validation loss: 1943.063720703125\n",
      "Epoch: 46300 Loss: 1862.471923828125 Validation loss: 1943.167236328125 best Validation loss: 1943.063232421875\n",
      "Epoch: 46400 Loss: 1862.339599609375 Validation loss: 1943.307861328125 best Validation loss: 1943.063232421875\n",
      "Epoch: 46500 Loss: 1862.192138671875 Validation loss: 1943.379150390625 best Validation loss: 1943.063232421875\n",
      "Epoch: 46600 Loss: 1862.0504150390625 Validation loss: 1943.350830078125 best Validation loss: 1943.063232421875\n",
      "Epoch: 46700 Loss: 1861.917724609375 Validation loss: 1943.3890380859375 best Validation loss: 1943.063232421875\n",
      "Epoch: 46800 Loss: 1861.7918701171875 Validation loss: 1943.261474609375 best Validation loss: 1943.063232421875\n",
      "Epoch: 46900 Loss: 1861.666748046875 Validation loss: 1943.253662109375 best Validation loss: 1943.063232421875\n",
      "Epoch: 47000 Loss: 1861.54541015625 Validation loss: 1943.343994140625 best Validation loss: 1943.063232421875\n",
      "Epoch: 47100 Loss: 1861.4263916015625 Validation loss: 1943.4337158203125 best Validation loss: 1943.063232421875\n",
      "Epoch: 47200 Loss: 1861.3087158203125 Validation loss: 1943.6893310546875 best Validation loss: 1943.063232421875\n",
      "Epoch: 47300 Loss: 1861.1884765625 Validation loss: 1943.633544921875 best Validation loss: 1943.063232421875\n",
      "Epoch: 47400 Loss: 1861.06494140625 Validation loss: 1943.52783203125 best Validation loss: 1943.063232421875\n",
      "Epoch: 47500 Loss: 1860.934326171875 Validation loss: 1943.6912841796875 best Validation loss: 1943.063232421875\n",
      "Epoch: 47600 Loss: 1860.8045654296875 Validation loss: 1943.8048095703125 best Validation loss: 1943.063232421875\n",
      "Epoch: 47700 Loss: 1860.6820068359375 Validation loss: 1944.1044921875 best Validation loss: 1943.063232421875\n",
      "Epoch: 47800 Loss: 1860.5545654296875 Validation loss: 1943.8673095703125 best Validation loss: 1943.063232421875\n",
      "Epoch: 47900 Loss: 1860.4307861328125 Validation loss: 1943.7596435546875 best Validation loss: 1943.063232421875\n",
      "Epoch: 48000 Loss: 1860.2802734375 Validation loss: 1943.605712890625 best Validation loss: 1943.063232421875\n",
      "Epoch: 48100 Loss: 1860.126220703125 Validation loss: 1943.6759033203125 best Validation loss: 1943.063232421875\n",
      "Epoch: 48200 Loss: 1859.98388671875 Validation loss: 1943.38037109375 best Validation loss: 1943.063232421875\n",
      "Epoch: 48300 Loss: 1859.84033203125 Validation loss: 1943.1978759765625 best Validation loss: 1943.0516357421875\n",
      "Epoch: 48400 Loss: 1859.703857421875 Validation loss: 1943.130615234375 best Validation loss: 1942.904052734375\n",
      "Epoch: 48500 Loss: 1859.5794677734375 Validation loss: 1942.9676513671875 best Validation loss: 1942.8466796875\n",
      "Epoch: 48600 Loss: 1859.4481201171875 Validation loss: 1943.0343017578125 best Validation loss: 1942.80078125\n",
      "Epoch: 48700 Loss: 1859.3167724609375 Validation loss: 1943.03271484375 best Validation loss: 1942.80078125\n",
      "Epoch: 48800 Loss: 1859.18701171875 Validation loss: 1943.127685546875 best Validation loss: 1942.80078125\n",
      "Epoch: 48900 Loss: 1859.0491943359375 Validation loss: 1943.34130859375 best Validation loss: 1942.80078125\n",
      "Epoch: 49000 Loss: 1858.9150390625 Validation loss: 1943.289794921875 best Validation loss: 1942.80078125\n",
      "Epoch: 49100 Loss: 1858.777587890625 Validation loss: 1943.1119384765625 best Validation loss: 1942.80078125\n",
      "Epoch: 49200 Loss: 1858.6595458984375 Validation loss: 1943.14306640625 best Validation loss: 1942.80078125\n",
      "Epoch: 49300 Loss: 1858.541259765625 Validation loss: 1943.2626953125 best Validation loss: 1942.80078125\n",
      "Epoch: 49400 Loss: 1858.407958984375 Validation loss: 1943.1571044921875 best Validation loss: 1942.80078125\n",
      "Epoch: 49500 Loss: 1858.2763671875 Validation loss: 1943.557861328125 best Validation loss: 1942.80078125\n",
      "Epoch: 49600 Loss: 1858.1361083984375 Validation loss: 1943.6114501953125 best Validation loss: 1942.80078125\n",
      "Epoch: 49700 Loss: 1857.9747314453125 Validation loss: 1943.670166015625 best Validation loss: 1942.80078125\n",
      "Epoch: 49800 Loss: 1857.8231201171875 Validation loss: 1943.775390625 best Validation loss: 1942.80078125\n",
      "Epoch: 49900 Loss: 1857.6846923828125 Validation loss: 1943.83447265625 best Validation loss: 1942.80078125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "last_val_loss = float('inf')\n",
    "count = 0\n",
    "\n",
    "for n in range(NUMBER_OF_EPOCH):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)[:, 0]\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    training_loss = np.append(training_loss, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = model(X_val)[:, 0]\n",
    "    val_loss = loss_fn(y_pred, y_val)\n",
    "    validation_loss = np.append(validation_loss, val_loss.item())\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model, 'mlp_model.pth')\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print(f'Epoch: {n} Loss: {loss.item()}'f' Validation loss: {val_loss.item()}'f' best Validation loss: {best_val_loss}')\n",
    "        \n",
    "    if last_val_loss < val_loss:\n",
    "        count += 1\n",
    "        if count == 25:\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "    last_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x150e63d90>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(training_loss.shape, validation_loss.shape)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/cy9kvd894_bfkfb71dsbxt2w0000gn/T/ipykernel_61623/228361571.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mlp_model.pth')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/test.csv')\n",
    "model = torch.load('mlp_model.pth')\n",
    "model.eval()\n",
    "\n",
    "index = df['id']\n",
    "df = df.drop(['id'], axis=1)\n",
    "X = pre.encoder(df)\n",
    "X = pre.standardize(X, scaler=pickle.load(open('scaler.pkl', 'rb')))\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_pred = model(X)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(y_pred.detach().numpy(), columns=['price'])\n",
    "df = pd.concat([index, df], axis=1)\n",
    "df.rename(columns={'price': 'answer'}, inplace=True)\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
